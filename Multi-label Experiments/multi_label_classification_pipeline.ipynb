{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to Fine-Tune CAMELBERT and Evaluate It Using Three Datasets\n",
    "\n",
    "This document outlines the pipeline to fine-tune the MARBERT model and evaluate its performance on three distinct datasets. After fine-tuning, we will compare the results across the datasets to assess performance consistency and accuracy across varying input types and data structures.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overview of Datasets:\n",
    "1. **Dataset 1:** GPT-4o-generated samples\n",
    "2. **Dataset 2:** Samples classified using 18 binary classifiers on random samples\n",
    "3. **Dataset 3:** Samples classified using 18 binary classifiers on equivalent samples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, precision_recall_fscore_support\n",
    "from preprocess import final_eliminations\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import IntervalStrategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer:\n",
    "    def __init__(self, training_dataset_path, labels, exp_num, stage = 0, threshold=0.5, model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-ca\"):\n",
    "        self.labels = labels\n",
    "        self.label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "        self.id2label = {idx: label for label, idx in self.label2id.items()}\n",
    "        self.model_name = model_name\n",
    "        self.exp_num = exp_num\n",
    "        training_dataset = pd.read_csv(training_dataset_path)\n",
    "        self.training_dataset_processed = pd.DataFrame({\n",
    "            'text': training_dataset['tweet'],\n",
    "            'label': training_dataset[self.labels].values.tolist()\n",
    "        })\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.train_df, self.val_df = train_test_split(self.training_dataset_processed, test_size=0.1, random_state=42)\n",
    "        self.train_df['text'] = self.train_df['text'].astype(str)\n",
    "        self.val_df['text'] = self.val_df['text'].astype(str)\n",
    "        self.train_dataset = self.create_dataset(self.train_df)\n",
    "        self.val_dataset = self.create_dataset(self.val_df)\n",
    "        self.threshold = threshold\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.load_model(dropout_rate=0.3)  # Adding dropout rate\n",
    "        self.stage = stage\n",
    "\n",
    "    def create_dataset(self, df):\n",
    "        encodings = self.tokenizer(\n",
    "            df['text'].tolist(), truncation=True, padding=True, max_length=128\n",
    "        )\n",
    "        return TweetDataset(encodings, df['label'].values)\n",
    "\n",
    "    def load_model(self, dropout_rate=0.3):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=len(self.labels),\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id,\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "       \n",
    "        model.config.hidden_dropout_prob = dropout_rate\n",
    "        model.config.attention_probs_dropout_prob = dropout_rate\n",
    "        \n",
    "        for param in model.bert.encoder.layer[:8].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        model.to(self.device)\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        encodings = self.tokenizer(\n",
    "            texts, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=128, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encodings['input_ids'].to(self.device)\n",
    "        attention_mask = encodings['attention_mask'].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "        probabilities = torch.sigmoid(logits).cpu().numpy()\n",
    "        predictions = (probabilities >= self.threshold).astype(int)\n",
    "        variation_score = 1 - (np.sum(probabilities)/18)\n",
    "        return predictions, probabilities, variation_score\n",
    "\n",
    "    \n",
    "    def evaluate(self, dev_path):\n",
    "        if '.tsv' in dev_path:\n",
    "            dev = pd.read_csv(dev_path, sep='\\t')\n",
    "        else:\n",
    "            dev = pd.read_csv(dev_path)\n",
    "        \n",
    "        dev = final_eliminations(dev, column_name=\"sentence\")\n",
    "\n",
    "        df_replaced = dev.replace({'y': 1, 'n': 0})\n",
    "        country_columns = df_replaced.columns.difference(['sentence'])\n",
    "        df_replaced['label'] = df_replaced[country_columns].values.tolist()\n",
    "        df_final = df_replaced[['sentence', 'label']]\n",
    "        \n",
    "        predictions, probabilities, _ = self.predict(df_final['sentence'].tolist())\n",
    "        output_dir = f'./exp_{self.exp_num}'\n",
    "        output_file = os.path.join(self.save_dir, f\"{self.model_name.replace('/', '-')}-experiment-{self.exp_num}_predictions.txt\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(output_file, 'w') as f:\n",
    "            for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "                pred_str = ','.join(map(str, pred))\n",
    "                f.write(f'{pred_str}\\n')\n",
    "\n",
    "        \n",
    "        indexes = [0, 2, 4, 10, 13, 14, 15, 17]\n",
    "        predictions = [output[indexes] for output in predictions]\n",
    "\n",
    "\n",
    "        subset_accuracy = accuracy_score(df_final['label'].tolist(), predictions)\n",
    "        print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "\n",
    "        hamming = hamming_loss(df_final['label'].tolist(), predictions)\n",
    "        print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            df_final['label'].tolist(), predictions, average='micro'  \n",
    "        )\n",
    "        print(f\"Micro Precision: {precision:.4f}\")\n",
    "        print(f\"Micro Recall: {recall:.4f}\")\n",
    "        print(f\"Micro F1-Score: {f1:.4f}\")\n",
    "\n",
    "        precision_per_label, recall_per_label, f1_per_label, _ = precision_recall_fscore_support(\n",
    "            df_final['label'].tolist(), predictions, average=None \n",
    "        )\n",
    "        print(f\"Precision per label: {precision_per_label}\")\n",
    "        print(f\"Recall per label: {recall_per_label}\")\n",
    "        print(f\"F1-Score per label: {f1_per_label}\")\n",
    "        multilabel_check = [np.sum(np.array(prediction)) for prediction in predictions]\n",
    "        print(set(multilabel_check))\n",
    "\n",
    "\n",
    "    def compute_metrics(self, p: EvalPrediction):\n",
    "        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        result = self.multi_label_metrics(preds, p.label_ids)\n",
    "        return result\n",
    "\n",
    "    def multi_label_metrics(self, predictions, labels):\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        probs = sigmoid(torch.Tensor(predictions))\n",
    "        y_pred = np.zeros(probs.shape)\n",
    "        y_pred[np.where(probs >= self.threshold)] = 1\n",
    "        f1 = f1_score(labels, y_pred, average='micro')\n",
    "        roc_auc = roc_auc_score(labels, y_pred, average='micro')\n",
    "        accuracy = accuracy_score(labels, y_pred)\n",
    "        return {'f1': f1, 'roc_auc': roc_auc, 'accuracy': accuracy}\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        num_train_epochs=3,  \n",
    "        metric_for_best_model=\"eval_f1\",  \n",
    "        greater_is_better=True,  \n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        patience=2,\n",
    "        warmup_steps=500,  \n",
    "        base_learning_rate=5e-5,  \n",
    "    ):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./exp_' + str(self.exp_num) + '/results',\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "            warmup_steps=warmup_steps,\n",
    "            learning_rate=base_learning_rate,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./exp_' + str(self.exp_num) + '/logs',\n",
    "            logging_steps=500,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=metric_for_best_model,\n",
    "            greater_is_better=greater_is_better,\n",
    "            fp16=True,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            lr_scheduler_type=\"linear\",  \n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=patience\n",
    "        )\n",
    "\n",
    "        trainer = CustomTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[early_stopping_callback]  \n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        \n",
    "        best_metric_value = trainer.state.best_metric \n",
    "        num_epochs = training_args.num_train_epochs\n",
    "        greater_is_better = training_args.greater_is_better\n",
    "        metric_name = training_args.metric_for_best_model\n",
    "        save_dir = f'./exp_{self.exp_num}/stage_{self.stage}'\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(save_dir, safe_serialization=False)\n",
    "        self.tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def save_model(self, output_dir=None, **kwargs):\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir\n",
    "        for param in self.model.parameters():\n",
    "            param.data = param.data.contiguous()\n",
    "        super().save_model(output_dir, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'Project/NADI2024/subtask1/multilabel/NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv'\n",
    "df = pd.read_csv(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset created and saved as 'balanced_multilabel_dataset_500.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "label_columns = df.columns[2:-1]  \n",
    "df[label_columns] = df[label_columns].astype(int)  \n",
    "\n",
    "threshold = 500  \n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for num_classes in range(1, len(label_columns) + 1):\n",
    "    subset = df[df[label_columns].sum(axis=1) == num_classes]  \n",
    "    \n",
    "   \n",
    "    if len(subset) > threshold:\n",
    "        subset = shuffle(subset).head(threshold)\n",
    "    \n",
    "    \n",
    "    balanced_df = pd.concat([balanced_df, subset], ignore_index=True)\n",
    "\n",
    "\n",
    "balanced_df = shuffle(balanced_df).reset_index(drop=True)\n",
    "balanced_df.to_csv('balanced_multilabel_dataset_' + str(threshold) + '.csv', index=False)\n",
    "\n",
    "print(\"Balanced dataset created and saved as 'balanced_multilabel_dataset_500.csv'\")\n",
    "\n",
    "label_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait', 'Lebanon', \n",
    "                 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar', 'Saudi_Arabia', \n",
    "                 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "\n",
    "counts = {}\n",
    "\n",
    "\n",
    "for i in range(19):\n",
    "    counts[i] = (balanced_df[label_columns].sum(axis=1) == i).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 500,\n",
       " 2: 500,\n",
       " 3: 500,\n",
       " 4: 500,\n",
       " 5: 500,\n",
       " 6: 500,\n",
       " 7: 500,\n",
       " 8: 500,\n",
       " 9: 500,\n",
       " 10: 168,\n",
       " 11: 137,\n",
       " 12: 334,\n",
       " 13: 273,\n",
       " 14: 256,\n",
       " 15: 500,\n",
       " 16: 11,\n",
       " 17: 26,\n",
       " 18: 500}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  5%|▍         | 502/10060 [00:27<08:58, 17.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5952, 'grad_norm': 1.13016939163208, 'learning_rate': 4.97e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1004/10060 [00:56<07:53, 19.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5852, 'grad_norm': 4.487825870513916, 'learning_rate': 4.7400627615062765e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1006/10060 [00:56<08:19, 18.13it/s]\n",
      " 10%|█         | 1006/10060 [00:56<08:19, 18.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5702190399169922, 'eval_f1': 0.721625086283404, 'eval_roc_auc': 0.6566456002973553, 'eval_accuracy': 0.08035714285714286, 'eval_runtime': 0.3316, 'eval_samples_per_second': 1350.871, 'eval_steps_per_second': 57.291, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1502/10060 [01:29<08:15, 17.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5474, 'grad_norm': 1.6961125135421753, 'learning_rate': 4.478556485355649e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 2002/10060 [01:59<08:12, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5359, 'grad_norm': 3.8677239418029785, 'learning_rate': 4.217050209205021e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2012/10060 [01:59<08:10, 16.40it/s]\n",
      " 20%|██        | 2012/10060 [02:00<08:10, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.551264226436615, 'eval_f1': 0.737281067556297, 'eval_roc_auc': 0.6925700472012972, 'eval_accuracy': 0.08035714285714286, 'eval_runtime': 0.3326, 'eval_samples_per_second': 1346.946, 'eval_steps_per_second': 57.125, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2502/10060 [02:32<07:33, 16.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4861, 'grad_norm': 3.64426326751709, 'learning_rate': 3.955543933054394e-05, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 3003/10060 [03:01<06:41, 17.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4749, 'grad_norm': 2.2494912147521973, 'learning_rate': 3.694560669456067e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 3017/10060 [03:02<06:55, 16.94it/s]\n",
      " 30%|███       | 3018/10060 [03:03<06:55, 16.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5411345362663269, 'eval_f1': 0.7527460210715087, 'eval_roc_auc': 0.7294907566665392, 'eval_accuracy': 0.08928571428571429, 'eval_runtime': 0.3123, 'eval_samples_per_second': 1434.531, 'eval_steps_per_second': 60.84, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 3503/10060 [03:33<06:20, 17.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4221, 'grad_norm': 3.1075947284698486, 'learning_rate': 3.433054393305439e-05, 'epoch': 3.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 4003/10060 [04:03<06:08, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4147, 'grad_norm': 5.762308120727539, 'learning_rate': 3.171548117154812e-05, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 4023/10060 [04:04<06:03, 16.60it/s]\n",
      " 40%|████      | 4024/10060 [04:04<06:03, 16.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5831584930419922, 'eval_f1': 0.7425149700598802, 'eval_roc_auc': 0.7443502191112906, 'eval_accuracy': 0.11607142857142858, 'eval_runtime': 0.3143, 'eval_samples_per_second': 1425.53, 'eval_steps_per_second': 60.458, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 4503/10060 [04:36<05:33, 16.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3712, 'grad_norm': 1.8115085363388062, 'learning_rate': 2.9100418410041842e-05, 'epoch': 4.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 5003/10060 [05:07<05:28, 15.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.369, 'grad_norm': 3.180023193359375, 'learning_rate': 2.6485355648535566e-05, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 5029/10060 [05:09<05:41, 14.71it/s]\n",
      " 50%|█████     | 5030/10060 [05:09<05:41, 14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5870450735092163, 'eval_f1': 0.7595059336401065, 'eval_roc_auc': 0.7547659683134773, 'eval_accuracy': 0.12946428571428573, 'eval_runtime': 0.3544, 'eval_samples_per_second': 1264.012, 'eval_steps_per_second': 53.608, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 5503/10060 [05:41<04:50, 15.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3241, 'grad_norm': 2.609508991241455, 'learning_rate': 2.387029288702929e-05, 'epoch': 5.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 6003/10060 [06:11<04:06, 16.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3189, 'grad_norm': 4.649298191070557, 'learning_rate': 2.1255230125523013e-05, 'epoch': 5.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 6035/10060 [06:13<04:18, 15.56it/s]\n",
      " 60%|██████    | 6036/10060 [06:14<04:18, 15.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.607021152973175, 'eval_f1': 0.7645331767469172, 'eval_roc_auc': 0.7531913915381108, 'eval_accuracy': 0.109375, 'eval_runtime': 0.3174, 'eval_samples_per_second': 1411.641, 'eval_steps_per_second': 59.869, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 6503/10060 [06:45<03:27, 17.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2827, 'grad_norm': 5.2005462646484375, 'learning_rate': 1.8640167364016737e-05, 'epoch': 6.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 7003/10060 [07:16<03:03, 16.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.269, 'grad_norm': 3.7985293865203857, 'learning_rate': 1.602510460251046e-05, 'epoch': 6.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 7041/10060 [07:18<03:09, 15.94it/s]\n",
      " 70%|███████   | 7042/10060 [07:18<03:09, 15.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6416592597961426, 'eval_f1': 0.7568916349809885, 'eval_roc_auc': 0.7478023782895085, 'eval_accuracy': 0.09151785714285714, 'eval_runtime': 0.32, 'eval_samples_per_second': 1399.965, 'eval_steps_per_second': 59.374, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 7503/10060 [07:50<02:30, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2388, 'grad_norm': 2.66792893409729, 'learning_rate': 1.3410041841004184e-05, 'epoch': 7.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 8003/10060 [08:22<02:12, 15.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2438, 'grad_norm': 1.4793015718460083, 'learning_rate': 1.079497907949791e-05, 'epoch': 7.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 8047/10060 [08:24<02:14, 14.91it/s]\n",
      " 80%|████████  | 8048/10060 [08:25<02:14, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6881314516067505, 'eval_f1': 0.756483082242529, 'eval_roc_auc': 0.7560156587990452, 'eval_accuracy': 0.109375, 'eval_runtime': 0.3317, 'eval_samples_per_second': 1350.675, 'eval_steps_per_second': 57.283, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 8502/10060 [08:55<01:48, 14.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2159, 'grad_norm': 2.017803192138672, 'learning_rate': 8.185146443514645e-06, 'epoch': 8.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 9002/10060 [09:26<01:02, 16.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2117, 'grad_norm': 2.7105093002319336, 'learning_rate': 5.570083682008369e-06, 'epoch': 8.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9054/10060 [09:29<00:59, 16.99it/s]\n",
      " 90%|█████████ | 9054/10060 [09:29<00:59, 16.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7002390623092651, 'eval_f1': 0.7545499262174127, 'eval_roc_auc': 0.7531412598940754, 'eval_accuracy': 0.10714285714285714, 'eval_runtime': 0.303, 'eval_samples_per_second': 1478.576, 'eval_steps_per_second': 62.707, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 9502/10060 [09:59<00:34, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1931, 'grad_norm': 1.5471935272216797, 'learning_rate': 2.955020920502092e-06, 'epoch': 9.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 10002/10060 [10:29<00:03, 17.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1922, 'grad_norm': 2.906043529510498, 'learning_rate': 3.3995815899581595e-07, 'epoch': 9.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10060/10060 [10:33<00:00, 16.57it/s]\n",
      "100%|██████████| 10060/10060 [10:35<00:00, 16.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7048290967941284, 'eval_f1': 0.7522365805168986, 'eval_roc_auc': 0.7531335047226225, 'eval_accuracy': 0.09821428571428571, 'eval_runtime': 0.3158, 'eval_samples_per_second': 1418.587, 'eval_steps_per_second': 60.163, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10060/10060 [10:38<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 638.6339, 'train_samples_per_second': 62.994, 'train_steps_per_second': 15.752, 'train_loss': 0.36358296686327957, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[3]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=6\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.0833\n",
      "Hamming Loss: 0.3354\n",
      "Micro Precision: 0.5590\n",
      "Micro Recall: 0.4522\n",
      "Micro F1-Score: 0.5000\n",
      "Precision per label: [0.42857143 0.45       0.53731343 0.62711864 0.71428571 0.50847458\n",
      " 0.25       0.63934426]\n",
      "Recall per label: [0.08571429 0.23076923 0.72       0.578125   0.11904762 0.65217391\n",
      " 0.0952381  0.66101695]\n",
      "F1-Score per label: [0.14285714 0.30508475 0.61538462 0.60162602 0.20408163 0.57142857\n",
      " 0.13793103 0.65      ]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 51.94 %\n",
      "MACRO AVERAGE RECALL SCORE: 39.28 %\n",
      "MACRO AVERAGE F1-SCORE: 40.35 %\n",
      "MACRO AVERAGE ACCURACY: 66.46 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_6/camelbert_finetuned_epochs_10_eval_f1_0.7645_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-ca-experiment-6_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 252/252 [00:19<00:00, 13.91it/s]\n",
      "100%|██████████| 252/252 [00:21<00:00, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48796308040618896, 'eval_f1': 0.7621827981136312, 'eval_roc_auc': 0.7501663991770002, 'eval_accuracy': 0.12667660208643816, 'eval_runtime': 0.4998, 'eval_samples_per_second': 1342.658, 'eval_steps_per_second': 56.027, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:24<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.2469, 'train_samples_per_second': 248.857, 'train_steps_per_second': 10.393, 'train_loss': 0.5769250052315849, 'epoch': 1.0}\n",
      "Subset Accuracy: 0.1250\n",
      "Hamming Loss: 0.2844\n",
      "Micro Precision: 0.6005\n",
      "Micro Recall: 0.6966\n",
      "Micro F1-Score: 0.6450\n",
      "Precision per label: [0.78571429 0.675      0.49484536 0.65384615 0.88235294 0.4875\n",
      " 0.66666667 0.65277778]\n",
      "Recall per label: [0.31428571 0.69230769 0.96       0.796875   0.35714286 0.84782609\n",
      " 0.47619048 0.79661017]\n",
      "F1-Score per label: [0.44897959 0.6835443  0.65306122 0.71830986 0.50847458 0.61904762\n",
      " 0.55555556 0.71755725]\n",
      "{np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = f\"/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=7\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=1,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 66.23 %\n",
      "MACRO AVERAGE RECALL SCORE: 65.52 %\n",
      "MACRO AVERAGE F1-SCORE: 61.31 %\n",
      "MACRO AVERAGE ACCURACY: 71.56 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"=/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"=/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"=/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_7/camelbert_finetuned_epochs_1_eval_f1_0.7622_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-mix-experiment-7_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 50%|█████     | 252/504 [00:19<00:19, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48796144127845764, 'eval_f1': 0.7621827981136312, 'eval_roc_auc': 0.7501663991770002, 'eval_accuracy': 0.12667660208643816, 'eval_runtime': 0.4798, 'eval_samples_per_second': 1398.644, 'eval_steps_per_second': 58.364, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 502/504 [00:42<00:00, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5124, 'grad_norm': 3.563359260559082, 'learning_rate': 4.99e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [00:42<00:00, 12.74it/s]\n",
      "100%|██████████| 504/504 [00:45<00:00, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4665817618370056, 'eval_f1': 0.769424942809813, 'eval_roc_auc': 0.7668044404020513, 'eval_accuracy': 0.14754098360655737, 'eval_runtime': 0.4957, 'eval_samples_per_second': 1353.615, 'eval_steps_per_second': 56.485, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [00:47<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 47.8604, 'train_samples_per_second': 252.15, 'train_steps_per_second': 10.531, 'train_loss': 0.5122848169671165, 'epoch': 2.0}\n",
      "Subset Accuracy: 0.1333\n",
      "Hamming Loss: 0.2646\n",
      "Micro Precision: 0.6321\n",
      "Micro Recall: 0.6854\n",
      "Micro F1-Score: 0.6577\n",
      "Precision per label: [0.6875     0.70833333 0.52808989 0.65384615 1.         0.53846154\n",
      " 0.64705882 0.75510204]\n",
      "Recall per label: [0.31428571 0.87179487 0.94       0.796875   0.26190476 0.91304348\n",
      " 0.52380952 0.62711864]\n",
      "F1-Score per label: [0.43137255 0.7816092  0.67625899 0.71830986 0.41509434 0.67741935\n",
      " 0.57894737 0.68518519]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=8\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=2,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 68.98 %\n",
      "MACRO AVERAGE RECALL SCORE: 65.61 %\n",
      "MACRO AVERAGE F1-SCORE: 62.05 %\n",
      "MACRO AVERAGE ACCURACY: 73.54 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-mix-experiment-8_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|█████     | 252/504 [00:19<00:18, 13.85it/s]\n",
      " 50%|█████     | 252/504 [00:19<00:18, 13.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4918179214000702, 'eval_f1': 0.7811874048553802, 'eval_roc_auc': 0.7815018201807157, 'eval_accuracy': 0.16989567809239942, 'eval_runtime': 0.495, 'eval_samples_per_second': 1355.615, 'eval_steps_per_second': 56.568, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 502/504 [00:41<00:00, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3169, 'grad_norm': 3.382401466369629, 'learning_rate': 4.99e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [00:41<00:00, 13.63it/s]\n",
      "100%|██████████| 504/504 [00:44<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5153090953826904, 'eval_f1': 0.7765307806568741, 'eval_roc_auc': 0.7810091627168709, 'eval_accuracy': 0.16095380029806258, 'eval_runtime': 0.4839, 'eval_samples_per_second': 1386.632, 'eval_steps_per_second': 57.862, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [00:46<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 46.7301, 'train_samples_per_second': 258.249, 'train_steps_per_second': 10.785, 'train_loss': 0.3173308519143907, 'epoch': 2.0}\n",
      "Subset Accuracy: 0.1833\n",
      "Hamming Loss: 0.2406\n",
      "Micro Precision: 0.6791\n",
      "Micro Recall: 0.6657\n",
      "Micro F1-Score: 0.6723\n",
      "Precision per label: [0.78571429 0.76744186 0.57746479 0.73134328 0.84210526 0.56060606\n",
      " 0.66666667 0.74074074]\n",
      "Recall per label: [0.31428571 0.84615385 0.82       0.765625   0.38095238 0.80434783\n",
      " 0.47619048 0.6779661 ]\n",
      "F1-Score per label: [0.44897959 0.80487805 0.67768595 0.7480916  0.52459016 0.66071429\n",
      " 0.55555556 0.7079646 ]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=9\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=2,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 70.90 %\n",
      "MACRO AVERAGE RECALL SCORE: 63.57 %\n",
      "MACRO AVERAGE F1-SCORE: 64.11 %\n",
      "MACRO AVERAGE ACCURACY: 75.94 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_9/camelbert_finetuned_epochs_2_eval_f1_0.7812_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_8-camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3-experiment-9_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 330/1980 [00:25<03:22,  8.15it/s]\n",
      " 17%|█▋        | 330/1980 [00:26<03:22,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4739709496498108, 'eval_f1': 0.7463900752491357, 'eval_roc_auc': 0.7755923502913882, 'eval_accuracy': 0.11161731207289294, 'eval_runtime': 0.6115, 'eval_samples_per_second': 1435.763, 'eval_steps_per_second': 60.505, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 502/1980 [00:42<02:00, 12.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5121, 'grad_norm': 3.3443150520324707, 'learning_rate': 4.99e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 660/1980 [00:55<02:11, 10.03it/s]\n",
      " 33%|███▎      | 660/1980 [00:56<02:11, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.454266220331192, 'eval_f1': 0.7598915235380015, 'eval_roc_auc': 0.7901453281480804, 'eval_accuracy': 0.17539863325740318, 'eval_runtime': 0.6578, 'eval_samples_per_second': 1334.809, 'eval_steps_per_second': 56.25, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 990/1980 [01:26<01:18, 12.68it/s]\n",
      " 50%|█████     | 990/1980 [01:26<01:18, 12.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.475048303604126, 'eval_f1': 0.7562757722636186, 'eval_roc_auc': 0.7892164231072305, 'eval_accuracy': 0.16856492027334852, 'eval_runtime': 0.6607, 'eval_samples_per_second': 1328.888, 'eval_steps_per_second': 56.001, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1002/1980 [01:30<02:36,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3603, 'grad_norm': 1.35929274559021, 'learning_rate': 3.314189189189189e-05, 'epoch': 3.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1320/1980 [01:57<00:52, 12.62it/s]\n",
      " 67%|██████▋   | 1320/1980 [01:57<00:52, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5146341323852539, 'eval_f1': 0.758403801803061, 'eval_roc_auc': 0.7890565235205352, 'eval_accuracy': 0.16287015945330297, 'eval_runtime': 0.6829, 'eval_samples_per_second': 1285.638, 'eval_steps_per_second': 54.178, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1502/1980 [02:15<00:39, 11.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2479, 'grad_norm': 2.6936798095703125, 'learning_rate': 1.6250000000000002e-05, 'epoch': 4.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1650/1980 [02:28<00:27, 11.80it/s]\n",
      " 83%|████████▎ | 1650/1980 [02:28<00:27, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5459094643592834, 'eval_f1': 0.7602208400307499, 'eval_roc_auc': 0.7907542852717375, 'eval_accuracy': 0.16173120728929385, 'eval_runtime': 0.7121, 'eval_samples_per_second': 1233.033, 'eval_steps_per_second': 51.962, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1980/1980 [02:58<00:00, 13.02it/s]\n",
      "100%|██████████| 1980/1980 [03:01<00:00, 13.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5531436800956726, 'eval_f1': 0.7577061437539677, 'eval_roc_auc': 0.7889758052175457, 'eval_accuracy': 0.15831435079726652, 'eval_runtime': 0.7475, 'eval_samples_per_second': 1174.636, 'eval_steps_per_second': 49.501, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1980/1980 [03:04<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 184.5783, 'train_samples_per_second': 256.704, 'train_steps_per_second': 10.727, 'train_loss': 0.32922791375054256, 'epoch': 6.0}\n",
      "Subset Accuracy: 0.1583\n",
      "Hamming Loss: 0.2594\n",
      "Micro Precision: 0.6801\n",
      "Micro Recall: 0.5674\n",
      "Micro F1-Score: 0.6187\n",
      "Precision per label: [0.69230769 0.775      0.6        0.71929825 0.69230769 0.62264151\n",
      " 0.46153846 0.77083333]\n",
      "Recall per label: [0.25714286 0.79487179 0.72       0.640625   0.21428571 0.7173913\n",
      " 0.28571429 0.62711864]\n",
      "F1-Score per label: [0.375      0.78481013 0.65454545 0.67768595 0.32727273 0.66666667\n",
      " 0.35294118 0.69158879]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\", \"balanced_multilabel_dataset_750.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[5]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=10\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=6,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 66.67 %\n",
      "MACRO AVERAGE RECALL SCORE: 53.21 %\n",
      "MACRO AVERAGE F1-SCORE: 56.63 %\n",
      "MACRO AVERAGE ACCURACY: 74.06 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"exp_10/camelbert_finetuned_epochs_6_eval_f1_0.7602_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-mix-experiment-10_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 252/1512 [00:19<01:30, 13.87it/s]\n",
      " 17%|█▋        | 252/1512 [00:19<01:30, 13.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28491029143333435, 'eval_f1': 0.8595482203884305, 'eval_roc_auc': 0.8654463416464854, 'eval_accuracy': 0.28315946348733234, 'eval_runtime': 0.4594, 'eval_samples_per_second': 1460.564, 'eval_steps_per_second': 60.948, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 502/1512 [00:41<01:19, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2935, 'grad_norm': 3.5294675827026367, 'learning_rate': 5e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 504/1512 [00:41<01:15, 13.29it/s]\n",
      " 33%|███▎      | 504/1512 [00:41<01:15, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30023330450057983, 'eval_f1': 0.8561597851628063, 'eval_roc_auc': 0.8624923837192617, 'eval_accuracy': 0.2786885245901639, 'eval_runtime': 0.4719, 'eval_samples_per_second': 1422.021, 'eval_steps_per_second': 59.339, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 756/1512 [01:03<00:55, 13.60it/s]\n",
      " 50%|█████     | 756/1512 [01:04<00:55, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33522099256515503, 'eval_f1': 0.8580319596299412, 'eval_roc_auc': 0.8644862487499366, 'eval_accuracy': 0.28912071535022354, 'eval_runtime': 0.4806, 'eval_samples_per_second': 1396.254, 'eval_steps_per_second': 58.264, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 1002/1512 [01:25<00:39, 12.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2058, 'grad_norm': 2.2834112644195557, 'learning_rate': 2.5296442687747035e-05, 'epoch': 3.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1008/1512 [01:25<00:38, 13.25it/s]\n",
      " 67%|██████▋   | 1008/1512 [01:26<00:38, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3422885537147522, 'eval_f1': 0.8558310376492194, 'eval_roc_auc': 0.8618272837252222, 'eval_accuracy': 0.2906110283159464, 'eval_runtime': 0.4943, 'eval_samples_per_second': 1357.393, 'eval_steps_per_second': 56.642, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1260/1512 [01:48<00:18, 13.55it/s]\n",
      " 83%|████████▎ | 1260/1512 [01:48<00:18, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3467765152454376, 'eval_f1': 0.8621884241656105, 'eval_roc_auc': 0.8688706876949601, 'eval_accuracy': 0.30849478390462, 'eval_runtime': 0.4589, 'eval_samples_per_second': 1462.286, 'eval_steps_per_second': 61.019, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1502/1512 [02:10<00:00, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1434, 'grad_norm': 1.2461748123168945, 'learning_rate': 5.928853754940711e-07, 'epoch': 5.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1512/1512 [02:10<00:00, 13.59it/s]\n",
      "100%|██████████| 1512/1512 [02:13<00:00, 13.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35399869084358215, 'eval_f1': 0.8584212747994935, 'eval_roc_auc': 0.8651041609176625, 'eval_accuracy': 0.30998509687034276, 'eval_runtime': 0.5414, 'eval_samples_per_second': 1239.478, 'eval_steps_per_second': 51.722, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1512/1512 [02:16<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 136.0435, 'train_samples_per_second': 266.121, 'train_steps_per_second': 11.114, 'train_loss': 0.21364403985164784, 'epoch': 6.0}\n",
      "Subset Accuracy: 0.1667\n",
      "Hamming Loss: 0.2687\n",
      "Micro Precision: 0.6960\n",
      "Micro Recall: 0.4888\n",
      "Micro F1-Score: 0.5743\n",
      "Precision per label: [0.69230769 0.74358974 0.63265306 0.73170732 0.76923077 0.65789474\n",
      " 0.5        0.76744186]\n",
      "Recall per label: [0.25714286 0.74358974 0.62       0.46875    0.23809524 0.54347826\n",
      " 0.33333333 0.55932203]\n",
      "F1-Score per label: [0.375      0.74358974 0.62626263 0.57142857 0.36363636 0.5952381\n",
      " 0.4        0.64705882]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\", \"balanced_multilabel_dataset_750.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_10/camelbert_finetuned_epochs_6_eval_f1_0.7602_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=10\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=6,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 68.69 %\n",
      "MACRO AVERAGE RECALL SCORE: 47.05 %\n",
      "MACRO AVERAGE F1-SCORE: 54.03 %\n",
      "MACRO AVERAGE ACCURACY: 73.12 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_10/camelbert_finetuned_epochs_6_eval_f1_0.8622_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_10-camelbert_finetuned_epochs_6_eval_f1_0.7602_greater_threshold_0.3-experiment-10_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 503/3952 [00:27<02:59, 19.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5519, 'grad_norm': 1.2335233688354492, 'learning_rate': 4.97e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 25%|██▌       | 988/3952 [00:54<02:33, 19.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48687317967414856, 'eval_f1': 0.7363865383373167, 'eval_roc_auc': 0.7617819014921697, 'eval_accuracy': 0.12414578587699317, 'eval_runtime': 0.608, 'eval_samples_per_second': 1444.047, 'eval_steps_per_second': 60.854, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1002/3952 [00:57<05:29,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4956, 'grad_norm': 3.4177091121673584, 'learning_rate': 4.2801274623406724e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1502/3952 [01:26<02:30, 16.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.409, 'grad_norm': 3.1355440616607666, 'learning_rate': 3.555909617612978e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 1976/3952 [01:56<02:11, 15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46634554862976074, 'eval_f1': 0.7590600808385285, 'eval_roc_auc': 0.7884490538348937, 'eval_accuracy': 0.17995444191343962, 'eval_runtime': 0.6889, 'eval_samples_per_second': 1274.564, 'eval_steps_per_second': 53.712, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 2002/3952 [02:00<02:16, 14.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3971, 'grad_norm': 3.05737566947937, 'learning_rate': 2.8316917728852837e-05, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2502/3952 [02:31<01:22, 17.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3024, 'grad_norm': 1.4415926933288574, 'learning_rate': 2.10747392815759e-05, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 2964/3952 [03:00<01:03, 15.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48994776606559753, 'eval_f1': 0.7606552726756968, 'eval_roc_auc': 0.7920201280773378, 'eval_accuracy': 0.1765375854214123, 'eval_runtime': 0.695, 'eval_samples_per_second': 1263.324, 'eval_steps_per_second': 53.238, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 3002/3952 [03:05<00:58, 16.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3012, 'grad_norm': 3.860936403274536, 'learning_rate': 1.3832560834298958e-05, 'epoch': 3.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 3502/3952 [03:35<00:26, 16.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2342, 'grad_norm': 2.205425500869751, 'learning_rate': 6.590382387022016e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 3952/3952 [04:05<00:00, 18.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5275660753250122, 'eval_f1': 0.7625708091474929, 'eval_roc_auc': 0.7929900864074229, 'eval_accuracy': 0.17198177676537585, 'eval_runtime': 0.6341, 'eval_samples_per_second': 1384.718, 'eval_steps_per_second': 58.354, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3952/3952 [04:08<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 248.0563, 'train_samples_per_second': 127.342, 'train_steps_per_second': 15.932, 'train_loss': 0.3670956538273738, 'epoch': 4.0}\n",
      "Subset Accuracy: 0.1833\n",
      "Hamming Loss: 0.2573\n",
      "Micro Precision: 0.6860\n",
      "Micro Recall: 0.5646\n",
      "Micro F1-Score: 0.6194\n",
      "Precision per label: [0.75       0.74418605 0.5862069  0.72222222 0.84615385 0.61538462\n",
      " 0.46666667 0.80434783]\n",
      "Recall per label: [0.25714286 0.82051282 0.68       0.609375   0.26190476 0.69565217\n",
      " 0.33333333 0.62711864]\n",
      "F1-Score per label: [0.38297872 0.7804878  0.62962963 0.66101695 0.4        0.65306122\n",
      " 0.38888889 0.7047619 ]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_750.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=11\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=4,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 69.19 %\n",
      "MACRO AVERAGE RECALL SCORE: 53.56 %\n",
      "MACRO AVERAGE F1-SCORE: 57.51 %\n",
      "MACRO AVERAGE ACCURACY: 74.27 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_11/camelbert_finetuned_epochs_4_eval_f1_0.7626_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-mix-experiment-11_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\n",
      "  5%|▌         | 52/988 [02:54<00:52, 17.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3145, 'grad_norm': 3.7815072536468506, 'learning_rate': 4.99e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      "  5%|▌         | 52/988 [02:55<00:52, 17.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5411572456359863, 'eval_f1': 0.7787280360752604, 'eval_roc_auc': 0.7744481507891129, 'eval_accuracy': 0.16393442622950818, 'eval_runtime': 0.4921, 'eval_samples_per_second': 1363.594, 'eval_steps_per_second': 56.901, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 52/988 [03:26<00:52, 17.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3036, 'grad_norm': 2.7231392860412598, 'learning_rate': 3.7617866004962784e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                                \n",
      "\n",
      "                                                \n",
      "  5%|▌         | 52/988 [03:27<00:52, 17.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5048529505729675, 'eval_f1': 0.7767005978348683, 'eval_roc_auc': 0.7780685608761594, 'eval_accuracy': 0.15797317436661698, 'eval_runtime': 0.4733, 'eval_samples_per_second': 1417.646, 'eval_steps_per_second': 59.157, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 52/988 [03:59<00:52, 17.78it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2531, 'grad_norm': 1.3901019096374512, 'learning_rate': 2.5210918114143922e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      "  5%|▌         | 52/988 [04:00<00:52, 17.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5629974603652954, 'eval_f1': 0.7656576200417536, 'eval_roc_auc': 0.7780476988897891, 'eval_accuracy': 0.15201192250372578, 'eval_runtime': 0.4748, 'eval_samples_per_second': 1413.085, 'eval_steps_per_second': 58.966, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 52/988 [04:31<00:52, 17.78it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2008, 'grad_norm': 1.9427447319030762, 'learning_rate': 1.2803970223325062e-05, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      "  5%|▌         | 52/988 [04:32<00:52, 17.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5819849967956543, 'eval_f1': 0.7798867798867799, 'eval_roc_auc': 0.7856383988591031, 'eval_accuracy': 0.16095380029806258, 'eval_runtime': 0.4735, 'eval_samples_per_second': 1416.998, 'eval_steps_per_second': 59.13, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 52/988 [05:03<00:52, 17.78it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1635, 'grad_norm': 1.539159893989563, 'learning_rate': 3.970223325062035e-07, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      "  5%|▌         | 52/988 [05:07<00:52, 17.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.599375307559967, 'eval_f1': 0.7798488664987405, 'eval_roc_auc': 0.7867945833894436, 'eval_accuracy': 0.15350223546944858, 'eval_runtime': 0.4795, 'eval_samples_per_second': 1399.392, 'eval_steps_per_second': 58.395, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2515/2515 [02:44<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 164.7329, 'train_samples_per_second': 183.145, 'train_steps_per_second': 15.267, 'train_loss': 0.24653972858937076, 'epoch': 5.0}\n",
      "Subset Accuracy: 0.1583\n",
      "Hamming Loss: 0.2635\n",
      "Micro Precision: 0.6873\n",
      "Micro Recall: 0.5309\n",
      "Micro F1-Score: 0.5990\n",
      "Precision per label: [0.8        0.7804878  0.60377358 0.71111111 0.66666667 0.61904762\n",
      " 0.625      0.72916667]\n",
      "Recall per label: [0.34285714 0.82051282 0.64       0.5        0.23809524 0.56521739\n",
      " 0.47619048 0.59322034]\n",
      "F1-Score per label: [0.48       0.8        0.62135922 0.58715596 0.35087719 0.59090909\n",
      " 0.54054054 0.65420561]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/525870965.py:81: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_9/camelbert_finetuned_epochs_2_eval_f1_0.7812_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=12\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=5,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lara.hassan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 69.19 %\n",
      "MACRO AVERAGE RECALL SCORE: 52.20 %\n",
      "MACRO AVERAGE F1-SCORE: 57.81 %\n",
      "MACRO AVERAGE ACCURACY: 73.65 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_12/camelbert_finetuned_epochs_5_eval_f1_0.7799_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_9-camelbert_finetuned_epochs_2_eval_f1_0.7812_greater_threshold_0.3-experiment-12_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " array([[0.9784671 , 0.98201376, 0.97339284, 0.97706646, 0.9714559 ,\n",
       "         0.98421544, 0.98141783, 0.9826909 , 0.9749646 , 0.98521465,\n",
       "         0.9819447 , 0.9868787 , 0.97414124, 0.9840936 , 0.98228765,\n",
       "         0.97850823, 0.9845754 , 0.987375  ]], dtype=float32))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"الله اكبر\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]]),\n",
       " array([[0.02100437, 0.56673807, 0.96826136, 0.6016521 , 0.6660625 ,\n",
       "         0.7235049 , 0.4344013 , 0.5190795 , 0.01851105, 0.41750893,\n",
       "         0.5469499 , 0.5705108 , 0.7168137 , 0.7305783 , 0.40751025,\n",
       "         0.01428194, 0.30002728, 0.4107563 ]], dtype=float32))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"انا مصرى ياسطااااا\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]),\n",
       " array([[0.0186178 , 0.00944908, 0.05379965, 0.11636177, 0.87768656,\n",
       "         0.0102486 , 0.91181   , 0.02546808, 0.01876109, 0.00818791,\n",
       "         0.8321002 , 0.00912564, 0.02909076, 0.01317195, 0.8830344 ,\n",
       "         0.01879707, 0.00575254, 0.01016966]], dtype=float32))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(['بدى فتوش و بدك ثومية'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " array([[0.13660839, 0.01159736, 0.08344117, 0.15241033, 0.03704717,\n",
       "         0.02442309, 0.02052779, 0.19329959, 0.10212548, 0.01141961,\n",
       "         0.0226292 , 0.01036816, 0.07504072, 0.01890545, 0.02465686,\n",
       "         0.20323272, 0.00656425, 0.0113756 ]], dtype=float32))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([' خى ما قصرت'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0]]),\n",
       " array([[0.0082516 , 0.78727776, 0.9410069 , 0.87997437, 0.9512329 ,\n",
       "         0.84376645, 0.8782099 , 0.6583077 , 0.00854433, 0.36274344,\n",
       "         0.9314625 , 0.71452844, 0.9380106 , 0.5837373 , 0.88602656,\n",
       "         0.00919655, 0.32562777, 0.27338037]], dtype=float32))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(['الحمد لله يا زلمة'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " array([[0.97029626, 0.98527133, 0.9717254 , 0.9797453 , 0.9780936 ,\n",
       "         0.9874236 , 0.9840936 , 0.9809491 , 0.9636434 , 0.9873262 ,\n",
       "         0.98481095, 0.98926485, 0.97879386, 0.9832145 , 0.98486924,\n",
       "         0.9704086 , 0.98646784, 0.98844737]], dtype=float32))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(['الحمد لله  '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]),\n",
       " array([[0.0349467 , 0.03507867, 0.8701566 , 0.22695492, 0.07303239,\n",
       "         0.05964694, 0.03890198, 0.17497347, 0.03704717, 0.0193451 ,\n",
       "         0.06176271, 0.02640552, 0.09704755, 0.4174496 , 0.03725677,\n",
       "         0.02992975, 0.01802074, 0.03015741]], dtype=float32))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"كل زول ليه الزول بتاعه\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE DATA TO BE MORE BALANCED\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "directory = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/\"\n",
    "dataset_path = directory + \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\"\n",
    "output_path  = directory + \"SORTED_multilabel_dataset.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "label_columns = df.columns.difference(['tweet'])\n",
    "df['binary_label'] = df[label_columns].astype(str).agg(''.join, axis=1)\n",
    "sorted_df = df.sort_values(by='binary_label').reset_index(drop=True)\n",
    "sorted_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait', 'Lebanon', \n",
    "                     'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar', 'Saudi_Arabia', \n",
    "                     'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_class_insight(balanced_df):\n",
    "    \n",
    "    counts = {i: (balanced_df[label_columns].sum(axis=1) == i).sum() for i in range(19)}\n",
    "\n",
    "    print(\"Counts of rows by number of active labels:\")\n",
    "    for count, value in counts.items():\n",
    "        print(f\"Class with {count} active labels: {value}\")\n",
    "\n",
    "    label_counts_by_class = {}\n",
    "\n",
    "    \n",
    "    for num_classes in range(1, len(label_columns) + 1):\n",
    "        subset = balanced_df[balanced_df[label_columns].sum(axis=1) == num_classes]\n",
    "        \n",
    "        label_counts = subset[label_columns].sum()\n",
    "        \n",
    "        label_counts_by_class[num_classes] = label_counts.to_dict()\n",
    "\n",
    "    for num_classes, counts in label_counts_by_class.items():\n",
    "        print(f\"\\nFor class group with {num_classes} active labels:\")\n",
    "        for label, count in counts.items():\n",
    "            print(f\"  {label}: {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_balanced_threshold(df, label_columns, threshold=750, uni_label=True):\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for num_classes in range(1, len(label_columns) + 1):\n",
    "        \n",
    "        subset = df[df[label_columns].sum(axis=1) == num_classes]\n",
    "\n",
    "        if num_classes == 1 and uni_label:\n",
    "            balanced_subset = pd.DataFrame()\n",
    "            \n",
    "            \n",
    "            for label in label_columns:\n",
    "                label_rows = subset[subset[label] == 1]\n",
    "                \n",
    "                if len(label_rows) > threshold//15:\n",
    "                    label_rows = resample(label_rows, n_samples=threshold//15, random_state=42, replace=False)\n",
    "                elif len(label_rows) < threshold//15:\n",
    "                    label_rows = resample(label_rows, n_samples=threshold//15, random_state=42, replace=True)\n",
    "                \n",
    "               \n",
    "                balanced_subset = pd.concat([balanced_subset, label_rows], ignore_index=True)\n",
    "            \n",
    "        \n",
    "            balanced_df = pd.concat([balanced_df, balanced_subset], ignore_index=True)\n",
    "        \n",
    "        else:\n",
    "            if len(subset) > threshold:\n",
    "                subset = shuffle(subset).head(threshold)\n",
    "            balanced_df = pd.concat([balanced_df, subset], ignore_index=True)\n",
    "\n",
    "    return balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_dataset_for_active_labels(df):                     \n",
    "    mask = df[label_columns].sum(axis=1).isin([16, 17])\n",
    "    df.loc[mask, label_columns] = 1  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Bahrain</th>\n",
       "      <th>Egypt</th>\n",
       "      <th>Iraq</th>\n",
       "      <th>Jordan</th>\n",
       "      <th>Kuwait</th>\n",
       "      <th>Lebanon</th>\n",
       "      <th>Libya</th>\n",
       "      <th>...</th>\n",
       "      <th>Palestine</th>\n",
       "      <th>Qatar</th>\n",
       "      <th>Saudi_Arabia</th>\n",
       "      <th>Sudan</th>\n",
       "      <th>Syria</th>\n",
       "      <th>Tunisia</th>\n",
       "      <th>UAE</th>\n",
       "      <th>Yemen</th>\n",
       "      <th>Computed</th>\n",
       "      <th>binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10012</td>\n",
       "      <td>تقريبا كلام السيسي بدل يتكلم عربي بيتكلم انجليزي</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>00no000000000000000110012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10032</td>\n",
       "      <td>وای ایشالا که حالش خوبه</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>00no000000000000000110032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10041</td>\n",
       "      <td>اعجبني فيديو علي قوات مدعومه اماراتيا تابعه لنجل</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>00no000000000000000110041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10064</td>\n",
       "      <td>حداقل دیگه حسرت اینکه چرا نگفتم رو نداری</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>00no000000000000000110064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10087</td>\n",
       "      <td>مرعبه كلمه سامحني لان اغلب الاوقات يكون بعدها ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>00no000000000000000110087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  Algeria  Bahrain  \\\n",
       "0  10012   تقريبا كلام السيسي بدل يتكلم عربي بيتكلم انجليزي        0        0   \n",
       "1  10032                            وای ایشالا که حالش خوبه        0        0   \n",
       "2  10041   اعجبني فيديو علي قوات مدعومه اماراتيا تابعه لنجل        0        0   \n",
       "3  10064           حداقل دیگه حسرت اینکه چرا نگفتم رو نداری        0        0   \n",
       "4  10087  مرعبه كلمه سامحني لان اغلب الاوقات يكون بعدها ...        0        0   \n",
       "\n",
       "   Egypt  Iraq  Jordan  Kuwait  Lebanon  Libya  ...  Palestine  Qatar  \\\n",
       "0      0     0       0       0        0      0  ...          0      0   \n",
       "1      0     0       0       0        0      0  ...          0      0   \n",
       "2      0     0       0       0        0      0  ...          0      0   \n",
       "3      0     0       0       0        0      0  ...          0      0   \n",
       "4      0     0       0       0        0      0  ...          0      0   \n",
       "\n",
       "   Saudi_Arabia  Sudan  Syria  Tunisia  UAE  Yemen  Computed  \\\n",
       "0             0      0      0        0    0      1        no   \n",
       "1             0      0      0        0    0      1        no   \n",
       "2             0      0      0        0    0      1        no   \n",
       "3             0      0      0        0    0      1        no   \n",
       "4             0      0      0        0    0      1        no   \n",
       "\n",
       "                binary_label  \n",
       "0  00no000000000000000110012  \n",
       "1  00no000000000000000110032  \n",
       "2  00no000000000000000110041  \n",
       "3  00no000000000000000110064  \n",
       "4  00no000000000000000110087  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of rows by number of active labels:\n",
      "Class with 0 active labels: 0\n",
      "Class with 1 active labels: 41837\n",
      "Class with 2 active labels: 3220\n",
      "Class with 3 active labels: 2329\n",
      "Class with 4 active labels: 2580\n",
      "Class with 5 active labels: 1416\n",
      "Class with 6 active labels: 1709\n",
      "Class with 7 active labels: 1794\n",
      "Class with 8 active labels: 1108\n",
      "Class with 9 active labels: 536\n",
      "Class with 10 active labels: 168\n",
      "Class with 11 active labels: 137\n",
      "Class with 12 active labels: 334\n",
      "Class with 13 active labels: 273\n",
      "Class with 14 active labels: 256\n",
      "Class with 15 active labels: 532\n",
      "Class with 16 active labels: 11\n",
      "Class with 17 active labels: 26\n",
      "Class with 18 active labels: 502\n",
      "\n",
      "For class group with 1 active labels:\n",
      "  Algeria: 2712 occurrences\n",
      "  Bahrain: 417 occurrences\n",
      "  Egypt: 10609 occurrences\n",
      "  Iraq: 6026 occurrences\n",
      "  Jordan: 762 occurrences\n",
      "  Kuwait: 672 occurrences\n",
      "  Lebanon: 1245 occurrences\n",
      "  Libya: 2567 occurrences\n",
      "  Morocco: 2213 occurrences\n",
      "  Oman: 2077 occurrences\n",
      "  Palestine: 794 occurrences\n",
      "  Qatar: 451 occurrences\n",
      "  Saudi_Arabia: 4176 occurrences\n",
      "  Sudan: 858 occurrences\n",
      "  Syria: 1896 occurrences\n",
      "  Tunisia: 1616 occurrences\n",
      "  UAE: 1418 occurrences\n",
      "  Yemen: 1328 occurrences\n",
      "\n",
      "For class group with 2 active labels:\n",
      "  Algeria: 370 occurrences\n",
      "  Bahrain: 54 occurrences\n",
      "  Egypt: 557 occurrences\n",
      "  Iraq: 816 occurrences\n",
      "  Jordan: 243 occurrences\n",
      "  Kuwait: 136 occurrences\n",
      "  Lebanon: 384 occurrences\n",
      "  Libya: 731 occurrences\n",
      "  Morocco: 352 occurrences\n",
      "  Oman: 63 occurrences\n",
      "  Palestine: 215 occurrences\n",
      "  Qatar: 45 occurrences\n",
      "  Saudi_Arabia: 857 occurrences\n",
      "  Sudan: 174 occurrences\n",
      "  Syria: 339 occurrences\n",
      "  Tunisia: 826 occurrences\n",
      "  UAE: 114 occurrences\n",
      "  Yemen: 164 occurrences\n",
      "\n",
      "For class group with 3 active labels:\n",
      "  Algeria: 1262 occurrences\n",
      "  Bahrain: 89 occurrences\n",
      "  Egypt: 235 occurrences\n",
      "  Iraq: 402 occurrences\n",
      "  Jordan: 386 occurrences\n",
      "  Kuwait: 223 occurrences\n",
      "  Lebanon: 317 occurrences\n",
      "  Libya: 378 occurrences\n",
      "  Morocco: 1134 occurrences\n",
      "  Oman: 25 occurrences\n",
      "  Palestine: 300 occurrences\n",
      "  Qatar: 80 occurrences\n",
      "  Saudi_Arabia: 414 occurrences\n",
      "  Sudan: 103 occurrences\n",
      "  Syria: 188 occurrences\n",
      "  Tunisia: 1340 occurrences\n",
      "  UAE: 43 occurrences\n",
      "  Yemen: 68 occurrences\n",
      "\n",
      "For class group with 4 active labels:\n",
      "  Algeria: 280 occurrences\n",
      "  Bahrain: 436 occurrences\n",
      "  Egypt: 111 occurrences\n",
      "  Iraq: 263 occurrences\n",
      "  Jordan: 1737 occurrences\n",
      "  Kuwait: 480 occurrences\n",
      "  Lebanon: 1670 occurrences\n",
      "  Libya: 320 occurrences\n",
      "  Morocco: 269 occurrences\n",
      "  Oman: 26 occurrences\n",
      "  Palestine: 1691 occurrences\n",
      "  Qatar: 425 occurrences\n",
      "  Saudi_Arabia: 573 occurrences\n",
      "  Sudan: 39 occurrences\n",
      "  Syria: 1629 occurrences\n",
      "  Tunisia: 287 occurrences\n",
      "  UAE: 40 occurrences\n",
      "  Yemen: 44 occurrences\n",
      "\n",
      "For class group with 5 active labels:\n",
      "  Algeria: 53 occurrences\n",
      "  Bahrain: 660 occurrences\n",
      "  Egypt: 116 occurrences\n",
      "  Iraq: 977 occurrences\n",
      "  Jordan: 671 occurrences\n",
      "  Kuwait: 694 occurrences\n",
      "  Lebanon: 618 occurrences\n",
      "  Libya: 98 occurrences\n",
      "  Morocco: 49 occurrences\n",
      "  Oman: 131 occurrences\n",
      "  Palestine: 644 occurrences\n",
      "  Qatar: 684 occurrences\n",
      "  Saudi_Arabia: 740 occurrences\n",
      "  Sudan: 23 occurrences\n",
      "  Syria: 628 occurrences\n",
      "  Tunisia: 52 occurrences\n",
      "  UAE: 150 occurrences\n",
      "  Yemen: 92 occurrences\n",
      "\n",
      "For class group with 6 active labels:\n",
      "  Algeria: 6 occurrences\n",
      "  Bahrain: 1402 occurrences\n",
      "  Egypt: 80 occurrences\n",
      "  Iraq: 453 occurrences\n",
      "  Jordan: 351 occurrences\n",
      "  Kuwait: 1427 occurrences\n",
      "  Lebanon: 276 occurrences\n",
      "  Libya: 95 occurrences\n",
      "  Morocco: 9 occurrences\n",
      "  Oman: 1205 occurrences\n",
      "  Palestine: 300 occurrences\n",
      "  Qatar: 1403 occurrences\n",
      "  Saudi_Arabia: 1616 occurrences\n",
      "  Sudan: 18 occurrences\n",
      "  Syria: 284 occurrences\n",
      "  Tunisia: 5 occurrences\n",
      "  UAE: 1133 occurrences\n",
      "  Yemen: 191 occurrences\n",
      "\n",
      "For class group with 7 active labels:\n",
      "  Algeria: 7 occurrences\n",
      "  Bahrain: 1654 occurrences\n",
      "  Egypt: 100 occurrences\n",
      "  Iraq: 528 occurrences\n",
      "  Jordan: 220 occurrences\n",
      "  Kuwait: 1684 occurrences\n",
      "  Lebanon: 143 occurrences\n",
      "  Libya: 79 occurrences\n",
      "  Morocco: 6 occurrences\n",
      "  Oman: 1574 occurrences\n",
      "  Palestine: 160 occurrences\n",
      "  Qatar: 1646 occurrences\n",
      "  Saudi_Arabia: 1759 occurrences\n",
      "  Sudan: 27 occurrences\n",
      "  Syria: 153 occurrences\n",
      "  Tunisia: 8 occurrences\n",
      "  UAE: 1527 occurrences\n",
      "  Yemen: 1283 occurrences\n",
      "\n",
      "For class group with 8 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 1052 occurrences\n",
      "  Egypt: 84 occurrences\n",
      "  Iraq: 891 occurrences\n",
      "  Jordan: 259 occurrences\n",
      "  Kuwait: 1066 occurrences\n",
      "  Lebanon: 116 occurrences\n",
      "  Libya: 100 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 974 occurrences\n",
      "  Palestine: 125 occurrences\n",
      "  Qatar: 1037 occurrences\n",
      "  Saudi_Arabia: 1099 occurrences\n",
      "  Sudan: 26 occurrences\n",
      "  Syria: 117 occurrences\n",
      "  Tunisia: 5 occurrences\n",
      "  UAE: 954 occurrences\n",
      "  Yemen: 950 occurrences\n",
      "\n",
      "For class group with 9 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 509 occurrences\n",
      "  Egypt: 66 occurrences\n",
      "  Iraq: 488 occurrences\n",
      "  Jordan: 328 occurrences\n",
      "  Kuwait: 516 occurrences\n",
      "  Lebanon: 176 occurrences\n",
      "  Libya: 206 occurrences\n",
      "  Morocco: 6 occurrences\n",
      "  Oman: 362 occurrences\n",
      "  Palestine: 188 occurrences\n",
      "  Qatar: 501 occurrences\n",
      "  Saudi_Arabia: 534 occurrences\n",
      "  Sudan: 37 occurrences\n",
      "  Syria: 181 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 354 occurrences\n",
      "  Yemen: 366 occurrences\n",
      "\n",
      "For class group with 10 active labels:\n",
      "  Algeria: 3 occurrences\n",
      "  Bahrain: 159 occurrences\n",
      "  Egypt: 46 occurrences\n",
      "  Iraq: 128 occurrences\n",
      "  Jordan: 153 occurrences\n",
      "  Kuwait: 164 occurrences\n",
      "  Lebanon: 89 occurrences\n",
      "  Libya: 62 occurrences\n",
      "  Morocco: 3 occurrences\n",
      "  Oman: 108 occurrences\n",
      "  Palestine: 105 occurrences\n",
      "  Qatar: 156 occurrences\n",
      "  Saudi_Arabia: 166 occurrences\n",
      "  Sudan: 27 occurrences\n",
      "  Syria: 98 occurrences\n",
      "  Tunisia: 3 occurrences\n",
      "  UAE: 91 occurrences\n",
      "  Yemen: 119 occurrences\n",
      "\n",
      "For class group with 11 active labels:\n",
      "  Algeria: 5 occurrences\n",
      "  Bahrain: 134 occurrences\n",
      "  Egypt: 41 occurrences\n",
      "  Iraq: 108 occurrences\n",
      "  Jordan: 119 occurrences\n",
      "  Kuwait: 134 occurrences\n",
      "  Lebanon: 97 occurrences\n",
      "  Libya: 40 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 110 occurrences\n",
      "  Palestine: 106 occurrences\n",
      "  Qatar: 134 occurrences\n",
      "  Saudi_Arabia: 136 occurrences\n",
      "  Sudan: 29 occurrences\n",
      "  Syria: 106 occurrences\n",
      "  Tunisia: 6 occurrences\n",
      "  UAE: 98 occurrences\n",
      "  Yemen: 99 occurrences\n",
      "\n",
      "For class group with 12 active labels:\n",
      "  Algeria: 7 occurrences\n",
      "  Bahrain: 332 occurrences\n",
      "  Egypt: 58 occurrences\n",
      "  Iraq: 308 occurrences\n",
      "  Jordan: 328 occurrences\n",
      "  Kuwait: 332 occurrences\n",
      "  Lebanon: 318 occurrences\n",
      "  Libya: 29 occurrences\n",
      "  Morocco: 7 occurrences\n",
      "  Oman: 324 occurrences\n",
      "  Palestine: 323 occurrences\n",
      "  Qatar: 331 occurrences\n",
      "  Saudi_Arabia: 334 occurrences\n",
      "  Sudan: 19 occurrences\n",
      "  Syria: 317 occurrences\n",
      "  Tunisia: 8 occurrences\n",
      "  UAE: 322 occurrences\n",
      "  Yemen: 311 occurrences\n",
      "\n",
      "For class group with 13 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 273 occurrences\n",
      "  Egypt: 179 occurrences\n",
      "  Iraq: 258 occurrences\n",
      "  Jordan: 273 occurrences\n",
      "  Kuwait: 273 occurrences\n",
      "  Lebanon: 264 occurrences\n",
      "  Libya: 91 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 272 occurrences\n",
      "  Palestine: 273 occurrences\n",
      "  Qatar: 273 occurrences\n",
      "  Saudi_Arabia: 273 occurrences\n",
      "  Sudan: 37 occurrences\n",
      "  Syria: 270 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 272 occurrences\n",
      "  Yemen: 268 occurrences\n",
      "\n",
      "For class group with 14 active labels:\n",
      "  Algeria: 1 occurrences\n",
      "  Bahrain: 256 occurrences\n",
      "  Egypt: 243 occurrences\n",
      "  Iraq: 252 occurrences\n",
      "  Jordan: 256 occurrences\n",
      "  Kuwait: 256 occurrences\n",
      "  Lebanon: 255 occurrences\n",
      "  Libya: 128 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 256 occurrences\n",
      "  Palestine: 256 occurrences\n",
      "  Qatar: 256 occurrences\n",
      "  Saudi_Arabia: 256 occurrences\n",
      "  Sudan: 149 occurrences\n",
      "  Syria: 255 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 256 occurrences\n",
      "  Yemen: 251 occurrences\n",
      "\n",
      "For class group with 15 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 532 occurrences\n",
      "  Egypt: 532 occurrences\n",
      "  Iraq: 531 occurrences\n",
      "  Jordan: 532 occurrences\n",
      "  Kuwait: 532 occurrences\n",
      "  Lebanon: 532 occurrences\n",
      "  Libya: 530 occurrences\n",
      "  Morocco: 1 occurrences\n",
      "  Oman: 532 occurrences\n",
      "  Palestine: 532 occurrences\n",
      "  Qatar: 532 occurrences\n",
      "  Saudi_Arabia: 532 occurrences\n",
      "  Sudan: 530 occurrences\n",
      "  Syria: 532 occurrences\n",
      "  Tunisia: 1 occurrences\n",
      "  UAE: 532 occurrences\n",
      "  Yemen: 531 occurrences\n",
      "\n",
      "For class group with 16 active labels:\n",
      "  Algeria: 9 occurrences\n",
      "  Bahrain: 11 occurrences\n",
      "  Egypt: 10 occurrences\n",
      "  Iraq: 11 occurrences\n",
      "  Jordan: 11 occurrences\n",
      "  Kuwait: 11 occurrences\n",
      "  Lebanon: 11 occurrences\n",
      "  Libya: 11 occurrences\n",
      "  Morocco: 1 occurrences\n",
      "  Oman: 11 occurrences\n",
      "  Palestine: 11 occurrences\n",
      "  Qatar: 11 occurrences\n",
      "  Saudi_Arabia: 11 occurrences\n",
      "  Sudan: 10 occurrences\n",
      "  Syria: 11 occurrences\n",
      "  Tunisia: 3 occurrences\n",
      "  UAE: 11 occurrences\n",
      "  Yemen: 11 occurrences\n",
      "\n",
      "For class group with 17 active labels:\n",
      "  Algeria: 26 occurrences\n",
      "  Bahrain: 26 occurrences\n",
      "  Egypt: 26 occurrences\n",
      "  Iraq: 25 occurrences\n",
      "  Jordan: 26 occurrences\n",
      "  Kuwait: 26 occurrences\n",
      "  Lebanon: 26 occurrences\n",
      "  Libya: 24 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 26 occurrences\n",
      "  Palestine: 26 occurrences\n",
      "  Qatar: 26 occurrences\n",
      "  Saudi_Arabia: 25 occurrences\n",
      "  Sudan: 26 occurrences\n",
      "  Syria: 26 occurrences\n",
      "  Tunisia: 25 occurrences\n",
      "  UAE: 26 occurrences\n",
      "  Yemen: 26 occurrences\n",
      "\n",
      "For class group with 18 active labels:\n",
      "  Algeria: 502 occurrences\n",
      "  Bahrain: 502 occurrences\n",
      "  Egypt: 502 occurrences\n",
      "  Iraq: 502 occurrences\n",
      "  Jordan: 502 occurrences\n",
      "  Kuwait: 502 occurrences\n",
      "  Lebanon: 502 occurrences\n",
      "  Libya: 502 occurrences\n",
      "  Morocco: 502 occurrences\n",
      "  Oman: 502 occurrences\n",
      "  Palestine: 502 occurrences\n",
      "  Qatar: 502 occurrences\n",
      "  Saudi_Arabia: 502 occurrences\n",
      "  Sudan: 502 occurrences\n",
      "  Syria: 502 occurrences\n",
      "  Tunisia: 502 occurrences\n",
      "  UAE: 502 occurrences\n",
      "  Yemen: 502 occurrences\n"
     ]
    }
   ],
   "source": [
    "get_class_insight(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = modify_dataset_for_active_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of rows by number of active labels:\n",
      "Class with 0 active labels: 0\n",
      "Class with 1 active labels: 41837\n",
      "Class with 2 active labels: 3220\n",
      "Class with 3 active labels: 2329\n",
      "Class with 4 active labels: 2580\n",
      "Class with 5 active labels: 1416\n",
      "Class with 6 active labels: 1709\n",
      "Class with 7 active labels: 1794\n",
      "Class with 8 active labels: 1108\n",
      "Class with 9 active labels: 536\n",
      "Class with 10 active labels: 168\n",
      "Class with 11 active labels: 137\n",
      "Class with 12 active labels: 334\n",
      "Class with 13 active labels: 273\n",
      "Class with 14 active labels: 256\n",
      "Class with 15 active labels: 532\n",
      "Class with 16 active labels: 0\n",
      "Class with 17 active labels: 0\n",
      "Class with 18 active labels: 539\n",
      "\n",
      "For class group with 1 active labels:\n",
      "  Algeria: 2712 occurrences\n",
      "  Bahrain: 417 occurrences\n",
      "  Egypt: 10609 occurrences\n",
      "  Iraq: 6026 occurrences\n",
      "  Jordan: 762 occurrences\n",
      "  Kuwait: 672 occurrences\n",
      "  Lebanon: 1245 occurrences\n",
      "  Libya: 2567 occurrences\n",
      "  Morocco: 2213 occurrences\n",
      "  Oman: 2077 occurrences\n",
      "  Palestine: 794 occurrences\n",
      "  Qatar: 451 occurrences\n",
      "  Saudi_Arabia: 4176 occurrences\n",
      "  Sudan: 858 occurrences\n",
      "  Syria: 1896 occurrences\n",
      "  Tunisia: 1616 occurrences\n",
      "  UAE: 1418 occurrences\n",
      "  Yemen: 1328 occurrences\n",
      "\n",
      "For class group with 2 active labels:\n",
      "  Algeria: 370 occurrences\n",
      "  Bahrain: 54 occurrences\n",
      "  Egypt: 557 occurrences\n",
      "  Iraq: 816 occurrences\n",
      "  Jordan: 243 occurrences\n",
      "  Kuwait: 136 occurrences\n",
      "  Lebanon: 384 occurrences\n",
      "  Libya: 731 occurrences\n",
      "  Morocco: 352 occurrences\n",
      "  Oman: 63 occurrences\n",
      "  Palestine: 215 occurrences\n",
      "  Qatar: 45 occurrences\n",
      "  Saudi_Arabia: 857 occurrences\n",
      "  Sudan: 174 occurrences\n",
      "  Syria: 339 occurrences\n",
      "  Tunisia: 826 occurrences\n",
      "  UAE: 114 occurrences\n",
      "  Yemen: 164 occurrences\n",
      "\n",
      "For class group with 3 active labels:\n",
      "  Algeria: 1262 occurrences\n",
      "  Bahrain: 89 occurrences\n",
      "  Egypt: 235 occurrences\n",
      "  Iraq: 402 occurrences\n",
      "  Jordan: 386 occurrences\n",
      "  Kuwait: 223 occurrences\n",
      "  Lebanon: 317 occurrences\n",
      "  Libya: 378 occurrences\n",
      "  Morocco: 1134 occurrences\n",
      "  Oman: 25 occurrences\n",
      "  Palestine: 300 occurrences\n",
      "  Qatar: 80 occurrences\n",
      "  Saudi_Arabia: 414 occurrences\n",
      "  Sudan: 103 occurrences\n",
      "  Syria: 188 occurrences\n",
      "  Tunisia: 1340 occurrences\n",
      "  UAE: 43 occurrences\n",
      "  Yemen: 68 occurrences\n",
      "\n",
      "For class group with 4 active labels:\n",
      "  Algeria: 280 occurrences\n",
      "  Bahrain: 436 occurrences\n",
      "  Egypt: 111 occurrences\n",
      "  Iraq: 263 occurrences\n",
      "  Jordan: 1737 occurrences\n",
      "  Kuwait: 480 occurrences\n",
      "  Lebanon: 1670 occurrences\n",
      "  Libya: 320 occurrences\n",
      "  Morocco: 269 occurrences\n",
      "  Oman: 26 occurrences\n",
      "  Palestine: 1691 occurrences\n",
      "  Qatar: 425 occurrences\n",
      "  Saudi_Arabia: 573 occurrences\n",
      "  Sudan: 39 occurrences\n",
      "  Syria: 1629 occurrences\n",
      "  Tunisia: 287 occurrences\n",
      "  UAE: 40 occurrences\n",
      "  Yemen: 44 occurrences\n",
      "\n",
      "For class group with 5 active labels:\n",
      "  Algeria: 53 occurrences\n",
      "  Bahrain: 660 occurrences\n",
      "  Egypt: 116 occurrences\n",
      "  Iraq: 977 occurrences\n",
      "  Jordan: 671 occurrences\n",
      "  Kuwait: 694 occurrences\n",
      "  Lebanon: 618 occurrences\n",
      "  Libya: 98 occurrences\n",
      "  Morocco: 49 occurrences\n",
      "  Oman: 131 occurrences\n",
      "  Palestine: 644 occurrences\n",
      "  Qatar: 684 occurrences\n",
      "  Saudi_Arabia: 740 occurrences\n",
      "  Sudan: 23 occurrences\n",
      "  Syria: 628 occurrences\n",
      "  Tunisia: 52 occurrences\n",
      "  UAE: 150 occurrences\n",
      "  Yemen: 92 occurrences\n",
      "\n",
      "For class group with 6 active labels:\n",
      "  Algeria: 6 occurrences\n",
      "  Bahrain: 1402 occurrences\n",
      "  Egypt: 80 occurrences\n",
      "  Iraq: 453 occurrences\n",
      "  Jordan: 351 occurrences\n",
      "  Kuwait: 1427 occurrences\n",
      "  Lebanon: 276 occurrences\n",
      "  Libya: 95 occurrences\n",
      "  Morocco: 9 occurrences\n",
      "  Oman: 1205 occurrences\n",
      "  Palestine: 300 occurrences\n",
      "  Qatar: 1403 occurrences\n",
      "  Saudi_Arabia: 1616 occurrences\n",
      "  Sudan: 18 occurrences\n",
      "  Syria: 284 occurrences\n",
      "  Tunisia: 5 occurrences\n",
      "  UAE: 1133 occurrences\n",
      "  Yemen: 191 occurrences\n",
      "\n",
      "For class group with 7 active labels:\n",
      "  Algeria: 7 occurrences\n",
      "  Bahrain: 1654 occurrences\n",
      "  Egypt: 100 occurrences\n",
      "  Iraq: 528 occurrences\n",
      "  Jordan: 220 occurrences\n",
      "  Kuwait: 1684 occurrences\n",
      "  Lebanon: 143 occurrences\n",
      "  Libya: 79 occurrences\n",
      "  Morocco: 6 occurrences\n",
      "  Oman: 1574 occurrences\n",
      "  Palestine: 160 occurrences\n",
      "  Qatar: 1646 occurrences\n",
      "  Saudi_Arabia: 1759 occurrences\n",
      "  Sudan: 27 occurrences\n",
      "  Syria: 153 occurrences\n",
      "  Tunisia: 8 occurrences\n",
      "  UAE: 1527 occurrences\n",
      "  Yemen: 1283 occurrences\n",
      "\n",
      "For class group with 8 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 1052 occurrences\n",
      "  Egypt: 84 occurrences\n",
      "  Iraq: 891 occurrences\n",
      "  Jordan: 259 occurrences\n",
      "  Kuwait: 1066 occurrences\n",
      "  Lebanon: 116 occurrences\n",
      "  Libya: 100 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 974 occurrences\n",
      "  Palestine: 125 occurrences\n",
      "  Qatar: 1037 occurrences\n",
      "  Saudi_Arabia: 1099 occurrences\n",
      "  Sudan: 26 occurrences\n",
      "  Syria: 117 occurrences\n",
      "  Tunisia: 5 occurrences\n",
      "  UAE: 954 occurrences\n",
      "  Yemen: 950 occurrences\n",
      "\n",
      "For class group with 9 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 509 occurrences\n",
      "  Egypt: 66 occurrences\n",
      "  Iraq: 488 occurrences\n",
      "  Jordan: 328 occurrences\n",
      "  Kuwait: 516 occurrences\n",
      "  Lebanon: 176 occurrences\n",
      "  Libya: 206 occurrences\n",
      "  Morocco: 6 occurrences\n",
      "  Oman: 362 occurrences\n",
      "  Palestine: 188 occurrences\n",
      "  Qatar: 501 occurrences\n",
      "  Saudi_Arabia: 534 occurrences\n",
      "  Sudan: 37 occurrences\n",
      "  Syria: 181 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 354 occurrences\n",
      "  Yemen: 366 occurrences\n",
      "\n",
      "For class group with 10 active labels:\n",
      "  Algeria: 3 occurrences\n",
      "  Bahrain: 159 occurrences\n",
      "  Egypt: 46 occurrences\n",
      "  Iraq: 128 occurrences\n",
      "  Jordan: 153 occurrences\n",
      "  Kuwait: 164 occurrences\n",
      "  Lebanon: 89 occurrences\n",
      "  Libya: 62 occurrences\n",
      "  Morocco: 3 occurrences\n",
      "  Oman: 108 occurrences\n",
      "  Palestine: 105 occurrences\n",
      "  Qatar: 156 occurrences\n",
      "  Saudi_Arabia: 166 occurrences\n",
      "  Sudan: 27 occurrences\n",
      "  Syria: 98 occurrences\n",
      "  Tunisia: 3 occurrences\n",
      "  UAE: 91 occurrences\n",
      "  Yemen: 119 occurrences\n",
      "\n",
      "For class group with 11 active labels:\n",
      "  Algeria: 5 occurrences\n",
      "  Bahrain: 134 occurrences\n",
      "  Egypt: 41 occurrences\n",
      "  Iraq: 108 occurrences\n",
      "  Jordan: 119 occurrences\n",
      "  Kuwait: 134 occurrences\n",
      "  Lebanon: 97 occurrences\n",
      "  Libya: 40 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 110 occurrences\n",
      "  Palestine: 106 occurrences\n",
      "  Qatar: 134 occurrences\n",
      "  Saudi_Arabia: 136 occurrences\n",
      "  Sudan: 29 occurrences\n",
      "  Syria: 106 occurrences\n",
      "  Tunisia: 6 occurrences\n",
      "  UAE: 98 occurrences\n",
      "  Yemen: 99 occurrences\n",
      "\n",
      "For class group with 12 active labels:\n",
      "  Algeria: 7 occurrences\n",
      "  Bahrain: 332 occurrences\n",
      "  Egypt: 58 occurrences\n",
      "  Iraq: 308 occurrences\n",
      "  Jordan: 328 occurrences\n",
      "  Kuwait: 332 occurrences\n",
      "  Lebanon: 318 occurrences\n",
      "  Libya: 29 occurrences\n",
      "  Morocco: 7 occurrences\n",
      "  Oman: 324 occurrences\n",
      "  Palestine: 323 occurrences\n",
      "  Qatar: 331 occurrences\n",
      "  Saudi_Arabia: 334 occurrences\n",
      "  Sudan: 19 occurrences\n",
      "  Syria: 317 occurrences\n",
      "  Tunisia: 8 occurrences\n",
      "  UAE: 322 occurrences\n",
      "  Yemen: 311 occurrences\n",
      "\n",
      "For class group with 13 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 273 occurrences\n",
      "  Egypt: 179 occurrences\n",
      "  Iraq: 258 occurrences\n",
      "  Jordan: 273 occurrences\n",
      "  Kuwait: 273 occurrences\n",
      "  Lebanon: 264 occurrences\n",
      "  Libya: 91 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 272 occurrences\n",
      "  Palestine: 273 occurrences\n",
      "  Qatar: 273 occurrences\n",
      "  Saudi_Arabia: 273 occurrences\n",
      "  Sudan: 37 occurrences\n",
      "  Syria: 270 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 272 occurrences\n",
      "  Yemen: 268 occurrences\n",
      "\n",
      "For class group with 14 active labels:\n",
      "  Algeria: 1 occurrences\n",
      "  Bahrain: 256 occurrences\n",
      "  Egypt: 243 occurrences\n",
      "  Iraq: 252 occurrences\n",
      "  Jordan: 256 occurrences\n",
      "  Kuwait: 256 occurrences\n",
      "  Lebanon: 255 occurrences\n",
      "  Libya: 128 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 256 occurrences\n",
      "  Palestine: 256 occurrences\n",
      "  Qatar: 256 occurrences\n",
      "  Saudi_Arabia: 256 occurrences\n",
      "  Sudan: 149 occurrences\n",
      "  Syria: 255 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 256 occurrences\n",
      "  Yemen: 251 occurrences\n",
      "\n",
      "For class group with 15 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 532 occurrences\n",
      "  Egypt: 532 occurrences\n",
      "  Iraq: 531 occurrences\n",
      "  Jordan: 532 occurrences\n",
      "  Kuwait: 532 occurrences\n",
      "  Lebanon: 532 occurrences\n",
      "  Libya: 530 occurrences\n",
      "  Morocco: 1 occurrences\n",
      "  Oman: 532 occurrences\n",
      "  Palestine: 532 occurrences\n",
      "  Qatar: 532 occurrences\n",
      "  Saudi_Arabia: 532 occurrences\n",
      "  Sudan: 530 occurrences\n",
      "  Syria: 532 occurrences\n",
      "  Tunisia: 1 occurrences\n",
      "  UAE: 532 occurrences\n",
      "  Yemen: 531 occurrences\n",
      "\n",
      "For class group with 16 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 0 occurrences\n",
      "  Egypt: 0 occurrences\n",
      "  Iraq: 0 occurrences\n",
      "  Jordan: 0 occurrences\n",
      "  Kuwait: 0 occurrences\n",
      "  Lebanon: 0 occurrences\n",
      "  Libya: 0 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 0 occurrences\n",
      "  Palestine: 0 occurrences\n",
      "  Qatar: 0 occurrences\n",
      "  Saudi_Arabia: 0 occurrences\n",
      "  Sudan: 0 occurrences\n",
      "  Syria: 0 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 0 occurrences\n",
      "  Yemen: 0 occurrences\n",
      "\n",
      "For class group with 17 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 0 occurrences\n",
      "  Egypt: 0 occurrences\n",
      "  Iraq: 0 occurrences\n",
      "  Jordan: 0 occurrences\n",
      "  Kuwait: 0 occurrences\n",
      "  Lebanon: 0 occurrences\n",
      "  Libya: 0 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 0 occurrences\n",
      "  Palestine: 0 occurrences\n",
      "  Qatar: 0 occurrences\n",
      "  Saudi_Arabia: 0 occurrences\n",
      "  Sudan: 0 occurrences\n",
      "  Syria: 0 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 0 occurrences\n",
      "  Yemen: 0 occurrences\n",
      "\n",
      "For class group with 18 active labels:\n",
      "  Algeria: 539 occurrences\n",
      "  Bahrain: 539 occurrences\n",
      "  Egypt: 539 occurrences\n",
      "  Iraq: 539 occurrences\n",
      "  Jordan: 539 occurrences\n",
      "  Kuwait: 539 occurrences\n",
      "  Lebanon: 539 occurrences\n",
      "  Libya: 539 occurrences\n",
      "  Morocco: 539 occurrences\n",
      "  Oman: 539 occurrences\n",
      "  Palestine: 539 occurrences\n",
      "  Qatar: 539 occurrences\n",
      "  Saudi_Arabia: 539 occurrences\n",
      "  Sudan: 539 occurrences\n",
      "  Syria: 539 occurrences\n",
      "  Tunisia: 539 occurrences\n",
      "  UAE: 539 occurrences\n",
      "  Yemen: 539 occurrences\n"
     ]
    }
   ],
   "source": [
    "get_class_insight(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = split_balanced_threshold(df, label_columns, threshold=500, uni_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of rows by number of active labels:\n",
      "Class with 0 active labels: 0\n",
      "Class with 1 active labels: 594\n",
      "Class with 2 active labels: 500\n",
      "Class with 3 active labels: 500\n",
      "Class with 4 active labels: 500\n",
      "Class with 5 active labels: 500\n",
      "Class with 6 active labels: 500\n",
      "Class with 7 active labels: 500\n",
      "Class with 8 active labels: 500\n",
      "Class with 9 active labels: 500\n",
      "Class with 10 active labels: 168\n",
      "Class with 11 active labels: 137\n",
      "Class with 12 active labels: 334\n",
      "Class with 13 active labels: 273\n",
      "Class with 14 active labels: 256\n",
      "Class with 15 active labels: 500\n",
      "Class with 16 active labels: 0\n",
      "Class with 17 active labels: 0\n",
      "Class with 18 active labels: 500\n",
      "\n",
      "For class group with 1 active labels:\n",
      "  Algeria: 33 occurrences\n",
      "  Bahrain: 33 occurrences\n",
      "  Egypt: 33 occurrences\n",
      "  Iraq: 33 occurrences\n",
      "  Jordan: 33 occurrences\n",
      "  Kuwait: 33 occurrences\n",
      "  Lebanon: 33 occurrences\n",
      "  Libya: 33 occurrences\n",
      "  Morocco: 33 occurrences\n",
      "  Oman: 33 occurrences\n",
      "  Palestine: 33 occurrences\n",
      "  Qatar: 33 occurrences\n",
      "  Saudi_Arabia: 33 occurrences\n",
      "  Sudan: 33 occurrences\n",
      "  Syria: 33 occurrences\n",
      "  Tunisia: 33 occurrences\n",
      "  UAE: 33 occurrences\n",
      "  Yemen: 33 occurrences\n",
      "\n",
      "For class group with 2 active labels:\n",
      "  Algeria: 57 occurrences\n",
      "  Bahrain: 7 occurrences\n",
      "  Egypt: 88 occurrences\n",
      "  Iraq: 117 occurrences\n",
      "  Jordan: 40 occurrences\n",
      "  Kuwait: 12 occurrences\n",
      "  Lebanon: 51 occurrences\n",
      "  Libya: 114 occurrences\n",
      "  Morocco: 56 occurrences\n",
      "  Oman: 13 occurrences\n",
      "  Palestine: 44 occurrences\n",
      "  Qatar: 8 occurrences\n",
      "  Saudi_Arabia: 136 occurrences\n",
      "  Sudan: 29 occurrences\n",
      "  Syria: 48 occurrences\n",
      "  Tunisia: 136 occurrences\n",
      "  UAE: 22 occurrences\n",
      "  Yemen: 22 occurrences\n",
      "\n",
      "For class group with 3 active labels:\n",
      "  Algeria: 267 occurrences\n",
      "  Bahrain: 18 occurrences\n",
      "  Egypt: 58 occurrences\n",
      "  Iraq: 92 occurrences\n",
      "  Jordan: 80 occurrences\n",
      "  Kuwait: 41 occurrences\n",
      "  Lebanon: 65 occurrences\n",
      "  Libya: 90 occurrences\n",
      "  Morocco: 235 occurrences\n",
      "  Oman: 5 occurrences\n",
      "  Palestine: 65 occurrences\n",
      "  Qatar: 14 occurrences\n",
      "  Saudi_Arabia: 93 occurrences\n",
      "  Sudan: 24 occurrences\n",
      "  Syria: 37 occurrences\n",
      "  Tunisia: 289 occurrences\n",
      "  UAE: 8 occurrences\n",
      "  Yemen: 19 occurrences\n",
      "\n",
      "For class group with 4 active labels:\n",
      "  Algeria: 53 occurrences\n",
      "  Bahrain: 75 occurrences\n",
      "  Egypt: 29 occurrences\n",
      "  Iraq: 39 occurrences\n",
      "  Jordan: 346 occurrences\n",
      "  Kuwait: 84 occurrences\n",
      "  Lebanon: 332 occurrences\n",
      "  Libya: 63 occurrences\n",
      "  Morocco: 53 occurrences\n",
      "  Oman: 7 occurrences\n",
      "  Palestine: 339 occurrences\n",
      "  Qatar: 75 occurrences\n",
      "  Saudi_Arabia: 101 occurrences\n",
      "  Sudan: 8 occurrences\n",
      "  Syria: 325 occurrences\n",
      "  Tunisia: 56 occurrences\n",
      "  UAE: 7 occurrences\n",
      "  Yemen: 8 occurrences\n",
      "\n",
      "For class group with 5 active labels:\n",
      "  Algeria: 21 occurrences\n",
      "  Bahrain: 214 occurrences\n",
      "  Egypt: 39 occurrences\n",
      "  Iraq: 368 occurrences\n",
      "  Jordan: 250 occurrences\n",
      "  Kuwait: 224 occurrences\n",
      "  Lebanon: 237 occurrences\n",
      "  Libya: 38 occurrences\n",
      "  Morocco: 20 occurrences\n",
      "  Oman: 44 occurrences\n",
      "  Palestine: 242 occurrences\n",
      "  Qatar: 219 occurrences\n",
      "  Saudi_Arabia: 245 occurrences\n",
      "  Sudan: 7 occurrences\n",
      "  Syria: 242 occurrences\n",
      "  Tunisia: 23 occurrences\n",
      "  UAE: 42 occurrences\n",
      "  Yemen: 25 occurrences\n",
      "\n",
      "For class group with 6 active labels:\n",
      "  Algeria: 1 occurrences\n",
      "  Bahrain: 424 occurrences\n",
      "  Egypt: 18 occurrences\n",
      "  Iraq: 121 occurrences\n",
      "  Jordan: 84 occurrences\n",
      "  Kuwait: 436 occurrences\n",
      "  Lebanon: 65 occurrences\n",
      "  Libya: 26 occurrences\n",
      "  Morocco: 2 occurrences\n",
      "  Oman: 367 occurrences\n",
      "  Palestine: 75 occurrences\n",
      "  Qatar: 426 occurrences\n",
      "  Saudi_Arabia: 483 occurrences\n",
      "  Sudan: 3 occurrences\n",
      "  Syria: 68 occurrences\n",
      "  Tunisia: 1 occurrences\n",
      "  UAE: 345 occurrences\n",
      "  Yemen: 55 occurrences\n",
      "\n",
      "For class group with 7 active labels:\n",
      "  Algeria: 1 occurrences\n",
      "  Bahrain: 449 occurrences\n",
      "  Egypt: 35 occurrences\n",
      "  Iraq: 145 occurrences\n",
      "  Jordan: 72 occurrences\n",
      "  Kuwait: 460 occurrences\n",
      "  Lebanon: 53 occurrences\n",
      "  Libya: 31 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 427 occurrences\n",
      "  Palestine: 57 occurrences\n",
      "  Qatar: 446 occurrences\n",
      "  Saudi_Arabia: 484 occurrences\n",
      "  Sudan: 9 occurrences\n",
      "  Syria: 58 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 423 occurrences\n",
      "  Yemen: 350 occurrences\n",
      "\n",
      "For class group with 8 active labels:\n",
      "  Algeria: 2 occurrences\n",
      "  Bahrain: 479 occurrences\n",
      "  Egypt: 31 occurrences\n",
      "  Iraq: 402 occurrences\n",
      "  Jordan: 117 occurrences\n",
      "  Kuwait: 487 occurrences\n",
      "  Lebanon: 55 occurrences\n",
      "  Libya: 36 occurrences\n",
      "  Morocco: 2 occurrences\n",
      "  Oman: 438 occurrences\n",
      "  Palestine: 59 occurrences\n",
      "  Qatar: 471 occurrences\n",
      "  Saudi_Arabia: 497 occurrences\n",
      "  Sudan: 7 occurrences\n",
      "  Syria: 54 occurrences\n",
      "  Tunisia: 3 occurrences\n",
      "  UAE: 430 occurrences\n",
      "  Yemen: 430 occurrences\n",
      "\n",
      "For class group with 9 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 475 occurrences\n",
      "  Egypt: 60 occurrences\n",
      "  Iraq: 457 occurrences\n",
      "  Jordan: 307 occurrences\n",
      "  Kuwait: 482 occurrences\n",
      "  Lebanon: 163 occurrences\n",
      "  Libya: 191 occurrences\n",
      "  Morocco: 6 occurrences\n",
      "  Oman: 338 occurrences\n",
      "  Palestine: 175 occurrences\n",
      "  Qatar: 467 occurrences\n",
      "  Saudi_Arabia: 498 occurrences\n",
      "  Sudan: 34 occurrences\n",
      "  Syria: 168 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 330 occurrences\n",
      "  Yemen: 343 occurrences\n",
      "\n",
      "For class group with 10 active labels:\n",
      "  Algeria: 3 occurrences\n",
      "  Bahrain: 159 occurrences\n",
      "  Egypt: 46 occurrences\n",
      "  Iraq: 128 occurrences\n",
      "  Jordan: 153 occurrences\n",
      "  Kuwait: 164 occurrences\n",
      "  Lebanon: 89 occurrences\n",
      "  Libya: 62 occurrences\n",
      "  Morocco: 3 occurrences\n",
      "  Oman: 108 occurrences\n",
      "  Palestine: 105 occurrences\n",
      "  Qatar: 156 occurrences\n",
      "  Saudi_Arabia: 166 occurrences\n",
      "  Sudan: 27 occurrences\n",
      "  Syria: 98 occurrences\n",
      "  Tunisia: 3 occurrences\n",
      "  UAE: 91 occurrences\n",
      "  Yemen: 119 occurrences\n",
      "\n",
      "For class group with 11 active labels:\n",
      "  Algeria: 5 occurrences\n",
      "  Bahrain: 134 occurrences\n",
      "  Egypt: 41 occurrences\n",
      "  Iraq: 108 occurrences\n",
      "  Jordan: 119 occurrences\n",
      "  Kuwait: 134 occurrences\n",
      "  Lebanon: 97 occurrences\n",
      "  Libya: 40 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 110 occurrences\n",
      "  Palestine: 106 occurrences\n",
      "  Qatar: 134 occurrences\n",
      "  Saudi_Arabia: 136 occurrences\n",
      "  Sudan: 29 occurrences\n",
      "  Syria: 106 occurrences\n",
      "  Tunisia: 6 occurrences\n",
      "  UAE: 98 occurrences\n",
      "  Yemen: 99 occurrences\n",
      "\n",
      "For class group with 12 active labels:\n",
      "  Algeria: 7 occurrences\n",
      "  Bahrain: 332 occurrences\n",
      "  Egypt: 58 occurrences\n",
      "  Iraq: 308 occurrences\n",
      "  Jordan: 328 occurrences\n",
      "  Kuwait: 332 occurrences\n",
      "  Lebanon: 318 occurrences\n",
      "  Libya: 29 occurrences\n",
      "  Morocco: 7 occurrences\n",
      "  Oman: 324 occurrences\n",
      "  Palestine: 323 occurrences\n",
      "  Qatar: 331 occurrences\n",
      "  Saudi_Arabia: 334 occurrences\n",
      "  Sudan: 19 occurrences\n",
      "  Syria: 317 occurrences\n",
      "  Tunisia: 8 occurrences\n",
      "  UAE: 322 occurrences\n",
      "  Yemen: 311 occurrences\n",
      "\n",
      "For class group with 13 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 273 occurrences\n",
      "  Egypt: 179 occurrences\n",
      "  Iraq: 258 occurrences\n",
      "  Jordan: 273 occurrences\n",
      "  Kuwait: 273 occurrences\n",
      "  Lebanon: 264 occurrences\n",
      "  Libya: 91 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 272 occurrences\n",
      "  Palestine: 273 occurrences\n",
      "  Qatar: 273 occurrences\n",
      "  Saudi_Arabia: 273 occurrences\n",
      "  Sudan: 37 occurrences\n",
      "  Syria: 270 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 272 occurrences\n",
      "  Yemen: 268 occurrences\n",
      "\n",
      "For class group with 14 active labels:\n",
      "  Algeria: 1 occurrences\n",
      "  Bahrain: 256 occurrences\n",
      "  Egypt: 243 occurrences\n",
      "  Iraq: 252 occurrences\n",
      "  Jordan: 256 occurrences\n",
      "  Kuwait: 256 occurrences\n",
      "  Lebanon: 255 occurrences\n",
      "  Libya: 128 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 256 occurrences\n",
      "  Palestine: 256 occurrences\n",
      "  Qatar: 256 occurrences\n",
      "  Saudi_Arabia: 256 occurrences\n",
      "  Sudan: 149 occurrences\n",
      "  Syria: 255 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 256 occurrences\n",
      "  Yemen: 251 occurrences\n",
      "\n",
      "For class group with 15 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 500 occurrences\n",
      "  Egypt: 500 occurrences\n",
      "  Iraq: 499 occurrences\n",
      "  Jordan: 500 occurrences\n",
      "  Kuwait: 500 occurrences\n",
      "  Lebanon: 500 occurrences\n",
      "  Libya: 498 occurrences\n",
      "  Morocco: 1 occurrences\n",
      "  Oman: 500 occurrences\n",
      "  Palestine: 500 occurrences\n",
      "  Qatar: 500 occurrences\n",
      "  Saudi_Arabia: 500 occurrences\n",
      "  Sudan: 498 occurrences\n",
      "  Syria: 500 occurrences\n",
      "  Tunisia: 1 occurrences\n",
      "  UAE: 500 occurrences\n",
      "  Yemen: 499 occurrences\n",
      "\n",
      "For class group with 16 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 0 occurrences\n",
      "  Egypt: 0 occurrences\n",
      "  Iraq: 0 occurrences\n",
      "  Jordan: 0 occurrences\n",
      "  Kuwait: 0 occurrences\n",
      "  Lebanon: 0 occurrences\n",
      "  Libya: 0 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 0 occurrences\n",
      "  Palestine: 0 occurrences\n",
      "  Qatar: 0 occurrences\n",
      "  Saudi_Arabia: 0 occurrences\n",
      "  Sudan: 0 occurrences\n",
      "  Syria: 0 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 0 occurrences\n",
      "  Yemen: 0 occurrences\n",
      "\n",
      "For class group with 17 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 0 occurrences\n",
      "  Egypt: 0 occurrences\n",
      "  Iraq: 0 occurrences\n",
      "  Jordan: 0 occurrences\n",
      "  Kuwait: 0 occurrences\n",
      "  Lebanon: 0 occurrences\n",
      "  Libya: 0 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 0 occurrences\n",
      "  Palestine: 0 occurrences\n",
      "  Qatar: 0 occurrences\n",
      "  Saudi_Arabia: 0 occurrences\n",
      "  Sudan: 0 occurrences\n",
      "  Syria: 0 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 0 occurrences\n",
      "  Yemen: 0 occurrences\n",
      "\n",
      "For class group with 18 active labels:\n",
      "  Algeria: 500 occurrences\n",
      "  Bahrain: 500 occurrences\n",
      "  Egypt: 500 occurrences\n",
      "  Iraq: 500 occurrences\n",
      "  Jordan: 500 occurrences\n",
      "  Kuwait: 500 occurrences\n",
      "  Lebanon: 500 occurrences\n",
      "  Libya: 500 occurrences\n",
      "  Morocco: 500 occurrences\n",
      "  Oman: 500 occurrences\n",
      "  Palestine: 500 occurrences\n",
      "  Qatar: 500 occurrences\n",
      "  Saudi_Arabia: 500 occurrences\n",
      "  Sudan: 500 occurrences\n",
      "  Syria: 500 occurrences\n",
      "  Tunisia: 500 occurrences\n",
      "  UAE: 500 occurrences\n",
      "  Yemen: 500 occurrences\n"
     ]
    }
   ],
   "source": [
    "get_class_insight(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(directory + \" BALANCED_NEW_DATASET_500.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 50%|█████     | 374/748 [00:32<00:27, 13.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4429030120372772, 'eval_f1': 0.7262548011197187, 'eval_roc_auc': 0.7910349145094046, 'eval_accuracy': 0.15060240963855423, 'eval_runtime': 0.7421, 'eval_samples_per_second': 1342.187, 'eval_steps_per_second': 56.598, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 501/748 [00:43<00:19, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4215, 'grad_norm': 4.750375270843506, 'learning_rate': 5e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 748/748 [01:05<00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4512268900871277, 'eval_f1': 0.7295721119192713, 'eval_roc_auc': 0.7940113323604024, 'eval_accuracy': 0.16967871485943775, 'eval_runtime': 0.7301, 'eval_samples_per_second': 1364.209, 'eval_steps_per_second': 57.527, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [01:08<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 68.2553, 'train_samples_per_second': 262.397, 'train_steps_per_second': 10.959, 'train_loss': 0.4062513361640155, 'epoch': 2.0}\n",
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.2740\n",
      "Micro Precision: 0.6768\n",
      "Micro Recall: 0.5000\n",
      "Micro F1-Score: 0.5751\n",
      "Precision per label: [0.8        0.89655172 0.58823529 0.66666667 0.66666667 0.55813953\n",
      " 0.64705882 0.73809524]\n",
      "Recall per label: [0.34285714 0.66666667 0.6        0.5        0.28571429 0.52173913\n",
      " 0.52380952 0.52542373]\n",
      "F1-Score per label: [0.48       0.76470588 0.59405941 0.57142857 0.4        0.53932584\n",
      " 0.57894737 0.61386139]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1992904/525870965.py:81: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/ BALANCED_NEW_DATASET_750.csv\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=13\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=2,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 69.52 %\n",
      "MACRO AVERAGE RECALL SCORE: 49.58 %\n",
      "MACRO AVERAGE F1-SCORE: 56.78 %\n",
      "MACRO AVERAGE ACCURACY: 72.60 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_13/camelbert_finetuned_epochs_2_eval_f1_0.7296_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_8-camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3-experiment-13_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 20%|█▉        | 253/1270 [00:19<01:17, 13.17it/s]\n",
      " 20%|██        | 254/1270 [00:20<01:17, 13.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41958171129226685, 'eval_f1': 0.7857666313818092, 'eval_roc_auc': 0.7992212978015347, 'eval_accuracy': 0.17429837518463812, 'eval_runtime': 0.514, 'eval_samples_per_second': 1317.176, 'eval_steps_per_second': 56.423, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 501/1270 [00:41<00:58, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3978, 'grad_norm': 2.486278533935547, 'learning_rate': 5e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 507/1270 [00:41<00:58, 13.04it/s]\n",
      " 40%|████      | 508/1270 [00:42<00:58, 13.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43696141242980957, 'eval_f1': 0.7823587385019711, 'eval_roc_auc': 0.7964695693203278, 'eval_accuracy': 0.16248153618906944, 'eval_runtime': 0.4917, 'eval_samples_per_second': 1376.887, 'eval_steps_per_second': 58.98, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 761/1270 [01:04<00:40, 12.70it/s]\n",
      " 60%|██████    | 762/1270 [01:04<00:39, 12.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4539303481578827, 'eval_f1': 0.7873862732974118, 'eval_roc_auc': 0.8072853133004487, 'eval_accuracy': 0.16691285081240767, 'eval_runtime': 0.502, 'eval_samples_per_second': 1348.666, 'eval_steps_per_second': 57.772, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 1001/1270 [01:25<00:21, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2821, 'grad_norm': 1.5508099794387817, 'learning_rate': 1.7532467532467535e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 1015/1270 [01:26<00:19, 12.87it/s]\n",
      " 80%|████████  | 1016/1270 [01:27<00:19, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.486063688993454, 'eval_f1': 0.7885090785099309, 'eval_roc_auc': 0.8056512106004009, 'eval_accuracy': 0.17872968980797638, 'eval_runtime': 0.5283, 'eval_samples_per_second': 1281.507, 'eval_steps_per_second': 54.895, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1269/1270 [01:49<00:00, 13.10it/s]\n",
      "100%|██████████| 1270/1270 [01:52<00:00, 13.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4963809847831726, 'eval_f1': 0.7906378600823045, 'eval_roc_auc': 0.8081659234473502, 'eval_accuracy': 0.17725258493353027, 'eval_runtime': 0.5111, 'eval_samples_per_second': 1324.562, 'eval_steps_per_second': 56.739, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1270/1270 [01:55<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 115.591, 'train_samples_per_second': 263.213, 'train_steps_per_second': 10.987, 'train_loss': 0.31099345590185934, 'epoch': 5.0}\n",
      "Subset Accuracy: 0.1583\n",
      "Hamming Loss: 0.2844\n",
      "Micro Precision: 0.6554\n",
      "Micro Recall: 0.4916\n",
      "Micro F1-Score: 0.5618\n",
      "Precision per label: [0.81818182 0.90322581 0.50909091 0.63265306 0.55       0.55813953\n",
      " 0.61538462 0.8       ]\n",
      "Recall per label: [0.25714286 0.71794872 0.56       0.484375   0.26190476 0.52173913\n",
      " 0.38095238 0.61016949]\n",
      "F1-Score per label: [0.39130435 0.8        0.53333333 0.54867257 0.35483871 0.53932584\n",
      " 0.47058824 0.69230769]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1992904/525870965.py:81: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/ BALANCED_NEW_DATASET_500.csv\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=14\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=5,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 67.33 %\n",
      "MACRO AVERAGE RECALL SCORE: 47.43 %\n",
      "MACRO AVERAGE F1-SCORE: 54.13 %\n",
      "MACRO AVERAGE ACCURACY: 71.56 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_14/camelbert_finetuned_epochs_5_eval_f1_0.7906_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_8-camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3-experiment-14_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  5%|▌         | 254/5080 [00:19<06:05, 13.19it/s]\n",
      "  5%|▌         | 254/5080 [00:20<06:05, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41958171129226685, 'eval_f1': 0.7857666313818092, 'eval_roc_auc': 0.7992212978015347, 'eval_accuracy': 0.17429837518463812, 'eval_runtime': 0.5048, 'eval_samples_per_second': 1340.993, 'eval_steps_per_second': 57.443, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 502/5080 [00:41<05:50, 13.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3978, 'grad_norm': 2.486278533935547, 'learning_rate': 5e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 508/5080 [00:42<05:39, 13.46it/s]\n",
      " 10%|█         | 508/5080 [00:42<05:39, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4369806945323944, 'eval_f1': 0.7821660234830445, 'eval_roc_auc': 0.7962522410096933, 'eval_accuracy': 0.16100443131462333, 'eval_runtime': 0.507, 'eval_samples_per_second': 1335.188, 'eval_steps_per_second': 57.194, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 762/5080 [01:04<05:22, 13.40it/s]\n",
      " 15%|█▌        | 762/5080 [01:04<05:22, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46652302145957947, 'eval_f1': 0.7806145301442264, 'eval_roc_auc': 0.8020244655851325, 'eval_accuracy': 0.16395864106351551, 'eval_runtime': 0.4911, 'eval_samples_per_second': 1378.409, 'eval_steps_per_second': 59.046, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1002/5080 [01:25<05:15, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2916, 'grad_norm': 2.4166393280029297, 'learning_rate': 4.4541484716157205e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1016/5080 [01:26<05:00, 13.54it/s]\n",
      " 20%|██        | 1016/5080 [01:27<05:00, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.498250812292099, 'eval_f1': 0.7837244639540774, 'eval_roc_auc': 0.8001604489455602, 'eval_accuracy': 0.17134416543574593, 'eval_runtime': 0.5226, 'eval_samples_per_second': 1295.344, 'eval_steps_per_second': 55.487, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1270/5080 [01:48<04:40, 13.61it/s]\n",
      " 25%|██▌       | 1270/5080 [01:49<04:40, 13.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.520680844783783, 'eval_f1': 0.7880685276980607, 'eval_roc_auc': 0.8067535362052624, 'eval_accuracy': 0.1757754800590842, 'eval_runtime': 0.5062, 'eval_samples_per_second': 1337.525, 'eval_steps_per_second': 57.294, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1502/5080 [02:09<04:34, 13.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2098, 'grad_norm': 1.620110034942627, 'learning_rate': 3.9082969432314415e-05, 'epoch': 5.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1524/5080 [02:10<04:24, 13.46it/s]\n",
      " 30%|███       | 1524/5080 [02:11<04:24, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.554932713508606, 'eval_f1': 0.7831502465184672, 'eval_roc_auc': 0.8015938122357977, 'eval_accuracy': 0.15952732644017725, 'eval_runtime': 0.4949, 'eval_samples_per_second': 1367.842, 'eval_steps_per_second': 58.593, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1778/5080 [02:33<04:04, 13.50it/s]\n",
      " 35%|███▌      | 1778/5080 [02:33<04:04, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5746973752975464, 'eval_f1': 0.7826010852441799, 'eval_roc_auc': 0.8020177751854611, 'eval_accuracy': 0.15214180206794684, 'eval_runtime': 0.5001, 'eval_samples_per_second': 1353.702, 'eval_steps_per_second': 57.987, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 2002/5080 [02:53<03:56, 13.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1568, 'grad_norm': 1.9167410135269165, 'learning_rate': 3.362445414847162e-05, 'epoch': 7.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2032/5080 [02:55<03:45, 13.50it/s]\n",
      " 40%|████      | 2032/5080 [02:55<03:45, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5851396918296814, 'eval_f1': 0.7846004757290107, 'eval_roc_auc': 0.8044437579777531, 'eval_accuracy': 0.14180206794682423, 'eval_runtime': 0.5022, 'eval_samples_per_second': 1347.95, 'eval_steps_per_second': 57.741, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 2286/5080 [03:18<03:30, 13.26it/s]\n",
      " 45%|████▌     | 2286/5080 [03:19<03:30, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6327815651893616, 'eval_f1': 0.7814134964972954, 'eval_roc_auc': 0.8019523244312996, 'eval_accuracy': 0.1447562776957164, 'eval_runtime': 0.5027, 'eval_samples_per_second': 1346.74, 'eval_steps_per_second': 57.689, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 2502/5080 [03:39<03:27, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.116, 'grad_norm': 2.9025323390960693, 'learning_rate': 2.816593886462882e-05, 'epoch': 9.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2540/5080 [03:42<03:09, 13.40it/s]\n",
      " 50%|█████     | 2540/5080 [03:42<03:09, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.626867949962616, 'eval_f1': 0.7873134328358209, 'eval_roc_auc': 0.8076503239579264, 'eval_accuracy': 0.16248153618906944, 'eval_runtime': 0.4999, 'eval_samples_per_second': 1354.185, 'eval_steps_per_second': 58.008, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 2794/5080 [04:05<02:50, 13.41it/s]\n",
      " 55%|█████▌    | 2794/5080 [04:05<02:50, 13.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6606573462486267, 'eval_f1': 0.7840213049267644, 'eval_roc_auc': 0.8044923182147118, 'eval_accuracy': 0.14771048744460857, 'eval_runtime': 0.5358, 'eval_samples_per_second': 1263.519, 'eval_steps_per_second': 54.124, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 3002/5080 [04:24<02:48, 12.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0863, 'grad_norm': 0.8163240551948547, 'learning_rate': 2.2707423580786028e-05, 'epoch': 11.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3048/5080 [04:28<02:41, 12.57it/s]\n",
      " 60%|██████    | 3048/5080 [04:28<02:41, 12.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6866637468338013, 'eval_f1': 0.7799626301272355, 'eval_roc_auc': 0.8008625844553279, 'eval_accuracy': 0.1432791728212703, 'eval_runtime': 0.5534, 'eval_samples_per_second': 1223.363, 'eval_steps_per_second': 52.404, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 3302/5080 [04:51<02:12, 13.38it/s]\n",
      " 65%|██████▌   | 3302/5080 [04:52<02:12, 13.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7243456244468689, 'eval_f1': 0.7818601348952786, 'eval_roc_auc': 0.8024372413091161, 'eval_accuracy': 0.14918759231905465, 'eval_runtime': 0.5135, 'eval_samples_per_second': 1318.463, 'eval_steps_per_second': 56.478, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3502/5080 [05:10<02:08, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0639, 'grad_norm': 0.9643259048461914, 'learning_rate': 1.7248908296943234e-05, 'epoch': 13.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3556/5080 [05:15<01:54, 13.26it/s]\n",
      " 70%|███████   | 3556/5080 [05:15<01:54, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7442327737808228, 'eval_f1': 0.7813854728833053, 'eval_roc_auc': 0.8017394929466736, 'eval_accuracy': 0.14771048744460857, 'eval_runtime': 0.515, 'eval_samples_per_second': 1314.548, 'eval_steps_per_second': 56.31, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3810/5080 [05:37<01:34, 13.47it/s]\n",
      " 75%|███████▌  | 3810/5080 [05:38<01:34, 13.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7518910765647888, 'eval_f1': 0.7786892634995042, 'eval_roc_auc': 0.8007468734446191, 'eval_accuracy': 0.1536189069423929, 'eval_runtime': 0.5041, 'eval_samples_per_second': 1343.01, 'eval_steps_per_second': 57.529, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 4002/5080 [05:55<01:25, 12.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0479, 'grad_norm': 0.5296791195869446, 'learning_rate': 1.1790393013100438e-05, 'epoch': 15.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4064/5080 [06:00<01:15, 13.55it/s]\n",
      " 80%|████████  | 4064/5080 [06:01<01:15, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7658830881118774, 'eval_f1': 0.7787119856887299, 'eval_roc_auc': 0.8001246662751869, 'eval_accuracy': 0.1447562776957164, 'eval_runtime': 0.4997, 'eval_samples_per_second': 1354.687, 'eval_steps_per_second': 58.029, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 4318/5080 [06:24<01:02, 12.26it/s]\n",
      " 85%|████████▌ | 4318/5080 [06:24<01:02, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7807112336158752, 'eval_f1': 0.779446354260901, 'eval_roc_auc': 0.8017343928879078, 'eval_accuracy': 0.14771048744460857, 'eval_runtime': 0.6318, 'eval_samples_per_second': 1071.517, 'eval_steps_per_second': 45.9, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 4502/5080 [06:41<00:44, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0383, 'grad_norm': 2.551100254058838, 'learning_rate': 6.342794759825328e-06, 'epoch': 17.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4572/5080 [06:47<00:37, 13.57it/s]\n",
      " 90%|█████████ | 4572/5080 [06:47<00:37, 13.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7894988656044006, 'eval_f1': 0.7802996321880327, 'eval_roc_auc': 0.801847005475818, 'eval_accuracy': 0.1536189069423929, 'eval_runtime': 0.4988, 'eval_samples_per_second': 1357.157, 'eval_steps_per_second': 58.135, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 4826/5080 [07:09<00:18, 13.68it/s]\n",
      " 95%|█████████▌| 4826/5080 [07:10<00:18, 13.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7982764840126038, 'eval_f1': 0.7787069508137757, 'eval_roc_auc': 0.8005562244736575, 'eval_accuracy': 0.15066469719350073, 'eval_runtime': 0.5016, 'eval_samples_per_second': 1349.693, 'eval_steps_per_second': 57.816, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 5002/5080 [07:26<00:06, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0324, 'grad_norm': 0.5551343560218811, 'learning_rate': 8.842794759825327e-07, 'epoch': 19.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5080/5080 [07:32<00:00, 13.52it/s]\n",
      "100%|██████████| 5080/5080 [07:35<00:00, 13.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.799960196018219, 'eval_f1': 0.7806718160589187, 'eval_roc_auc': 0.8022875573263056, 'eval_accuracy': 0.155096011816839, 'eval_runtime': 0.5184, 'eval_samples_per_second': 1305.893, 'eval_steps_per_second': 55.939, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5080/5080 [07:41<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 461.3046, 'train_samples_per_second': 263.817, 'train_steps_per_second': 11.012, 'train_loss': 0.14231886192569582, 'epoch': 20.0}\n",
      "Subset Accuracy: 0.1333\n",
      "Hamming Loss: 0.2969\n",
      "Micro Precision: 0.6473\n",
      "Micro Recall: 0.4382\n",
      "Micro F1-Score: 0.5226\n",
      "Precision per label: [0.76923077 0.88       0.5        0.65853659 0.46153846 0.61764706\n",
      " 0.53333333 0.74      ]\n",
      "Recall per label: [0.28571429 0.56410256 0.5        0.421875   0.14285714 0.45652174\n",
      " 0.38095238 0.62711864]\n",
      "F1-Score per label: [0.41666667 0.6875     0.5        0.51428571 0.21818182 0.525\n",
      " 0.44444444 0.67889908]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1992904/525870965.py:81: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/ BALANCED_NEW_DATASET_500.csv\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=14\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=20,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 64.50 %\n",
      "MACRO AVERAGE RECALL SCORE: 42.24 %\n",
      "MACRO AVERAGE F1-SCORE: 49.81 %\n",
      "MACRO AVERAGE ACCURACY: 70.31 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_14/camelbert_finetuned_epochs_20_eval_f1_0.7881_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_8-camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3-experiment-14_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " array([[0.9507779 , 0.98521465, 0.97288233, 0.97754294, 0.98201376,\n",
       "         0.9885804 , 0.98475236, 0.96628344, 0.9546474 , 0.98197925,\n",
       "         0.9845754 , 0.9857729 , 0.9827572 , 0.97788346, 0.98521465,\n",
       "         0.94539934, 0.98340684, 0.9881765 ]], dtype=float32))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"الله اكبر\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]]),\n",
       " array([[0.00941259, 0.624179  , 0.7916601 , 0.4977589 , 0.6311397 ,\n",
       "         0.73278314, 0.33133605, 0.26999873, 0.00754792, 0.4181622 ,\n",
       "         0.47498128, 0.66093874, 0.86025184, 0.3869372 , 0.33654907,\n",
       "         0.00871138, 0.32637876, 0.47081262]], dtype=float32))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"انا مصرى ياسطااااا\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 16 Mekky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 10%|█         | 252/2520 [00:22<03:11, 11.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.467267781496048, 'eval_f1': 0.7741051549922932, 'eval_roc_auc': 0.7878222018813346, 'eval_accuracy': 0.16542473919523099, 'eval_runtime': 0.5593, 'eval_samples_per_second': 1199.706, 'eval_steps_per_second': 50.062, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 502/2520 [00:49<03:01, 11.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4168, 'grad_norm': 6.076703071594238, 'learning_rate': 5e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 504/2520 [00:49<02:55, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47096696496009827, 'eval_f1': 0.7664578147798472, 'eval_roc_auc': 0.7806076513088027, 'eval_accuracy': 0.12667660208643816, 'eval_runtime': 0.515, 'eval_samples_per_second': 1302.944, 'eval_steps_per_second': 54.37, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 30%|███       | 756/2520 [01:15<02:28, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5002021789550781, 'eval_f1': 0.772990812594773, 'eval_roc_auc': 0.7912177076066527, 'eval_accuracy': 0.14008941877794337, 'eval_runtime': 0.5085, 'eval_samples_per_second': 1319.577, 'eval_steps_per_second': 55.064, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 1002/2520 [01:41<02:14, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3131, 'grad_norm': 3.2008445262908936, 'learning_rate': 3.7673267326732673e-05, 'epoch': 3.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 1008/2520 [01:42<02:08, 11.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5147558450698853, 'eval_f1': 0.7785293097467916, 'eval_roc_auc': 0.7934895058052991, 'eval_accuracy': 0.15052160953800298, 'eval_runtime': 0.5046, 'eval_samples_per_second': 1329.877, 'eval_steps_per_second': 55.494, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 1260/2520 [02:09<01:46, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.544553816318512, 'eval_f1': 0.7688691564377121, 'eval_roc_auc': 0.7845958507551555, 'eval_accuracy': 0.14307004470938897, 'eval_runtime': 0.5047, 'eval_samples_per_second': 1329.552, 'eval_steps_per_second': 55.481, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 1501/2520 [02:34<01:30, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2126, 'grad_norm': 12.227588653564453, 'learning_rate': 2.5321782178217822e-05, 'epoch': 5.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 1512/2520 [02:35<01:28, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5832158327102661, 'eval_f1': 0.7710987601539119, 'eval_roc_auc': 0.784712128920743, 'eval_accuracy': 0.14605067064083457, 'eval_runtime': 0.5035, 'eval_samples_per_second': 1332.608, 'eval_steps_per_second': 55.608, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|███████   | 1764/2520 [03:02<01:03, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6284805536270142, 'eval_f1': 0.7659278574532812, 'eval_roc_auc': 0.7815790613766535, 'eval_accuracy': 0.12965722801788376, 'eval_runtime': 0.5013, 'eval_samples_per_second': 1338.444, 'eval_steps_per_second': 55.852, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 2002/2520 [03:26<00:45, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1538, 'grad_norm': 1.6658316850662231, 'learning_rate': 1.2945544554455447e-05, 'epoch': 7.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 2016/2520 [03:28<00:43, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6430134177207947, 'eval_f1': 0.7737251512532412, 'eval_roc_auc': 0.7884810466867804, 'eval_accuracy': 0.15648286140089418, 'eval_runtime': 0.5069, 'eval_samples_per_second': 1323.688, 'eval_steps_per_second': 55.236, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 2268/2520 [03:55<00:21, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6525330543518066, 'eval_f1': 0.7699037620297463, 'eval_roc_auc': 0.7861548168653606, 'eval_accuracy': 0.12965722801788376, 'eval_runtime': 0.5062, 'eval_samples_per_second': 1325.542, 'eval_steps_per_second': 55.313, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 2502/2520 [04:19<00:01, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1132, 'grad_norm': 1.9419139623641968, 'learning_rate': 5.693069306930693e-07, 'epoch': 9.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 2520/2520 [04:25<00:00, 11.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6510425209999084, 'eval_f1': 0.7713090465400035, 'eval_roc_auc': 0.7870727950749379, 'eval_accuracy': 0.13710879284649777, 'eval_runtime': 0.5029, 'eval_samples_per_second': 1334.228, 'eval_steps_per_second': 55.676, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2520/2520 [04:30<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 270.5929, 'train_samples_per_second': 222.992, 'train_steps_per_second': 9.313, 'train_loss': 0.24082989210174197, 'epoch': 10.0}\n",
      "Subset Accuracy: 0.1667\n",
      "Hamming Loss: 0.2510\n",
      "Micro Precision: 0.7061\n",
      "Micro Recall: 0.5534\n",
      "Micro F1-Score: 0.6205\n",
      "Precision per label: [0.66666667 0.90909091 0.6        0.72222222 0.77777778 0.59259259\n",
      " 0.6        0.84444444]\n",
      "Recall per label: [0.17142857 0.76923077 0.78       0.609375   0.16666667 0.69565217\n",
      " 0.28571429 0.6440678 ]\n",
      "F1-Score per label: [0.27272727 0.83333333 0.67826087 0.66101695 0.2745098  0.64\n",
      " 0.38709677 0.73076923]\n",
      "{0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2559847/247078281.py:82: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/balanced_multilabel_dataset_500.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_16/marbert_finetuned_epochs_2_eval_f1_0.7607_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=17\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 71.41 %\n",
      "MACRO AVERAGE RECALL SCORE: 51.53 %\n",
      "MACRO AVERAGE F1-SCORE: 55.97 %\n",
      "MACRO AVERAGE ACCURACY: 74.90 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_17/marbert_finetuned_epochs_10_eval_f1_0.7785_greater_threshold_0.3/-home-ali.mekky-Documents-NLP-Project-Cross-Country-Dialectal-Arabic-Identification-exp_16-marbert_finetuned_epochs_2_eval_f1_0.7607_greater_threshold_0.3-experiment-17_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset created and saved as 'balanced_multilabel_dataset_500.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "label_columns = df.columns[2:-1]  \n",
    "df[label_columns] = df[label_columns].astype(int) \n",
    "\n",
    "threshold = 500  \n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for num_classes in range(1, len(label_columns) + 1):\n",
    "    subset = df[df[label_columns].sum(axis=1) == num_classes]  \n",
    "    \n",
    "    if len(subset) > threshold:\n",
    "        subset = shuffle(subset).head(threshold)\n",
    "    \n",
    "    balanced_df = pd.concat([balanced_df, subset], ignore_index=True)\n",
    "\n",
    "balanced_df = shuffle(balanced_df).reset_index(drop=True)\n",
    "balanced_df.to_csv('balanced_multilabel_dataset_lr_' + str(threshold) + '.csv', index=False)\n",
    "\n",
    "print(\"Balanced dataset created and saved as 'balanced_multilabel_dataset_500.csv'\")\n",
    "\n",
    "label_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait', 'Lebanon', \n",
    "                 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar', 'Saudi_Arabia', \n",
    "                 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "counts = {}\n",
    "\n",
    "for i in range(19):\n",
    "    counts[i] = (balanced_df[label_columns].sum(axis=1) == i).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  2%|▏         | 503/22040 [00:34<21:18, 16.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5979, 'grad_norm': 1.3358558416366577, 'learning_rate': 5e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1003/22040 [01:04<20:59, 16.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5129, 'grad_norm': 1.0376207828521729, 'learning_rate': 4.883936861652739e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1503/22040 [01:34<20:34, 16.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4715, 'grad_norm': 1.1235110759735107, 'learning_rate': 4.767873723305478e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2003/22040 [02:04<20:16, 16.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4428, 'grad_norm': 1.3721928596496582, 'learning_rate': 4.6518105849582176e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 10%|█         | 2204/22040 [02:21<20:13, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.408955454826355, 'eval_f1': 0.7052565213964851, 'eval_roc_auc': 0.7866487638135383, 'eval_accuracy': 0.07452782031648801, 'eval_runtime': 5.2099, 'eval_samples_per_second': 1128.044, 'eval_steps_per_second': 47.026, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 2503/22040 [02:42<20:22, 15.98it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4151, 'grad_norm': 1.163678765296936, 'learning_rate': 4.535747446610957e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3003/22040 [03:14<19:58, 15.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3937, 'grad_norm': 1.7127900123596191, 'learning_rate': 4.4196843082636956e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3503/22040 [03:45<19:32, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3805, 'grad_norm': 1.4778358936309814, 'learning_rate': 4.303621169916435e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4003/22040 [04:17<19:15, 15.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3744, 'grad_norm': 1.4954077005386353, 'learning_rate': 4.1877901578458685e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 20%|██        | 4408/22040 [04:47<18:39, 15.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35471436381340027, 'eval_f1': 0.7469332598627675, 'eval_roc_auc': 0.8209755039772592, 'eval_accuracy': 0.09715841415688277, 'eval_runtime': 4.8477, 'eval_samples_per_second': 1212.331, 'eval_steps_per_second': 50.54, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4503/22040 [04:54<15:50, 18.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3572, 'grad_norm': 1.518789291381836, 'learning_rate': 4.071727019498607e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5003/22040 [05:21<15:13, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3321, 'grad_norm': 1.5034887790679932, 'learning_rate': 3.9556638811513465e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 5503/22040 [05:47<14:47, 18.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3258, 'grad_norm': 1.4828273057937622, 'learning_rate': 3.839600742804086e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6003/22040 [06:14<14:19, 18.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3195, 'grad_norm': 1.8899327516555786, 'learning_rate': 3.723537604456825e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 6503/22040 [06:41<13:51, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3175, 'grad_norm': 1.8249070644378662, 'learning_rate': 3.607706592386259e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 30%|███       | 6612/22040 [06:51<13:59, 18.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32624778151512146, 'eval_f1': 0.7740718980146747, 'eval_roc_auc': 0.839169561082601, 'eval_accuracy': 0.11757699506550962, 'eval_runtime': 4.5337, 'eval_samples_per_second': 1296.297, 'eval_steps_per_second': 54.04, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7003/22040 [07:13<13:24, 18.70it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2948, 'grad_norm': 17.41426658630371, 'learning_rate': 3.4916434540389974e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 7503/22040 [07:40<13:21, 18.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2835, 'grad_norm': 2.4894626140594482, 'learning_rate': 3.375580315691737e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 8003/22040 [08:07<12:30, 18.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2849, 'grad_norm': 1.489452838897705, 'learning_rate': 3.2595171773444754e-05, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 8503/22040 [08:33<12:01, 18.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2808, 'grad_norm': 1.241461992263794, 'learning_rate': 3.143454038997215e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 40%|████      | 8816/22040 [08:55<12:01, 18.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3031369745731354, 'eval_f1': 0.7921221233424086, 'eval_roc_auc': 0.8544168727866787, 'eval_accuracy': 0.1419091373149566, 'eval_runtime': 4.5082, 'eval_samples_per_second': 1303.62, 'eval_steps_per_second': 54.345, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9003/22040 [09:06<11:38, 18.68it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2672, 'grad_norm': 1.5944093465805054, 'learning_rate': 3.0273909006499534e-05, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9503/22040 [09:33<11:11, 18.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2517, 'grad_norm': 1.7384564876556396, 'learning_rate': 2.911327762302693e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 10003/22040 [10:00<10:44, 18.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2519, 'grad_norm': 1.6146438121795654, 'learning_rate': 2.795264623955432e-05, 'epoch': 4.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10503/22040 [10:26<10:17, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2455, 'grad_norm': 1.8552796840667725, 'learning_rate': 2.679201485608171e-05, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 11003/22040 [10:53<09:54, 18.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2478, 'grad_norm': 1.6663494110107422, 'learning_rate': 2.56313834726091e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 50%|█████     | 11020/22040 [10:58<09:58, 18.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2934257388114929, 'eval_f1': 0.8008027961334717, 'eval_roc_auc': 0.8603707558253062, 'eval_accuracy': 0.15177811808745958, 'eval_runtime': 4.5861, 'eval_samples_per_second': 1281.484, 'eval_steps_per_second': 53.422, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11503/22040 [11:26<09:22, 18.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2234, 'grad_norm': 1.7112796306610107, 'learning_rate': 2.447075208913649e-05, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 12003/22040 [11:52<08:57, 18.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.222, 'grad_norm': 1.6034361124038696, 'learning_rate': 2.3314763231197773e-05, 'epoch': 5.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12503/22040 [12:19<08:38, 18.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2217, 'grad_norm': 1.8515009880065918, 'learning_rate': 2.2154131847725163e-05, 'epoch': 5.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 13003/22040 [12:46<08:02, 18.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2212, 'grad_norm': 1.5926158428192139, 'learning_rate': 2.0993500464252556e-05, 'epoch': 5.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 60%|██████    | 13224/22040 [13:02<07:55, 18.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2887248992919922, 'eval_f1': 0.809963896904055, 'eval_roc_auc': 0.8649819256952302, 'eval_accuracy': 0.1676025182916454, 'eval_runtime': 4.5198, 'eval_samples_per_second': 1300.279, 'eval_steps_per_second': 54.206, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 13503/22040 [13:19<07:36, 18.69it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2088, 'grad_norm': 1.393267035484314, 'learning_rate': 1.9832869080779946e-05, 'epoch': 6.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 14003/22040 [13:45<07:09, 18.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1982, 'grad_norm': 1.3073937892913818, 'learning_rate': 1.8672237697307336e-05, 'epoch': 6.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 14502/22040 [14:12<06:45, 18.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2017, 'grad_norm': 1.654129981994629, 'learning_rate': 1.7513927576601672e-05, 'epoch': 6.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 15002/22040 [14:39<06:19, 18.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1987, 'grad_norm': 2.4459002017974854, 'learning_rate': 1.6353296193129066e-05, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 70%|███████   | 15428/22040 [15:06<05:48, 18.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28579017519950867, 'eval_f1': 0.8130670007296155, 'eval_roc_auc': 0.8689891348559958, 'eval_accuracy': 0.171856389314276, 'eval_runtime': 4.5011, 'eval_samples_per_second': 1305.685, 'eval_steps_per_second': 54.431, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 15502/22040 [15:11<05:51, 18.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1955, 'grad_norm': 2.0298521518707275, 'learning_rate': 1.5192664809656454e-05, 'epoch': 7.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 16002/22040 [15:38<05:23, 18.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1794, 'grad_norm': 1.6589235067367554, 'learning_rate': 1.4032033426183844e-05, 'epoch': 7.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 16502/22040 [16:05<04:56, 18.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1815, 'grad_norm': 2.1766350269317627, 'learning_rate': 1.2871402042711237e-05, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 17002/22040 [16:31<04:30, 18.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1798, 'grad_norm': 1.8269281387329102, 'learning_rate': 1.1710770659238625e-05, 'epoch': 7.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 17503/22040 [16:58<04:04, 18.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1799, 'grad_norm': 1.7236909866333008, 'learning_rate': 1.0552460538532963e-05, 'epoch': 7.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 80%|████████  | 17632/22040 [17:10<04:00, 18.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28462493419647217, 'eval_f1': 0.8157062302296796, 'eval_roc_auc': 0.8711823527403646, 'eval_accuracy': 0.17917304747320062, 'eval_runtime': 4.5361, 'eval_samples_per_second': 1295.614, 'eval_steps_per_second': 54.011, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 18003/22040 [17:31<03:38, 18.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1724, 'grad_norm': 2.3094849586486816, 'learning_rate': 9.391829155060353e-06, 'epoch': 8.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 18503/22040 [17:58<03:11, 18.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1657, 'grad_norm': 1.6379061937332153, 'learning_rate': 8.231197771587743e-06, 'epoch': 8.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 19003/22040 [18:25<02:42, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1671, 'grad_norm': 1.9871485233306885, 'learning_rate': 7.070566388115135e-06, 'epoch': 8.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 19503/22040 [18:51<02:16, 18.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1662, 'grad_norm': 1.8685622215270996, 'learning_rate': 5.909935004642526e-06, 'epoch': 8.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 90%|█████████ | 19836/22040 [19:14<01:57, 18.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28471267223358154, 'eval_f1': 0.8181240327216449, 'eval_roc_auc': 0.8724516948598547, 'eval_accuracy': 0.183256763654926, 'eval_runtime': 4.5795, 'eval_samples_per_second': 1283.33, 'eval_steps_per_second': 53.499, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 20002/22040 [19:24<01:50, 18.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.163, 'grad_norm': 1.9178637266159058, 'learning_rate': 4.751624883936862e-06, 'epoch': 9.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 20502/22040 [19:51<01:23, 18.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1591, 'grad_norm': 2.212700605392456, 'learning_rate': 3.590993500464253e-06, 'epoch': 9.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 21002/22040 [20:18<00:55, 18.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1583, 'grad_norm': 1.9245855808258057, 'learning_rate': 2.4303621169916438e-06, 'epoch': 9.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 21502/22040 [20:44<00:28, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1574, 'grad_norm': 1.6813673973083496, 'learning_rate': 1.2697307335190344e-06, 'epoch': 9.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 22002/22040 [21:11<00:02, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1552, 'grad_norm': 1.9536502361297607, 'learning_rate': 1.0909935004642526e-07, 'epoch': 9.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 22040/22040 [21:19<00:00, 18.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2849634289741516, 'eval_f1': 0.818281100958016, 'eval_roc_auc': 0.8725174125949922, 'eval_accuracy': 0.18104475072315807, 'eval_runtime': 4.5205, 'eval_samples_per_second': 1300.087, 'eval_steps_per_second': 54.198, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22040/22040 [21:21<00:00, 17.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1281.7072, 'train_samples_per_second': 412.661, 'train_steps_per_second': 17.196, 'train_loss': 0.26628883518887086, 'epoch': 10.0}\n",
      "Subset Accuracy: 0.1000\n",
      "Hamming Loss: 0.3333\n",
      "Micro Precision: 0.5462\n",
      "Micro Recall: 0.5983\n",
      "Micro F1-Score: 0.5710\n",
      "Precision per label: [0.51162791 0.63265306 0.53846154 0.57692308 0.73076923 0.48484848\n",
      " 0.38095238 0.58333333]\n",
      "Recall per label: [0.62857143 0.79487179 0.56       0.46875    0.45238095 0.69565217\n",
      " 0.76190476 0.59322034]\n",
      "F1-Score per label: [0.56410256 0.70454545 0.54901961 0.51724138 0.55882353 0.57142857\n",
      " 0.50793651 0.58823529]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4102662/3917706724.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-ca\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=30\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 46.80 %\n",
      "MACRO AVERAGE RECALL SCORE: 64.37 %\n",
      "MACRO AVERAGE F1-SCORE: 52.38 %\n",
      "MACRO AVERAGE ACCURACY: 58.44 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_18/marbert_finetuned_epochs_10_eval_f1_0.8353_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-ca-experiment-18_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " array([[0.99621993, 0.99293363, 0.9789554 , 0.9881765 , 0.99378294,\n",
       "         0.99346113, 0.98922324, 0.99135584, 0.9954261 , 0.9932811 ,\n",
       "         0.994292  , 0.9939731 , 0.994089  , 0.9908035 , 0.99158704,\n",
       "         0.99317604, 0.9935369 , 0.9952632 ]], dtype=float32),\n",
       " 0.007803493075900558)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"الله اكبر\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]),\n",
       " array([[0.00602695, 0.01210039, 0.02551661, 0.13398075, 0.02532306,\n",
       "         0.01596765, 0.12995382, 0.0085113 , 0.00219124, 0.01200735,\n",
       "         0.04689926, 0.00372171, 0.01778039, 0.00564191, 0.43236297,\n",
       "         0.0102883 , 0.01764446, 0.0124797 ]], dtype=float32),\n",
       " 0.9489779008759393)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"إيه يا عم الجو حر موت النهارده، لازم نشرب حاجة ساقعة\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "  0%|          | 17/22040 [3:31:48<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5377, 'grad_norm': 2.3052988052368164, 'learning_rate': 5e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:32:33<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4135, 'grad_norm': 1.8364241123199463, 'learning_rate': 4.883936861652739e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:33:18<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3668, 'grad_norm': 2.281428098678589, 'learning_rate': 4.768105849582173e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:34:04<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.33, 'grad_norm': 2.3681914806365967, 'learning_rate': 4.652042711234912e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                 \n",
      "  0%|          | 17/22040 [3:34:27<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30051612854003906, 'eval_f1': 0.7969495534439868, 'eval_roc_auc': 0.8578380791930998, 'eval_accuracy': 0.12710566615620214, 'eval_runtime': 4.8859, 'eval_samples_per_second': 1202.838, 'eval_steps_per_second': 50.144, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:34:59<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2879, 'grad_norm': 3.490952968597412, 'learning_rate': 4.535979572887651e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:35:46<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.257, 'grad_norm': 3.019273042678833, 'learning_rate': 4.4199164345403905e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:36:33<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2451, 'grad_norm': 3.494957208633423, 'learning_rate': 4.303853296193129e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:37:20<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2384, 'grad_norm': 4.560043811798096, 'learning_rate': 4.188022284122563e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                 \n",
      "  0%|          | 17/22040 [3:38:04<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24287019670009613, 'eval_f1': 0.8345204436711614, 'eval_roc_auc': 0.887667324454597, 'eval_accuracy': 0.18461800238216777, 'eval_runtime': 5.0269, 'eval_samples_per_second': 1169.116, 'eval_steps_per_second': 48.738, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:38:16<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2193, 'grad_norm': 2.826425075531006, 'learning_rate': 4.071959145775302e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:39:03<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1644, 'grad_norm': 4.39860725402832, 'learning_rate': 3.955896007428041e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:39:51<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.162, 'grad_norm': 3.645512104034424, 'learning_rate': 3.83983286908078e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:40:33<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1606, 'grad_norm': 4.56809663772583, 'learning_rate': 3.724001857010214e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:41:15<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1592, 'grad_norm': 4.311180591583252, 'learning_rate': 3.6079387186629524e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                 \n",
      "  0%|          | 17/22040 [3:41:29<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23717337846755981, 'eval_f1': 0.8507988622051145, 'eval_roc_auc': 0.8942350016502764, 'eval_accuracy': 0.2222222222222222, 'eval_runtime': 4.5348, 'eval_samples_per_second': 1295.974, 'eval_steps_per_second': 54.026, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:42:06<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1169, 'grad_norm': 2.881850242614746, 'learning_rate': 3.4918755803156924e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:42:48<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1001, 'grad_norm': 3.451972246170044, 'learning_rate': 3.375812441968431e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:43:30<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1012, 'grad_norm': 3.4587137699127197, 'learning_rate': 3.2597493036211704e-05, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:44:12<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1014, 'grad_norm': 3.3298377990722656, 'learning_rate': 3.143686165273909e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                 \n",
      "  0%|          | 17/22040 [3:44:43<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24726076424121857, 'eval_f1': 0.8516292944719601, 'eval_roc_auc': 0.8935461907367093, 'eval_accuracy': 0.22596562872213716, 'eval_runtime': 4.5281, 'eval_samples_per_second': 1297.89, 'eval_steps_per_second': 54.106, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:45:02<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0855, 'grad_norm': 2.2144157886505127, 'learning_rate': 3.027623026926648e-05, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:45:44<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0616, 'grad_norm': 3.439931631088257, 'learning_rate': 2.911559888579387e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:46:26<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0616, 'grad_norm': 2.223876714706421, 'learning_rate': 2.7954967502321267e-05, 'epoch': 4.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:47:08<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0608, 'grad_norm': 4.037909507751465, 'learning_rate': 2.6796657381615596e-05, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:47:50<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0631, 'grad_norm': 3.489192008972168, 'learning_rate': 2.5636025998142993e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [3:47:57<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.273395299911499, 'eval_f1': 0.850458689212895, 'eval_roc_auc': 0.8915369591754604, 'eval_accuracy': 0.23175089331291476, 'eval_runtime': 4.5245, 'eval_samples_per_second': 1298.938, 'eval_steps_per_second': 54.15, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:48:41<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0374, 'grad_norm': 2.9217259883880615, 'learning_rate': 2.4475394614670383e-05, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:49:23<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0372, 'grad_norm': 1.4874317646026611, 'learning_rate': 2.3314763231197773e-05, 'epoch': 5.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:50:05<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0377, 'grad_norm': 2.8260183334350586, 'learning_rate': 2.2154131847725163e-05, 'epoch': 5.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:50:47<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0368, 'grad_norm': 2.603254556655884, 'learning_rate': 2.0993500464252556e-05, 'epoch': 5.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [3:51:10<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30497220158576965, 'eval_f1': 0.8452645367048237, 'eval_roc_auc': 0.8891073677067548, 'eval_accuracy': 0.21728773183597072, 'eval_runtime': 4.5055, 'eval_samples_per_second': 1304.398, 'eval_steps_per_second': 54.378, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:51:37<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0296, 'grad_norm': 1.689935326576233, 'learning_rate': 1.9835190343546892e-05, 'epoch': 6.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:52:19<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0228, 'grad_norm': 1.8808188438415527, 'learning_rate': 1.8674558960074282e-05, 'epoch': 6.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:53:01<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0233, 'grad_norm': 2.7396364212036133, 'learning_rate': 1.7513927576601672e-05, 'epoch': 6.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:53:44<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0219, 'grad_norm': 5.668574333190918, 'learning_rate': 1.6353296193129066e-05, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [3:54:24<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3278944790363312, 'eval_f1': 0.8469969184254899, 'eval_roc_auc': 0.8873300295222103, 'eval_accuracy': 0.22443423515399014, 'eval_runtime': 4.4972, 'eval_samples_per_second': 1306.805, 'eval_steps_per_second': 54.478, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:54:34<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.02, 'grad_norm': 1.0567728281021118, 'learning_rate': 1.5192664809656454e-05, 'epoch': 7.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:55:18<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0134, 'grad_norm': 2.4436826705932617, 'learning_rate': 1.4032033426183844e-05, 'epoch': 7.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:56:04<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0135, 'grad_norm': 1.4169901609420776, 'learning_rate': 1.2871402042711237e-05, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:56:48<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0133, 'grad_norm': 1.036405086517334, 'learning_rate': 1.1710770659238625e-05, 'epoch': 7.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:57:32<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.013, 'grad_norm': 2.0383734703063965, 'learning_rate': 1.0550139275766017e-05, 'epoch': 7.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [3:57:47<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3496192693710327, 'eval_f1': 0.8465674230380112, 'eval_roc_auc': 0.8878549000025162, 'eval_accuracy': 0.22613578356304237, 'eval_runtime': 4.5112, 'eval_samples_per_second': 1302.762, 'eval_steps_per_second': 54.309, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:58:22<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0097, 'grad_norm': 1.4177751541137695, 'learning_rate': 9.394150417827298e-06, 'epoch': 8.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:59:04<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0079, 'grad_norm': 0.9546614289283752, 'learning_rate': 8.23351903435469e-06, 'epoch': 8.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:59:46<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0078, 'grad_norm': 0.8336026668548584, 'learning_rate': 7.07288765088208e-06, 'epoch': 8.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:00:28<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0074, 'grad_norm': 0.28554850816726685, 'learning_rate': 5.912256267409471e-06, 'epoch': 8.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [4:01:01<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36540687084198, 'eval_f1': 0.8474516695957821, 'eval_roc_auc': 0.8865577468982784, 'eval_accuracy': 0.23158073847200952, 'eval_runtime': 4.5015, 'eval_samples_per_second': 1305.557, 'eval_steps_per_second': 54.426, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:01:18<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0063, 'grad_norm': 0.3903147578239441, 'learning_rate': 4.751624883936862e-06, 'epoch': 9.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:02:00<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0055, 'grad_norm': 1.0045398473739624, 'learning_rate': 3.590993500464253e-06, 'epoch': 9.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:02:42<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0049, 'grad_norm': 0.40790706872940063, 'learning_rate': 2.4303621169916438e-06, 'epoch': 9.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:03:24<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0048, 'grad_norm': 0.6210858821868896, 'learning_rate': 1.2697307335190344e-06, 'epoch': 9.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:04:06<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0047, 'grad_norm': 0.9460890889167786, 'learning_rate': 1.0909935004642526e-07, 'epoch': 9.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [4:04:18<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3677639365196228, 'eval_f1': 0.8469693880523111, 'eval_roc_auc': 0.8870306658177636, 'eval_accuracy': 0.2285179513357155, 'eval_runtime': 4.508, 'eval_samples_per_second': 1303.674, 'eval_steps_per_second': 54.347, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "100%|██████████| 22040/22040 [33:18<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1998.5777, 'train_samples_per_second': 264.643, 'train_steps_per_second': 11.028, 'train_loss': 0.10601788969835547, 'epoch': 10.0}\n",
      "Subset Accuracy: 0.0917\n",
      "Hamming Loss: 0.3302\n",
      "Micro Precision: 0.5499\n",
      "Micro Recall: 0.6039\n",
      "Micro F1-Score: 0.5756\n",
      "Precision per label: [0.54901961 0.68181818 0.54       0.58695652 0.63333333 0.50724638\n",
      " 0.34693878 0.61538462]\n",
      "Recall per label: [0.8        0.76923077 0.54       0.421875   0.45238095 0.76086957\n",
      " 0.80952381 0.54237288]\n",
      "F1-Score per label: [0.65116279 0.72289157 0.54       0.49090909 0.52777778 0.60869565\n",
      " 0.48571429 0.57657658]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2559847/4032790609.py:82: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"UBC-NLP/MARBERT\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=18\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 55.76 %\n",
      "MACRO AVERAGE RECALL SCORE: 63.70 %\n",
      "MACRO AVERAGE F1-SCORE: 57.55 %\n",
      "MACRO AVERAGE ACCURACY: 66.98 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_18/marbert_finetuned_epochs_10_eval_f1_0.8516_greater_threshold_0.3/UBC-NLP-MARBERT-experiment-18_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balancing the dataset produced by the logistic regression and finetuning marbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = 'Project/Cross-Country-Dialectal-Arabic-Identification/balanced_multilabel_dataset_lr_500.csv'\n",
    "dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"Project/Cross-Country-Dialectal-Arabic-Identification/exp_20/marbert_finetuned_epochs_10_eval_f1_0.8621_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=21\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=20,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 53.12 %\n",
      "MACRO AVERAGE RECALL SCORE: 64.70 %\n",
      "MACRO AVERAGE F1-SCORE: 55.99 %\n",
      "MACRO AVERAGE ACCURACY: 64.48 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"Project/Cross-Country-Dialectal-Arabic-Identification/exp_20/marbert_finetuned_epochs_20_eval_f1_0.8577_greater_threshold_0.3/-home-ali.mekky-Documents-NLP-Project-Cross-Country-Dialectal-Arabic-Identification-exp_20-marbert_finetuned_epochs_10_eval_f1_0.8621_greater_threshold_0.3-experiment-20_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balancing the dataset produced by the logistic regression and finetuning camelbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "  5%|▌         | 319/6380 [00:30<07:29, 13.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5511866211891174, 'eval_f1': 0.7437626750931472, 'eval_roc_auc': 0.6177656031477913, 'eval_accuracy': 0.07529411764705882, 'eval_runtime': 0.7482, 'eval_samples_per_second': 1136.014, 'eval_steps_per_second': 48.114, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 501/6380 [00:47<07:57, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5907, 'grad_norm': 6.361403942108154, 'learning_rate': 4.96e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 638/6380 [00:59<07:41, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49951958656311035, 'eval_f1': 0.7718301581860032, 'eval_roc_auc': 0.6728528798889093, 'eval_accuracy': 0.08235294117647059, 'eval_runtime': 0.5506, 'eval_samples_per_second': 1543.774, 'eval_steps_per_second': 65.383, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 15%|█▌        | 957/6380 [01:28<07:26, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4561315178871155, 'eval_f1': 0.8000838662333578, 'eval_roc_auc': 0.7355714535748665, 'eval_accuracy': 0.0988235294117647, 'eval_runtime': 0.5717, 'eval_samples_per_second': 1486.689, 'eval_steps_per_second': 62.966, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1001/6380 [01:34<07:38, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4842, 'grad_norm': 1.7892683744430542, 'learning_rate': 4.5782312925170066e-05, 'epoch': 3.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 20%|██        | 1276/6380 [01:57<07:04, 12.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4299485385417938, 'eval_f1': 0.8184115126881248, 'eval_roc_auc': 0.7736265977137893, 'eval_accuracy': 0.10235294117647059, 'eval_runtime': 0.5718, 'eval_samples_per_second': 1486.479, 'eval_steps_per_second': 62.957, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 1501/6380 [02:18<06:41, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3728, 'grad_norm': 1.6473968029022217, 'learning_rate': 4.153061224489796e-05, 'epoch': 4.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 25%|██▌       | 1595/6380 [02:26<06:27, 12.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41549763083457947, 'eval_f1': 0.8282073813708261, 'eval_roc_auc': 0.7853263786034731, 'eval_accuracy': 0.11058823529411765, 'eval_runtime': 0.5629, 'eval_samples_per_second': 1510.111, 'eval_steps_per_second': 63.958, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 30%|███       | 1914/6380 [02:55<06:11, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41730809211730957, 'eval_f1': 0.8345218709641192, 'eval_roc_auc': 0.7993547225951115, 'eval_accuracy': 0.12117647058823529, 'eval_runtime': 0.5703, 'eval_samples_per_second': 1490.544, 'eval_steps_per_second': 63.129, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 2001/6380 [03:05<06:09, 11.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2875, 'grad_norm': 1.2916576862335205, 'learning_rate': 3.727891156462585e-05, 'epoch': 6.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 35%|███▌      | 2233/6380 [03:25<05:33, 12.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41028809547424316, 'eval_f1': 0.836555163332773, 'eval_roc_auc': 0.8011264284688202, 'eval_accuracy': 0.11647058823529412, 'eval_runtime': 0.5653, 'eval_samples_per_second': 1503.722, 'eval_steps_per_second': 63.687, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 2501/6380 [03:49<05:24, 11.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2105, 'grad_norm': 1.6996122598648071, 'learning_rate': 3.302721088435374e-05, 'epoch': 7.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 2552/6380 [03:54<05:15, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4211941063404083, 'eval_f1': 0.8384971808184001, 'eval_roc_auc': 0.8023545896758918, 'eval_accuracy': 0.12705882352941175, 'eval_runtime': 0.5642, 'eval_samples_per_second': 1506.544, 'eval_steps_per_second': 63.807, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 45%|████▌     | 2871/6380 [04:23<04:45, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43933889269828796, 'eval_f1': 0.8359919119299034, 'eval_roc_auc': 0.8011575943107271, 'eval_accuracy': 0.1188235294117647, 'eval_runtime': 0.5867, 'eval_samples_per_second': 1448.751, 'eval_steps_per_second': 61.359, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 3001/6380 [04:37<04:42, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1482, 'grad_norm': 1.660212516784668, 'learning_rate': 2.8775510204081635e-05, 'epoch': 9.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 3190/6380 [04:53<04:22, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4412885010242462, 'eval_f1': 0.8401271715680708, 'eval_roc_auc': 0.8090094257064121, 'eval_accuracy': 0.13058823529411764, 'eval_runtime': 0.5685, 'eval_samples_per_second': 1495.264, 'eval_steps_per_second': 63.329, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 3501/6380 [05:21<03:58, 12.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1047, 'grad_norm': 1.1694947481155396, 'learning_rate': 2.4523809523809523e-05, 'epoch': 10.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 55%|█████▌    | 3509/6380 [05:22<03:52, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46076735854148865, 'eval_f1': 0.8412990901762064, 'eval_roc_auc': 0.8142748018182688, 'eval_accuracy': 0.1388235294117647, 'eval_runtime': 0.5929, 'eval_samples_per_second': 1433.657, 'eval_steps_per_second': 60.72, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 3828/6380 [05:52<03:33, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4691307246685028, 'eval_f1': 0.8411923496640055, 'eval_roc_auc': 0.8134435407038278, 'eval_accuracy': 0.1411764705882353, 'eval_runtime': 0.578, 'eval_samples_per_second': 1470.688, 'eval_steps_per_second': 62.288, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 4001/6380 [06:08<03:18, 11.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0726, 'grad_norm': 1.3389629125595093, 'learning_rate': 2.0272108843537416e-05, 'epoch': 12.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 65%|██████▌   | 4147/6380 [06:21<03:01, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48363080620765686, 'eval_f1': 0.8430549968392621, 'eval_roc_auc': 0.815733507696931, 'eval_accuracy': 0.14941176470588236, 'eval_runtime': 0.5698, 'eval_samples_per_second': 1491.861, 'eval_steps_per_second': 63.185, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|███████   | 4466/6380 [06:50<02:39, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4931025505065918, 'eval_f1': 0.843399930707934, 'eval_roc_auc': 0.8174246158139197, 'eval_accuracy': 0.15764705882352942, 'eval_runtime': 0.5832, 'eval_samples_per_second': 1457.583, 'eval_steps_per_second': 61.733, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 4501/6380 [06:55<02:38, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0538, 'grad_norm': 1.497381567955017, 'learning_rate': 1.6020408163265308e-05, 'epoch': 14.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 4785/6380 [07:20<02:09, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5058793425559998, 'eval_f1': 0.844122756224667, 'eval_roc_auc': 0.8189954980838167, 'eval_accuracy': 0.14705882352941177, 'eval_runtime': 0.5678, 'eval_samples_per_second': 1497.078, 'eval_steps_per_second': 63.406, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 5001/6380 [07:40<01:55, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0398, 'grad_norm': 0.752790093421936, 'learning_rate': 1.1768707482993198e-05, 'epoch': 15.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 5104/6380 [07:49<01:45, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5167987942695618, 'eval_f1': 0.8434346346808143, 'eval_roc_auc': 0.8171050111371445, 'eval_accuracy': 0.14705882352941177, 'eval_runtime': 0.5673, 'eval_samples_per_second': 1498.223, 'eval_steps_per_second': 63.454, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 85%|████████▌ | 5423/6380 [08:18<01:17, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5189736485481262, 'eval_f1': 0.8436211602113698, 'eval_roc_auc': 0.8191901297985077, 'eval_accuracy': 0.14823529411764705, 'eval_runtime': 0.593, 'eval_samples_per_second': 1433.499, 'eval_steps_per_second': 60.713, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 5501/6380 [08:27<01:13, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0323, 'grad_norm': 0.48990684747695923, 'learning_rate': 7.5170068027210886e-06, 'epoch': 17.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 5742/6380 [08:48<00:52, 12.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5246774554252625, 'eval_f1': 0.8456507899762717, 'eval_roc_auc': 0.8205923862880203, 'eval_accuracy': 0.14352941176470588, 'eval_runtime': 0.5655, 'eval_samples_per_second': 1503.036, 'eval_steps_per_second': 63.658, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 6001/6380 [09:09<00:28, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0271, 'grad_norm': 0.5851431488990784, 'learning_rate': 3.26530612244898e-06, 'epoch': 18.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 95%|█████████▌| 6061/6380 [09:14<00:22, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5259179472923279, 'eval_f1': 0.8454016298020954, 'eval_roc_auc': 0.8218422191070139, 'eval_accuracy': 0.14823529411764705, 'eval_runtime': 0.507, 'eval_samples_per_second': 1676.544, 'eval_steps_per_second': 71.007, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 6380/6380 [09:42<00:00, 13.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5265424847602844, 'eval_f1': 0.8449036452286974, 'eval_roc_auc': 0.8205344921247426, 'eval_accuracy': 0.14941176470588236, 'eval_runtime': 0.5301, 'eval_samples_per_second': 1603.392, 'eval_steps_per_second': 67.908, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6380/6380 [09:45<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 585.0119, 'train_samples_per_second': 261.533, 'train_steps_per_second': 10.906, 'train_loss': 0.19146193889988627, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_845921/3956158625.py:82: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1000\n",
      "Hamming Loss: 0.3635\n",
      "Micro Precision: 0.5081\n",
      "Micro Recall: 0.6152\n",
      "Micro F1-Score: 0.5565\n",
      "Precision per label: [0.42105263 0.50909091 0.49152542 0.62745098 0.65517241 0.53030303\n",
      " 0.29787234 0.56716418]\n",
      "Recall per label: [0.68571429 0.71794872 0.58       0.5        0.45238095 0.76086957\n",
      " 0.66666667 0.6440678 ]\n",
      "F1-Score per label: [0.52173913 0.59574468 0.53211009 0.55652174 0.53521127 0.625\n",
      " 0.41176471 0.6031746 ]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = 'Project/Cross-Country-Dialectal-Arabic-Identification/balanced_multilabel_dataset_lr_500.csv'\n",
    "dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-ca\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=21\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=20,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 51.25 %\n",
      "MACRO AVERAGE RECALL SCORE: 62.60 %\n",
      "MACRO AVERAGE F1-SCORE: 54.77 %\n",
      "MACRO AVERAGE ACCURACY: 63.65 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"Project/Cross-Country-Dialectal-Arabic-Identification/exp_21/marbert_finetuned_epochs_20_eval_f1_0.8457_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-ca-experiment-21_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freezing lower layers and adding dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 119/22040 [17:08<52:38:49,  8.65s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  2%|▏         | 502/22040 [00:34<25:11, 14.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4872, 'grad_norm': 2.7567391395568848, 'learning_rate': 5e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1002/22040 [01:08<25:13, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4106, 'grad_norm': 2.746913194656372, 'learning_rate': 5e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1502/22040 [01:43<23:29, 14.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3744, 'grad_norm': 3.761868953704834, 'learning_rate': 5e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2002/22040 [02:17<22:55, 14.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3473, 'grad_norm': 3.082247495651245, 'learning_rate': 5e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2204/22040 [02:30<22:20, 14.80it/s]\n",
      " 10%|█         | 2204/22040 [02:35<22:20, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31659334897994995, 'eval_f1': 0.7849032858198134, 'eval_roc_auc': 0.8475967977783262, 'eval_accuracy': 0.13850604049685214, 'eval_runtime': 4.8319, 'eval_samples_per_second': 1216.299, 'eval_steps_per_second': 50.705, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 2502/22040 [02:59<22:55, 14.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3059, 'grad_norm': 4.013148307800293, 'learning_rate': 5e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3002/22040 [03:34<22:17, 14.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2746, 'grad_norm': 4.039422512054443, 'learning_rate': 5e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3502/22040 [04:09<21:44, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2628, 'grad_norm': 3.6592023372650146, 'learning_rate': 5e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4002/22040 [04:44<21:14, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2578, 'grad_norm': 3.9496212005615234, 'learning_rate': 5e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4408/22040 [05:13<20:41, 14.21it/s]\n",
      " 20%|██        | 4408/22040 [05:18<20:41, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2645597755908966, 'eval_f1': 0.8199577972908584, 'eval_roc_auc': 0.8758989645539288, 'eval_accuracy': 0.158924621405479, 'eval_runtime': 4.9064, 'eval_samples_per_second': 1197.82, 'eval_steps_per_second': 49.935, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4502/22040 [05:27<21:32, 13.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.24, 'grad_norm': 3.2590744495391846, 'learning_rate': 5e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5002/22040 [06:02<20:04, 14.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.186, 'grad_norm': 3.98660945892334, 'learning_rate': 5e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 5502/22040 [06:37<17:24, 15.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.183, 'grad_norm': 4.314870834350586, 'learning_rate': 5e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6002/22040 [07:08<16:56, 15.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.183, 'grad_norm': 5.021315574645996, 'learning_rate': 5e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 6502/22040 [07:40<16:21, 15.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1821, 'grad_norm': 4.458516597747803, 'learning_rate': 5e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6612/22040 [07:47<16:12, 15.87it/s]\n",
      " 30%|███       | 6612/22040 [07:51<16:12, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25569793581962585, 'eval_f1': 0.8340886203423967, 'eval_roc_auc': 0.8831668167738843, 'eval_accuracy': 0.19669899608643865, 'eval_runtime': 4.4823, 'eval_samples_per_second': 1311.148, 'eval_steps_per_second': 54.659, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7002/22040 [08:19<15:44, 15.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1389, 'grad_norm': 3.714444875717163, 'learning_rate': 5e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 7502/22040 [08:50<15:16, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1241, 'grad_norm': 4.097795009613037, 'learning_rate': 5e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 8002/22040 [09:21<14:49, 15.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1275, 'grad_norm': 3.6288466453552246, 'learning_rate': 5e-05, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 8502/22040 [09:53<14:18, 15.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1291, 'grad_norm': 3.088449478149414, 'learning_rate': 5e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8816/22040 [10:12<13:49, 15.94it/s]\n",
      " 40%|████      | 8816/22040 [10:17<13:49, 15.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27678731083869934, 'eval_f1': 0.8337880728325631, 'eval_roc_auc': 0.8836161360711593, 'eval_accuracy': 0.19550791220010208, 'eval_runtime': 4.4663, 'eval_samples_per_second': 1315.845, 'eval_steps_per_second': 54.855, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9002/22040 [10:31<13:43, 15.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1122, 'grad_norm': 2.8917648792266846, 'learning_rate': 5e-05, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9502/22040 [11:02<13:13, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0862, 'grad_norm': 3.912954330444336, 'learning_rate': 5e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 10002/22040 [11:34<12:55, 15.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0872, 'grad_norm': 2.9539389610290527, 'learning_rate': 5e-05, 'epoch': 4.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10502/22040 [12:05<12:12, 15.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0885, 'grad_norm': 4.532630920410156, 'learning_rate': 5e-05, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 11002/22040 [12:36<11:37, 15.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.094, 'grad_norm': 4.640472888946533, 'learning_rate': 5e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 11020/22040 [12:38<11:44, 15.63it/s]\n",
      " 50%|█████     | 11020/22040 [12:42<11:44, 15.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31265273690223694, 'eval_f1': 0.8298505777940823, 'eval_roc_auc': 0.8786760389194355, 'eval_accuracy': 0.18291645397311554, 'eval_runtime': 4.5224, 'eval_samples_per_second': 1299.54, 'eval_steps_per_second': 54.175, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 11020/22040 [12:46<12:46, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 766.6979, 'train_samples_per_second': 689.854, 'train_steps_per_second': 28.747, 'train_loss': 0.21260312045983523, 'epoch': 5.0}\n",
      "Subset Accuracy: 0.1000\n",
      "Hamming Loss: 0.3156\n",
      "Micro Precision: 0.5692\n",
      "Micro Recall: 0.6124\n",
      "Micro F1-Score: 0.5900\n",
      "Precision per label: [0.54761905 0.71052632 0.54716981 0.64814815 0.73913043 0.53731343\n",
      " 0.34782609 0.58333333]\n",
      "Recall per label: [0.65714286 0.69230769 0.58       0.546875   0.4047619  0.7826087\n",
      " 0.76190476 0.59322034]\n",
      "F1-Score per label: [0.5974026  0.7012987  0.5631068  0.59322034 0.52307692 0.63716814\n",
      " 0.47761194 0.58823529]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_875058/1135678770.py:76: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = 'Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"UBC-NLP/MARBERT\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=22\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 58.26 %\n",
      "MACRO AVERAGE RECALL SCORE: 62.74 %\n",
      "MACRO AVERAGE F1-SCORE: 58.51 %\n",
      "MACRO AVERAGE ACCURACY: 68.44 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"Project/Cross-Country-Dialectal-Arabic-Identification/exp_22/marbert_finetuned_epochs_10_eval_f1_0.8341_greater_threshold_0.3/UBC-NLP-MARBERT-experiment-22_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as above but using camelbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  2%|▏         | 502/22040 [00:28<20:15, 17.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5649, 'grad_norm': 1.48406183719635, 'learning_rate': 5e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1002/22040 [00:57<20:23, 17.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4982, 'grad_norm': 1.0999702215194702, 'learning_rate': 5e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1502/22040 [01:26<19:48, 17.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4645, 'grad_norm': 1.0644991397857666, 'learning_rate': 5e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2002/22040 [01:55<19:24, 17.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4377, 'grad_norm': 1.2332335710525513, 'learning_rate': 5e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2204/22040 [02:07<19:18, 17.13it/s]\n",
      " 10%|█         | 2204/22040 [02:12<19:18, 17.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4055422842502594, 'eval_f1': 0.7099067198689322, 'eval_roc_auc': 0.7901697753917134, 'eval_accuracy': 0.0781010719754977, 'eval_runtime': 4.9592, 'eval_samples_per_second': 1185.07, 'eval_steps_per_second': 49.403, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 2502/22040 [02:31<19:16, 16.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4105, 'grad_norm': 0.9981072545051575, 'learning_rate': 5e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3002/22040 [03:00<19:02, 16.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3896, 'grad_norm': 2.099914312362671, 'learning_rate': 5e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3502/22040 [03:30<18:27, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.376, 'grad_norm': 1.4147392511367798, 'learning_rate': 5e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4002/22040 [03:59<17:56, 16.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3704, 'grad_norm': 2.7498104572296143, 'learning_rate': 5e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4408/22040 [04:23<17:39, 16.64it/s]\n",
      " 20%|██        | 4408/22040 [04:28<17:39, 16.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3480108380317688, 'eval_f1': 0.7540106951871658, 'eval_roc_auc': 0.8264540125901423, 'eval_accuracy': 0.09783903352050366, 'eval_runtime': 4.9726, 'eval_samples_per_second': 1181.87, 'eval_steps_per_second': 49.27, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4502/22040 [04:35<17:27, 16.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3528, 'grad_norm': 1.365309238433838, 'learning_rate': 5e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5002/22040 [05:05<16:57, 16.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3264, 'grad_norm': 1.3290443420410156, 'learning_rate': 5e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 5502/22040 [05:35<16:46, 16.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3218, 'grad_norm': 2.5377919673919678, 'learning_rate': 5e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6002/22040 [06:05<16:00, 16.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3147, 'grad_norm': 1.8217381238937378, 'learning_rate': 5e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 6502/22040 [06:34<15:43, 16.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3112, 'grad_norm': 1.522733211517334, 'learning_rate': 5e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6612/22040 [06:41<16:10, 15.89it/s]\n",
      " 30%|███       | 6612/22040 [06:46<16:10, 15.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32097285985946655, 'eval_f1': 0.782281024953944, 'eval_roc_auc': 0.8434294707617457, 'eval_accuracy': 0.12948783392887528, 'eval_runtime': 5.0717, 'eval_samples_per_second': 1158.791, 'eval_steps_per_second': 48.308, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7002/22040 [07:12<14:59, 16.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2837, 'grad_norm': 1.4617409706115723, 'learning_rate': 5e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 7502/22040 [07:41<14:41, 16.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.273, 'grad_norm': 1.5124166011810303, 'learning_rate': 5e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 8002/22040 [08:11<14:29, 16.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2737, 'grad_norm': 1.4198979139328003, 'learning_rate': 5e-05, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 8502/22040 [08:41<13:24, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.269, 'grad_norm': 1.253678560256958, 'learning_rate': 5e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8816/22040 [08:59<13:15, 16.62it/s]\n",
      " 40%|████      | 8816/22040 [09:04<13:15, 16.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2930040955543518, 'eval_f1': 0.7997309244666538, 'eval_roc_auc': 0.8588660718502263, 'eval_accuracy': 0.1524587374510805, 'eval_runtime': 5.032, 'eval_samples_per_second': 1167.917, 'eval_steps_per_second': 48.688, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9002/22040 [09:17<13:05, 16.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2539, 'grad_norm': 1.4674493074417114, 'learning_rate': 5e-05, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9502/22040 [09:47<12:26, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2358, 'grad_norm': 1.907118797302246, 'learning_rate': 5e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 10002/22040 [10:16<12:01, 16.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2363, 'grad_norm': 1.558811068534851, 'learning_rate': 5e-05, 'epoch': 4.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10502/22040 [10:46<11:35, 16.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2318, 'grad_norm': 1.9798033237457275, 'learning_rate': 5e-05, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 11002/22040 [11:16<10:59, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2345, 'grad_norm': 1.7607308626174927, 'learning_rate': 5e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 11020/22040 [11:17<11:06, 16.55it/s]\n",
      " 50%|█████     | 11020/22040 [11:22<11:06, 16.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28577277064323425, 'eval_f1': 0.8072094130449569, 'eval_roc_auc': 0.8658987054182624, 'eval_accuracy': 0.15705291815552153, 'eval_runtime': 5.0708, 'eval_samples_per_second': 1158.99, 'eval_steps_per_second': 48.316, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11502/22040 [11:52<10:33, 16.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2022, 'grad_norm': 1.5855951309204102, 'learning_rate': 5e-05, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 12002/22040 [12:22<09:59, 16.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2022, 'grad_norm': 1.5022097826004028, 'learning_rate': 5e-05, 'epoch': 5.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12502/22040 [12:51<09:31, 16.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2038, 'grad_norm': 2.084074020385742, 'learning_rate': 5e-05, 'epoch': 5.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 13002/22040 [13:21<09:05, 16.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2048, 'grad_norm': 1.5258960723876953, 'learning_rate': 5e-05, 'epoch': 5.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 13223/22040 [13:33<07:56, 18.50it/s]\n",
      " 60%|██████    | 13224/22040 [13:37<07:56, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2837856709957123, 'eval_f1': 0.8147543478561645, 'eval_roc_auc': 0.8696570737623799, 'eval_accuracy': 0.16743236345074017, 'eval_runtime': 4.5042, 'eval_samples_per_second': 1304.769, 'eval_steps_per_second': 54.393, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 13503/22040 [13:54<07:34, 18.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1872, 'grad_norm': 1.551269769668579, 'learning_rate': 5e-05, 'epoch': 6.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 14002/22040 [14:20<07:12, 18.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1734, 'grad_norm': 1.4010202884674072, 'learning_rate': 5e-05, 'epoch': 6.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 14502/22040 [14:49<07:28, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1775, 'grad_norm': 1.9627329111099243, 'learning_rate': 5e-05, 'epoch': 6.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 15002/22040 [15:19<07:03, 16.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1772, 'grad_norm': 2.12345027923584, 'learning_rate': 5e-05, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 15428/22040 [15:45<06:38, 16.58it/s]\n",
      " 70%|███████   | 15428/22040 [15:50<06:38, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2869652211666107, 'eval_f1': 0.8176449485332671, 'eval_roc_auc': 0.8722715311141812, 'eval_accuracy': 0.17083546026884464, 'eval_runtime': 5.0575, 'eval_samples_per_second': 1162.034, 'eval_steps_per_second': 48.443, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 15502/22040 [15:56<06:39, 16.38it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1732, 'grad_norm': 1.748171091079712, 'learning_rate': 5e-05, 'epoch': 7.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 16002/22040 [16:25<05:57, 16.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1461, 'grad_norm': 1.665237545967102, 'learning_rate': 5e-05, 'epoch': 7.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 16502/22040 [16:55<05:28, 16.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1513, 'grad_norm': 2.2277581691741943, 'learning_rate': 5e-05, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 17002/22040 [17:24<05:02, 16.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1535, 'grad_norm': 1.8187083005905151, 'learning_rate': 5e-05, 'epoch': 7.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 17502/22040 [17:54<04:30, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1539, 'grad_norm': 2.229557752609253, 'learning_rate': 5e-05, 'epoch': 7.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 17632/22040 [18:02<04:22, 16.82it/s]\n",
      " 80%|████████  | 17632/22040 [18:07<04:22, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29364240169525146, 'eval_f1': 0.8242024327041532, 'eval_roc_auc': 0.8738783607227875, 'eval_accuracy': 0.1970393057682491, 'eval_runtime': 5.0136, 'eval_samples_per_second': 1172.214, 'eval_steps_per_second': 48.867, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 18002/22040 [18:30<04:01, 16.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1374, 'grad_norm': 2.4780025482177734, 'learning_rate': 5e-05, 'epoch': 8.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 18502/22040 [19:00<03:30, 16.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1276, 'grad_norm': 1.5429753065109253, 'learning_rate': 5e-05, 'epoch': 8.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 19002/22040 [19:29<03:01, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1318, 'grad_norm': 1.8818618059158325, 'learning_rate': 5e-05, 'epoch': 8.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 19502/22040 [19:59<02:30, 16.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1332, 'grad_norm': 1.835778832435608, 'learning_rate': 5e-05, 'epoch': 8.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 19836/22040 [20:18<02:09, 16.98it/s]\n",
      " 90%|█████████ | 19836/22040 [20:23<02:09, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30559656023979187, 'eval_f1': 0.8216548012780985, 'eval_roc_auc': 0.8733135069799041, 'eval_accuracy': 0.18478815722307299, 'eval_runtime': 4.9725, 'eval_samples_per_second': 1181.903, 'eval_steps_per_second': 49.271, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 20002/22040 [20:35<02:00, 16.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1266, 'grad_norm': 2.3318207263946533, 'learning_rate': 5e-05, 'epoch': 9.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 20502/22040 [21:04<01:30, 16.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1119, 'grad_norm': 2.0546348094940186, 'learning_rate': 5e-05, 'epoch': 9.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 21002/22040 [21:35<01:05, 15.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.114, 'grad_norm': 2.1959190368652344, 'learning_rate': 5e-05, 'epoch': 9.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 21502/22040 [22:07<00:33, 16.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1166, 'grad_norm': 1.897908091545105, 'learning_rate': 5e-05, 'epoch': 9.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 22002/22040 [22:36<00:02, 16.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1172, 'grad_norm': 2.7256805896759033, 'learning_rate': 5e-05, 'epoch': 9.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22040/22040 [22:39<00:00, 16.71it/s]\n",
      "100%|██████████| 22040/22040 [22:45<00:00, 16.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3176794946193695, 'eval_f1': 0.8254703494024133, 'eval_roc_auc': 0.8735565425375277, 'eval_accuracy': 0.19499744767738642, 'eval_runtime': 4.9762, 'eval_samples_per_second': 1181.022, 'eval_steps_per_second': 49.234, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22040/22040 [22:47<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1367.1415, 'train_samples_per_second': 386.873, 'train_steps_per_second': 16.121, 'train_loss': 0.24643782885667417, 'epoch': 10.0}\n",
      "Subset Accuracy: 0.1083\n",
      "Hamming Loss: 0.3344\n",
      "Micro Precision: 0.5482\n",
      "Micro Recall: 0.5590\n",
      "Micro F1-Score: 0.5535\n",
      "Precision per label: [0.54545455 0.59574468 0.57777778 0.61702128 0.64       0.51612903\n",
      " 0.35714286 0.56862745]\n",
      "Recall per label: [0.68571429 0.71794872 0.52       0.453125   0.38095238 0.69565217\n",
      " 0.71428571 0.49152542]\n",
      "F1-Score per label: [0.60759494 0.65116279 0.54736842 0.52252252 0.47761194 0.59259259\n",
      " 0.47619048 0.52727273]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_875058/1135678770.py:76: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = 'Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-ca\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=23\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 55.22 %\n",
      "MACRO AVERAGE RECALL SCORE: 58.24 %\n",
      "MACRO AVERAGE F1-SCORE: 55.03 %\n",
      "MACRO AVERAGE ACCURACY: 66.56 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"Project/Cross-Country-Dialectal-Arabic-Identification/exp_23/marbert_finetuned_epochs_10_eval_f1_0.8255_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-ca-experiment-23_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1000\n",
      "Hamming Loss: 0.6292\n",
      "Micro Precision: 0.3708\n",
      "Micro Recall: 1.0000\n",
      "Micro F1-Score: 0.5410\n",
      "Precision per label: [0.29166667 0.325      0.41666667 0.53333333 0.35       0.38333333\n",
      " 0.175      0.49166667]\n",
      "Recall per label: [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "F1-Score per label: [0.4516129  0.49056604 0.58823529 0.69565217 0.51851852 0.55421687\n",
      " 0.29787234 0.65921788]\n",
      "{8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_875058/1135678770.py:76: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = 'Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"Project/Cross-Country-Dialectal-Arabic-Identification/exp_5/marbert_finetuned\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=5\n",
    ")\n",
    "\n",
    "trainer.save_dir = f'./exp_{trainer.exp_num}'\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 37.08 %\n",
      "MACRO AVERAGE RECALL SCORE: 100.00 %\n",
      "MACRO AVERAGE F1-SCORE: 53.20 %\n",
      "MACRO AVERAGE ACCURACY: 37.08 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"Project/Cross-Country-Dialectal-Arabic-Identification/exp_5/-home-ali.mekky-Documents-NLP-Project-Cross-Country-Dialectal-Arabic-Identification-exp_5-marbert_finetuned-experiment-5_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling and Undersampling to balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Project/NADI2024/subtask1/multilabel/NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset['Computed'] == 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Bahrain</th>\n",
       "      <th>Egypt</th>\n",
       "      <th>Iraq</th>\n",
       "      <th>Jordan</th>\n",
       "      <th>Kuwait</th>\n",
       "      <th>Lebanon</th>\n",
       "      <th>Libya</th>\n",
       "      <th>...</th>\n",
       "      <th>Oman</th>\n",
       "      <th>Palestine</th>\n",
       "      <th>Qatar</th>\n",
       "      <th>Saudi_Arabia</th>\n",
       "      <th>Sudan</th>\n",
       "      <th>Syria</th>\n",
       "      <th>Tunisia</th>\n",
       "      <th>UAE</th>\n",
       "      <th>Yemen</th>\n",
       "      <th>Computed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>الفار العور يشوف فقط كيسي ومايشوف ماتويد</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ديني ربنا يستر</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>اساسا نسبكم قذر ونجس بلاش تتفاخروا بنجاستكم وه...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>المشاعر تحتاج الي المشاعر تحتاج الي رفيق يخذل ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ني حاضرها لايف</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58742</th>\n",
       "      <td>58734</td>\n",
       "      <td>جماعه الخير المنخفض الي جاي ايام منخفض قوي ماط...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58743</th>\n",
       "      <td>58735</td>\n",
       "      <td>انا بايع الكل وشاري عيونك</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58744</th>\n",
       "      <td>58736</td>\n",
       "      <td>USER USER USER شكلها نست يوم ترامب قال للامريك...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58745</th>\n",
       "      <td>58737</td>\n",
       "      <td>السطلات البنقو الحشيش معاكم URL ΉМĄDĄ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58746</th>\n",
       "      <td>58738</td>\n",
       "      <td>USER USER USER ميرسي نيسو لعقوبه ليك الفانزالع...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31760 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  Algeria  \\\n",
       "0          0           الفار العور يشوف فقط كيسي ومايشوف ماتويد        0   \n",
       "1          1                                     ديني ربنا يستر        0   \n",
       "2          2  اساسا نسبكم قذر ونجس بلاش تتفاخروا بنجاستكم وه...        0   \n",
       "3          3  المشاعر تحتاج الي المشاعر تحتاج الي رفيق يخذل ...        0   \n",
       "4          4                                     ني حاضرها لايف        0   \n",
       "...      ...                                                ...      ...   \n",
       "58742  58734  جماعه الخير المنخفض الي جاي ايام منخفض قوي ماط...        0   \n",
       "58743  58735                          انا بايع الكل وشاري عيونك        0   \n",
       "58744  58736  USER USER USER شكلها نست يوم ترامب قال للامريك...        0   \n",
       "58745  58737              السطلات البنقو الحشيش معاكم URL ΉМĄDĄ        0   \n",
       "58746  58738  USER USER USER ميرسي نيسو لعقوبه ليك الفانزالع...        1   \n",
       "\n",
       "       Bahrain  Egypt  Iraq  Jordan  Kuwait  Lebanon  Libya  ...  Oman  \\\n",
       "0            0      0     0       0       0        0      1  ...     0   \n",
       "1            0      1     1       1       0        1      0  ...     0   \n",
       "2            0      1     1       1       0        1      0  ...     0   \n",
       "3            0      1     0       1       0        1      0  ...     0   \n",
       "4            0      1     0       0       0        0      0  ...     0   \n",
       "...        ...    ...   ...     ...     ...      ...    ...  ...   ...   \n",
       "58742        0      0     0       1       0        1      0  ...     0   \n",
       "58743        0      1     0       1       0        0      0  ...     0   \n",
       "58744        0      0     0       0       0        0      0  ...     0   \n",
       "58745        0      0     0       0       0        0      0  ...     0   \n",
       "58746        0      0     0       0       0        0      0  ...     0   \n",
       "\n",
       "       Palestine  Qatar  Saudi_Arabia  Sudan  Syria  Tunisia  UAE  Yemen  \\\n",
       "0              0      0             0      0      0        0    0      0   \n",
       "1              1      0             0      1      1        0    0      0   \n",
       "2              1      0             1      0      1        0    0      1   \n",
       "3              1      0             0      0      0        0    0      0   \n",
       "4              0      0             1      0      0        0    0      0   \n",
       "...          ...    ...           ...    ...    ...      ...  ...    ...   \n",
       "58742          1      0             0      0      1        0    0      0   \n",
       "58743          1      0             1      1      0        0    0      0   \n",
       "58744          0      0             1      0      0        0    0      0   \n",
       "58745          0      0             0      1      0        0    0      0   \n",
       "58746          0      0             0      0      0        1    0      0   \n",
       "\n",
       "       Computed  \n",
       "0           yes  \n",
       "1           yes  \n",
       "2           yes  \n",
       "3           yes  \n",
       "4           yes  \n",
       "...         ...  \n",
       "58742       yes  \n",
       "58743       yes  \n",
       "58744       yes  \n",
       "58745       yes  \n",
       "58746       yes  \n",
       "\n",
       "[31760 rows x 21 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait', 'Lebanon', \n",
    "                 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar', 'Saudi_Arabia', \n",
    "                 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['dialect_sum'] = dataset[label_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dialect_sum\n",
       "1     14829\n",
       "2      3220\n",
       "4      2580\n",
       "3      2329\n",
       "7      1794\n",
       "6      1709\n",
       "5      1416\n",
       "8      1108\n",
       "9       536\n",
       "15      532\n",
       "18      502\n",
       "12      334\n",
       "13      273\n",
       "14      256\n",
       "10      168\n",
       "11      137\n",
       "17       26\n",
       "16       11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['dialect_sum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_samples = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialect_sum\n",
      "1     14829\n",
      "2      3220\n",
      "3      2329\n",
      "4      2580\n",
      "5      1416\n",
      "6      1709\n",
      "7      1794\n",
      "8      1108\n",
      "9       536\n",
      "10      168\n",
      "11      137\n",
      "12      334\n",
      "13      273\n",
      "14      256\n",
      "15      532\n",
      "18      539\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for index, row in dataset.iterrows():\n",
    "    if row['dialect_sum'] in [16, 17]:\n",
    "        dataset.at[index, 'dialect_sum'] = 18\n",
    "\n",
    "        dataset.loc[index, label_columns] = row[label_columns].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "print(dataset['dialect_sum'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced rows with dialect_sum = 1:\n",
      "Algeria         150\n",
      "Bahrain         150\n",
      "Egypt           150\n",
      "Iraq            150\n",
      "Jordan          150\n",
      "Kuwait          150\n",
      "Lebanon         150\n",
      "Libya           150\n",
      "Morocco         150\n",
      "Oman            150\n",
      "Palestine       150\n",
      "Qatar           150\n",
      "Saudi_Arabia    150\n",
      "Sudan           150\n",
      "Syria           150\n",
      "Tunisia         150\n",
      "UAE             150\n",
      "Yemen           150\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "dialect_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "\n",
    "rows_with_sum_1 = dataset[dataset['dialect_sum'] == 1]\n",
    "\n",
    "\n",
    "balanced_rows = pd.DataFrame()\n",
    "\n",
    "\n",
    "for dialect in dialect_columns:\n",
    "    dialect_rows = rows_with_sum_1[rows_with_sum_1[dialect] == 1]\n",
    "    \n",
    "    resampled_rows = resample(dialect_rows, replace=True, \n",
    "                              n_samples=150, \n",
    "                              random_state=42)\n",
    "    balanced_rows = pd.concat([balanced_rows, resampled_rows])\n",
    "\n",
    "other_rows = dataset[dataset['dialect_sum'] != 1]\n",
    "balanced_dataset = pd.concat([balanced_rows, other_rows]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"Balanced rows with dialect_sum = 1:\")\n",
    "print(balanced_rows[dialect_columns].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Bahrain</th>\n",
       "      <th>Egypt</th>\n",
       "      <th>Iraq</th>\n",
       "      <th>Jordan</th>\n",
       "      <th>Kuwait</th>\n",
       "      <th>Lebanon</th>\n",
       "      <th>Libya</th>\n",
       "      <th>...</th>\n",
       "      <th>Palestine</th>\n",
       "      <th>Qatar</th>\n",
       "      <th>Saudi_Arabia</th>\n",
       "      <th>Sudan</th>\n",
       "      <th>Syria</th>\n",
       "      <th>Tunisia</th>\n",
       "      <th>UAE</th>\n",
       "      <th>Yemen</th>\n",
       "      <th>Computed</th>\n",
       "      <th>dialect_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>6803</td>\n",
       "      <td>السنا بالقوارير رفقا بانفسنا المره حتي نخدش</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55262</th>\n",
       "      <td>55262</td>\n",
       "      <td>USER USER USER لكان نتي جزايريه واش قاعده تبعي...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47614</th>\n",
       "      <td>47614</td>\n",
       "      <td>USER اختي مالكي افهمي مليح باش تهدري راني علي ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25429</th>\n",
       "      <td>24865</td>\n",
       "      <td>ÙÙØ± Ø§Ø®ÙØ§ÙØ§</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>7008</td>\n",
       "      <td>المشي طريق الامير يملاها العبيد يثير حيرتي ولا...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47955</th>\n",
       "      <td>47955</td>\n",
       "      <td>USER حركاتك يبو يمن ياسعم ماتعرفيش تتحاكي صنعاني</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55817</th>\n",
       "      <td>55817</td>\n",
       "      <td>جيش اليمن ركبوه سيكل وهيكلوه فصار كالهيكل</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>3781</td>\n",
       "      <td>تكون المسافات وساءل النقل تشكل عامل مساعدا الت...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>5075</td>\n",
       "      <td>اخرجمعهفيرمضان اللهم انك عفو كريم تحب العفو فا...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55421</th>\n",
       "      <td>55421</td>\n",
       "      <td>رساله تلميذ يمني الي اعزاءي المتحاربين بخصوص ا...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  Algeria  \\\n",
       "6805    6803        السنا بالقوارير رفقا بانفسنا المره حتي نخدش        1   \n",
       "55262  55262  USER USER USER لكان نتي جزايريه واش قاعده تبعي...        1   \n",
       "47614  47614  USER اختي مالكي افهمي مليح باش تهدري راني علي ...        1   \n",
       "25429  24865                                ÙÙ\n",
       "Ø± Ø§Ø®ÙØ§ÙØ§        1   \n",
       "7008    7008  المشي طريق الامير يملاها العبيد يثير حيرتي ولا...        1   \n",
       "...      ...                                                ...      ...   \n",
       "47955  47955   USER حركاتك يبو يمن ياسعم ماتعرفيش تتحاكي صنعاني        0   \n",
       "55817  55817          جيش اليمن ركبوه سيكل وهيكلوه فصار كالهيكل        0   \n",
       "3781    3781  تكون المسافات وساءل النقل تشكل عامل مساعدا الت...        0   \n",
       "5075    5075  اخرجمعهفيرمضان اللهم انك عفو كريم تحب العفو فا...        0   \n",
       "55421  55421  رساله تلميذ يمني الي اعزاءي المتحاربين بخصوص ا...        0   \n",
       "\n",
       "       Bahrain  Egypt  Iraq  Jordan  Kuwait  Lebanon  Libya  ...  Palestine  \\\n",
       "6805         0      0     0       0       0        0      0  ...          0   \n",
       "55262        0      0     0       0       0        0      0  ...          0   \n",
       "47614        0      0     0       0       0        0      0  ...          0   \n",
       "25429        0      0     0       0       0        0      0  ...          0   \n",
       "7008         0      0     0       0       0        0      0  ...          0   \n",
       "...        ...    ...   ...     ...     ...      ...    ...  ...        ...   \n",
       "47955        0      0     0       0       0        0      0  ...          0   \n",
       "55817        0      0     0       0       0        0      0  ...          0   \n",
       "3781         0      0     0       0       0        0      0  ...          0   \n",
       "5075         0      0     0       0       0        0      0  ...          0   \n",
       "55421        0      0     0       0       0        0      0  ...          0   \n",
       "\n",
       "       Qatar  Saudi_Arabia  Sudan  Syria  Tunisia  UAE  Yemen  Computed  \\\n",
       "6805       0             0      0      0        0    0      0       yes   \n",
       "55262      0             0      0      0        0    0      0       yes   \n",
       "47614      0             0      0      0        0    0      0       yes   \n",
       "25429      0             0      0      0        0    0      0       yes   \n",
       "7008       0             0      0      0        0    0      0       yes   \n",
       "...      ...           ...    ...    ...      ...  ...    ...       ...   \n",
       "47955      0             0      0      0        0    0      1       yes   \n",
       "55817      0             0      0      0        0    0      1       yes   \n",
       "3781       0             0      0      0        0    0      1       yes   \n",
       "5075       0             0      0      0        0    0      1       yes   \n",
       "55421      0             0      0      0        0    0      1       yes   \n",
       "\n",
       "       dialect_sum  \n",
       "6805             1  \n",
       "55262            1  \n",
       "47614            1  \n",
       "25429            1  \n",
       "7008             1  \n",
       "...            ...  \n",
       "47955            1  \n",
       "55817            1  \n",
       "3781             1  \n",
       "5075             1  \n",
       "55421            1  \n",
       "\n",
       "[2700 rows x 22 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset distribution:\n",
      "dialect_sum\n",
      "3     1500\n",
      "7     1500\n",
      "14    1500\n",
      "9     1500\n",
      "12    1500\n",
      "2     1500\n",
      "11    1500\n",
      "18    1500\n",
      "10    1500\n",
      "15    1500\n",
      "4     1500\n",
      "8     1500\n",
      "6     1500\n",
      "5     1500\n",
      "13    1500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "desired_samples = 1500\n",
    "\n",
    "filtered_dataset = dataset[dataset['dialect_sum'].isin(list(range(2, 16)) + [18])]\n",
    "\n",
    "\n",
    "balanced_dataset_2 = pd.DataFrame()\n",
    "\n",
    "for value, group in filtered_dataset.groupby('dialect_sum'):\n",
    "    if len(group) > desired_samples:\n",
    "        group = resample(group, replace=False, n_samples=desired_samples, random_state=42)\n",
    "    elif len(group) < desired_samples:\n",
    "        group = resample(group, replace=True, n_samples=desired_samples, random_state=42)\n",
    "    balanced_dataset_2 = pd.concat([balanced_dataset_2, group])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_dataset_2 = balanced_dataset_2.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced dataset distribution:\")\n",
    "print(balanced_dataset_2['dialect_sum'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset['dialect_sum'].value_counts()\n",
    "balanced_dataset = balanced_dataset[balanced_dataset['dialect_sum'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = pd.concat([balanced_dataset, balanced_dataset_2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dialect_sum\n",
       "1     2700\n",
       "3     1500\n",
       "7     1500\n",
       "14    1500\n",
       "9     1500\n",
       "12    1500\n",
       "2     1500\n",
       "11    1500\n",
       "18    1500\n",
       "10    1500\n",
       "15    1500\n",
       "4     1500\n",
       "8     1500\n",
       "6     1500\n",
       "5     1500\n",
       "13    1500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset['dialect_sum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.to_csv(\"Project/NADI2024/subtask1/multilabel/NADIcombined_oversampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6616' max='9450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6614/9450 06:11 < 02:39, 17.80 it/s, Epoch 7.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>0.361579</td>\n",
       "      <td>0.830442</td>\n",
       "      <td>0.835948</td>\n",
       "      <td>0.182143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.323300</td>\n",
       "      <td>0.299061</td>\n",
       "      <td>0.855057</td>\n",
       "      <td>0.860643</td>\n",
       "      <td>0.249603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>0.272866</td>\n",
       "      <td>0.875414</td>\n",
       "      <td>0.882615</td>\n",
       "      <td>0.357937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.270055</td>\n",
       "      <td>0.883791</td>\n",
       "      <td>0.891444</td>\n",
       "      <td>0.438095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>0.282849</td>\n",
       "      <td>0.890102</td>\n",
       "      <td>0.897536</td>\n",
       "      <td>0.475397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.115200</td>\n",
       "      <td>0.269263</td>\n",
       "      <td>0.898637</td>\n",
       "      <td>0.906114</td>\n",
       "      <td>0.516270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.287619</td>\n",
       "      <td>0.897030</td>\n",
       "      <td>0.904455</td>\n",
       "      <td>0.540079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = 'Project/NADI2024/subtask1/multilabel/NADIcombined_oversampled.csv'\n",
    "dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=26\n",
    ")\n",
    "\n",
    "trainer.save_dir = f'./exp_{trainer.exp_num}'\n",
    "\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 70.54 %\n",
      "MACRO AVERAGE RECALL SCORE: 44.46 %\n",
      "MACRO AVERAGE F1-SCORE: 52.90 %\n",
      "MACRO AVERAGE ACCURACY: 72.50 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"Project/Cross-Country-Dialectal-Arabic-Identification/exp_24/marbert_finetuned_epochs_3_eval_f1_0.9034_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-mix-experiment-24_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum-based approach to balance the dataset exp 27-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Project/NADI2024/subtask1/multilabel/NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset['dialect_sum'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 503/557 [00:29<00:03, 17.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2406, 'grad_norm': 0.5045507550239563, 'learning_rate': 5e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 557/557 [00:35<00:00, 11.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11656107008457184, 'eval_f1': 0.636464909023394, 'eval_roc_auc': 0.7819404228313038, 'eval_accuracy': 0.5664194200944033, 'eval_runtime': 1.2789, 'eval_samples_per_second': 1159.604, 'eval_steps_per_second': 48.48, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:37<00:00, 14.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 37.6679, 'train_samples_per_second': 354.307, 'train_steps_per_second': 14.787, 'train_loss': 0.2286419217847622, 'epoch': 1.0}\n",
      "Subset Accuracy: 0.1750\n",
      "Hamming Loss: 0.3500\n",
      "Micro Precision: 0.7273\n",
      "Micro Recall: 0.0899\n",
      "Micro F1-Score: 0.1600\n",
      "Precision per label: [0.         0.68421053 0.         0.         1.         1.\n",
      " 1.         1.        ]\n",
      "Recall per label: [0.         0.66666667 0.         0.         0.04761905 0.02173913\n",
      " 0.0952381  0.01694915]\n",
      "F1-Score per label: [0.         0.67532468 0.         0.         0.09090909 0.04255319\n",
      " 0.17391304 0.03333333]\n",
      "{0, 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/3611314930.py:76: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_1.csv\"\n",
    "dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=27\n",
    ")\n",
    "\n",
    "trainer.save_dir = f'./exp_{trainer.exp_num}'\n",
    "\n",
    "trainer.train(\n",
    "    num_train_epochs=1,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_1.csv\"\n",
    "dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=27\n",
    ")\n",
    "\n",
    "trainer.save_dir = f'./exp_{trainer.exp_num}'\n",
    "\n",
    "trainer.train(\n",
    "    num_train_epochs=1,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Bahrain</th>\n",
       "      <th>Egypt</th>\n",
       "      <th>Iraq</th>\n",
       "      <th>Jordan</th>\n",
       "      <th>Kuwait</th>\n",
       "      <th>Lebanon</th>\n",
       "      <th>Libya</th>\n",
       "      <th>...</th>\n",
       "      <th>Palestine</th>\n",
       "      <th>Qatar</th>\n",
       "      <th>Saudi_Arabia</th>\n",
       "      <th>Sudan</th>\n",
       "      <th>Syria</th>\n",
       "      <th>Tunisia</th>\n",
       "      <th>UAE</th>\n",
       "      <th>Yemen</th>\n",
       "      <th>Computed</th>\n",
       "      <th>dialect_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>الفار العور يشوف فقط كيسي ومايشوف ماتويد</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>هيتقال محروق ومش عارف تعمل زيهم</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>والهوسات الزينه ابيت ماجد ياسين حمدالله سلامه ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>انا الناس المتخلفه اللي بتعمل كده للاسف عايزه ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>مش عارف ليه حظي قليل اوي اتنشن انا وابويا برغم...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58735</th>\n",
       "      <td>58727</td>\n",
       "      <td>publié الحروف العربيه المقايس URL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58737</th>\n",
       "      <td>58729</td>\n",
       "      <td>USER جشع وطمع اصحاب الشركات واستقواءهم علي الع...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58738</th>\n",
       "      <td>58730</td>\n",
       "      <td>USER انت حلم كسرته كني اكسر ساقي حلم القدس رام...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58744</th>\n",
       "      <td>58736</td>\n",
       "      <td>USER USER USER شكلها نست يوم ترامب قال للامريك...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58745</th>\n",
       "      <td>58737</td>\n",
       "      <td>السطلات البنقو الحشيش معاكم URL ΉМĄDĄ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14829 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  Algeria  \\\n",
       "0          0           الفار العور يشوف فقط كيسي ومايشوف ماتويد        0   \n",
       "6          6                    هيتقال محروق ومش عارف تعمل زيهم        0   \n",
       "7          7  والهوسات الزينه ابيت ماجد ياسين حمدالله سلامه ...        0   \n",
       "8          8  انا الناس المتخلفه اللي بتعمل كده للاسف عايزه ...        0   \n",
       "12        12  مش عارف ليه حظي قليل اوي اتنشن انا وابويا برغم...        0   \n",
       "...      ...                                                ...      ...   \n",
       "58735  58727                  publié الحروف العربيه المقايس URL        0   \n",
       "58737  58729  USER جشع وطمع اصحاب الشركات واستقواءهم علي الع...        0   \n",
       "58738  58730  USER انت حلم كسرته كني اكسر ساقي حلم القدس رام...        0   \n",
       "58744  58736  USER USER USER شكلها نست يوم ترامب قال للامريك...        0   \n",
       "58745  58737              السطلات البنقو الحشيش معاكم URL ΉМĄDĄ        0   \n",
       "\n",
       "       Bahrain  Egypt  Iraq  Jordan  Kuwait  Lebanon  Libya  ...  Palestine  \\\n",
       "0            0      0     0       0       0        0      1  ...          0   \n",
       "6            0      1     0       0       0        0      0  ...          0   \n",
       "7            0      0     1       0       0        0      0  ...          0   \n",
       "8            0      1     0       0       0        0      0  ...          0   \n",
       "12           0      1     0       0       0        0      0  ...          0   \n",
       "...        ...    ...   ...     ...     ...      ...    ...  ...        ...   \n",
       "58735        0      0     0       0       0        0      0  ...          0   \n",
       "58737        0      0     0       0       0        0      0  ...          0   \n",
       "58738        0      0     0       0       0        0      0  ...          1   \n",
       "58744        0      0     0       0       0        0      0  ...          0   \n",
       "58745        0      0     0       0       0        0      0  ...          0   \n",
       "\n",
       "       Qatar  Saudi_Arabia  Sudan  Syria  Tunisia  UAE  Yemen  Computed  \\\n",
       "0          0             0      0      0        0    0      0       yes   \n",
       "6          0             0      0      0        0    0      0       yes   \n",
       "7          0             0      0      0        0    0      0       yes   \n",
       "8          0             0      0      0        0    0      0       yes   \n",
       "12         0             0      0      0        0    0      0       yes   \n",
       "...      ...           ...    ...    ...      ...  ...    ...       ...   \n",
       "58735      0             0      0      0        1    0      0       yes   \n",
       "58737      0             0      1      0        0    0      0       yes   \n",
       "58738      0             0      0      0        0    0      0       yes   \n",
       "58744      0             1      0      0        0    0      0       yes   \n",
       "58745      0             0      1      0        0    0      0       yes   \n",
       "\n",
       "       dialect_sum  \n",
       "0                1  \n",
       "6                1  \n",
       "7                1  \n",
       "8                1  \n",
       "12               1  \n",
       "...            ...  \n",
       "58735            1  \n",
       "58737            1  \n",
       "58738            1  \n",
       "58744            1  \n",
       "58745            1  \n",
       "\n",
       "[14829 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_countries(n_samples,dataset):\n",
    "    dialect_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "\n",
    "    rows_with_sum_1 = dataset[dataset['dialect_sum'] == 1]\n",
    "\n",
    "\n",
    "    balanced_rows = pd.DataFrame()\n",
    "\n",
    "    for dialect in dialect_columns:\n",
    "     \n",
    "        dialect_rows = rows_with_sum_1[rows_with_sum_1[dialect] == 1]\n",
    "        \n",
    "    \n",
    "        resampled_rows = resample(dialect_rows, replace=True, \n",
    "                                n_samples=n_samples, \n",
    "                                random_state=42)\n",
    "        balanced_rows = pd.concat([balanced_rows, resampled_rows])\n",
    "\n",
    "\n",
    "    return balanced_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3220\n",
      "dialect_sum\n",
      "2     3220\n",
      "14     256\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 3):\n",
    "    current_rows = dataset[dataset['dialect_sum'] == i]\n",
    "    print(len(current_rows))\n",
    "    for j in range(1, i):\n",
    "        if j == 1:\n",
    "            rows_with_sum_j = balance_countries(len(current_rows)//18, dataset)\n",
    "        else:\n",
    "            rows_with_sum_j = dataset[dataset[\"dialect_sum\"] == j]\n",
    "            if len(rows_with_sum_j) > len(current_rows):\n",
    "                resampled_rows = resample(rows_with_sum_j, replace=True, \n",
    "                            n_samples=len(current_rows), \n",
    "                            random_state=42)\n",
    "            else:\n",
    "                resampled_rows = rows_with_sum_j\n",
    "        current_rows = pd.concat([current_rows, resampled_rows])\n",
    "        print(current_rows['dialect_sum'].value_counts())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Bahrain</th>\n",
       "      <th>Egypt</th>\n",
       "      <th>Iraq</th>\n",
       "      <th>Jordan</th>\n",
       "      <th>Kuwait</th>\n",
       "      <th>Lebanon</th>\n",
       "      <th>Libya</th>\n",
       "      <th>...</th>\n",
       "      <th>Palestine</th>\n",
       "      <th>Qatar</th>\n",
       "      <th>Saudi_Arabia</th>\n",
       "      <th>Sudan</th>\n",
       "      <th>Syria</th>\n",
       "      <th>Tunisia</th>\n",
       "      <th>UAE</th>\n",
       "      <th>Yemen</th>\n",
       "      <th>Computed</th>\n",
       "      <th>dialect_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>الفار العور يشوف فقط كيسي ومايشوف ماتويد</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ديني ربنا يستر</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>اساسا نسبكم قذر ونجس بلاش تتفاخروا بنجاستكم وه...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>المشاعر تحتاج الي المشاعر تحتاج الي رفيق يخذل ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ني حاضرها لايف</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58742</th>\n",
       "      <td>58734</td>\n",
       "      <td>جماعه الخير المنخفض الي جاي ايام منخفض قوي ماط...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58743</th>\n",
       "      <td>58735</td>\n",
       "      <td>انا بايع الكل وشاري عيونك</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58744</th>\n",
       "      <td>58736</td>\n",
       "      <td>USER USER USER شكلها نست يوم ترامب قال للامريك...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58745</th>\n",
       "      <td>58737</td>\n",
       "      <td>السطلات البنقو الحشيش معاكم URL ΉМĄDĄ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58746</th>\n",
       "      <td>58738</td>\n",
       "      <td>USER USER USER ميرسي نيسو لعقوبه ليك الفانزالع...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31760 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  Algeria  \\\n",
       "0          0           الفار العور يشوف فقط كيسي ومايشوف ماتويد        0   \n",
       "1          1                                     ديني ربنا يستر        0   \n",
       "2          2  اساسا نسبكم قذر ونجس بلاش تتفاخروا بنجاستكم وه...        0   \n",
       "3          3  المشاعر تحتاج الي المشاعر تحتاج الي رفيق يخذل ...        0   \n",
       "4          4                                     ني حاضرها لايف        0   \n",
       "...      ...                                                ...      ...   \n",
       "58742  58734  جماعه الخير المنخفض الي جاي ايام منخفض قوي ماط...        0   \n",
       "58743  58735                          انا بايع الكل وشاري عيونك        0   \n",
       "58744  58736  USER USER USER شكلها نست يوم ترامب قال للامريك...        0   \n",
       "58745  58737              السطلات البنقو الحشيش معاكم URL ΉМĄDĄ        0   \n",
       "58746  58738  USER USER USER ميرسي نيسو لعقوبه ليك الفانزالع...        1   \n",
       "\n",
       "       Bahrain  Egypt  Iraq  Jordan  Kuwait  Lebanon  Libya  ...  Palestine  \\\n",
       "0            0      0     0       0       0        0      1  ...          0   \n",
       "1            0      1     1       1       0        1      0  ...          1   \n",
       "2            0      1     1       1       0        1      0  ...          1   \n",
       "3            0      1     0       1       0        1      0  ...          1   \n",
       "4            0      1     0       0       0        0      0  ...          0   \n",
       "...        ...    ...   ...     ...     ...      ...    ...  ...        ...   \n",
       "58742        0      0     0       1       0        1      0  ...          1   \n",
       "58743        0      1     0       1       0        0      0  ...          1   \n",
       "58744        0      0     0       0       0        0      0  ...          0   \n",
       "58745        0      0     0       0       0        0      0  ...          0   \n",
       "58746        0      0     0       0       0        0      0  ...          0   \n",
       "\n",
       "       Qatar  Saudi_Arabia  Sudan  Syria  Tunisia  UAE  Yemen  Computed  \\\n",
       "0          0             0      0      0        0    0      0       yes   \n",
       "1          0             0      1      1        0    0      0       yes   \n",
       "2          0             1      0      1        0    0      1       yes   \n",
       "3          0             0      0      0        0    0      0       yes   \n",
       "4          0             1      0      0        0    0      0       yes   \n",
       "...      ...           ...    ...    ...      ...  ...    ...       ...   \n",
       "58742      0             0      0      1        0    0      0       yes   \n",
       "58743      0             1      1      0        0    0      0       yes   \n",
       "58744      0             1      0      0        0    0      0       yes   \n",
       "58745      0             0      1      0        0    0      0       yes   \n",
       "58746      0             0      0      0        1    0      0       yes   \n",
       "\n",
       "       dialect_sum  \n",
       "0                1  \n",
       "1                7  \n",
       "2                8  \n",
       "3                4  \n",
       "4                2  \n",
       "...            ...  \n",
       "58742            4  \n",
       "58743            5  \n",
       "58744            1  \n",
       "58745            1  \n",
       "58746            3  \n",
       "\n",
       "[31760 rows x 22 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows for dialect_sum = 2: 3220\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "2    3220\n",
      "1    3204\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 3: 2329\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "3    2329\n",
      "1    2322\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "3    2329\n",
      "2    2329\n",
      "1    2322\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 4: 2580\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "4    2580\n",
      "1    2574\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "4    2580\n",
      "2    2580\n",
      "1    2574\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "4    2580\n",
      "2    2580\n",
      "1    2574\n",
      "3    2329\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 5: 1416\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "5    1416\n",
      "1    1404\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "5    1416\n",
      "2    1416\n",
      "1    1404\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "5    1416\n",
      "2    1416\n",
      "3    1416\n",
      "1    1404\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "5    1416\n",
      "2    1416\n",
      "3    1416\n",
      "4    1416\n",
      "1    1404\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 6: 1709\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "6    1709\n",
      "1    1692\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "6    1709\n",
      "2    1709\n",
      "1    1692\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "6    1709\n",
      "2    1709\n",
      "3    1709\n",
      "1    1692\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "6    1709\n",
      "2    1709\n",
      "3    1709\n",
      "4    1709\n",
      "1    1692\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "6    1709\n",
      "2    1709\n",
      "4    1709\n",
      "3    1709\n",
      "1    1692\n",
      "5    1416\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 7: 1794\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "7    1794\n",
      "1    1782\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "7    1794\n",
      "2    1794\n",
      "1    1782\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "7    1794\n",
      "2    1794\n",
      "3    1794\n",
      "1    1782\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "7    1794\n",
      "2    1794\n",
      "3    1794\n",
      "4    1794\n",
      "1    1782\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "7    1794\n",
      "2    1794\n",
      "4    1794\n",
      "3    1794\n",
      "1    1782\n",
      "5    1416\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 6:\n",
      "dialect_sum\n",
      "7    1794\n",
      "2    1794\n",
      "3    1794\n",
      "4    1794\n",
      "1    1782\n",
      "6    1709\n",
      "5    1416\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 8: 1108\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "8    1108\n",
      "1    1098\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "8    1108\n",
      "2    1108\n",
      "1    1098\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "8    1108\n",
      "2    1108\n",
      "3    1108\n",
      "1    1098\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "8    1108\n",
      "2    1108\n",
      "3    1108\n",
      "4    1108\n",
      "1    1098\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "8    1108\n",
      "2    1108\n",
      "4    1108\n",
      "3    1108\n",
      "5    1108\n",
      "1    1098\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 6:\n",
      "dialect_sum\n",
      "8    1108\n",
      "2    1108\n",
      "3    1108\n",
      "5    1108\n",
      "4    1108\n",
      "6    1108\n",
      "1    1098\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 7:\n",
      "dialect_sum\n",
      "8    1108\n",
      "2    1108\n",
      "3    1108\n",
      "4    1108\n",
      "6    1108\n",
      "5    1108\n",
      "7    1108\n",
      "1    1098\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 9: 536\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "9    536\n",
      "1    522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "9    536\n",
      "2    536\n",
      "1    522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "9    536\n",
      "2    536\n",
      "3    536\n",
      "1    522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "9    536\n",
      "2    536\n",
      "3    536\n",
      "4    536\n",
      "1    522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "9    536\n",
      "2    536\n",
      "4    536\n",
      "3    536\n",
      "5    536\n",
      "1    522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 6:\n",
      "dialect_sum\n",
      "9    536\n",
      "2    536\n",
      "3    536\n",
      "5    536\n",
      "4    536\n",
      "6    536\n",
      "1    522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 7:\n",
      "dialect_sum\n",
      "9    536\n",
      "2    536\n",
      "3    536\n",
      "4    536\n",
      "6    536\n",
      "5    536\n",
      "7    536\n",
      "1    522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 8:\n",
      "dialect_sum\n",
      "9    536\n",
      "2    536\n",
      "3    536\n",
      "4    536\n",
      "5    536\n",
      "7    536\n",
      "6    536\n",
      "8    536\n",
      "1    522\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 10: 168\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "10    168\n",
      "1     162\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "10    168\n",
      "2     168\n",
      "1     162\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "10    168\n",
      "2     168\n",
      "3     168\n",
      "1     162\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "10    168\n",
      "2     168\n",
      "3     168\n",
      "4     168\n",
      "1     162\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "10    168\n",
      "2     168\n",
      "4     168\n",
      "3     168\n",
      "5     168\n",
      "1     162\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 6:\n",
      "dialect_sum\n",
      "10    168\n",
      "2     168\n",
      "3     168\n",
      "5     168\n",
      "4     168\n",
      "6     168\n",
      "1     162\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 7:\n",
      "dialect_sum\n",
      "10    168\n",
      "2     168\n",
      "3     168\n",
      "4     168\n",
      "6     168\n",
      "5     168\n",
      "7     168\n",
      "1     162\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 8:\n",
      "dialect_sum\n",
      "10    168\n",
      "2     168\n",
      "3     168\n",
      "4     168\n",
      "5     168\n",
      "7     168\n",
      "6     168\n",
      "8     168\n",
      "1     162\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 9:\n",
      "dialect_sum\n",
      "10    168\n",
      "2     168\n",
      "6     168\n",
      "3     168\n",
      "4     168\n",
      "5     168\n",
      "8     168\n",
      "7     168\n",
      "9     168\n",
      "1     162\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 11: 137\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "11    137\n",
      "1     126\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "11    137\n",
      "2     137\n",
      "1     126\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "11    137\n",
      "2     137\n",
      "3     137\n",
      "1     126\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "11    137\n",
      "2     137\n",
      "3     137\n",
      "4     137\n",
      "1     126\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "11    137\n",
      "2     137\n",
      "4     137\n",
      "3     137\n",
      "5     137\n",
      "1     126\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 6:\n",
      "dialect_sum\n",
      "11    137\n",
      "2     137\n",
      "3     137\n",
      "5     137\n",
      "4     137\n",
      "6     137\n",
      "1     126\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 7:\n",
      "dialect_sum\n",
      "11    137\n",
      "2     137\n",
      "3     137\n",
      "4     137\n",
      "6     137\n",
      "5     137\n",
      "7     137\n",
      "1     126\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 8:\n",
      "dialect_sum\n",
      "11    137\n",
      "2     137\n",
      "3     137\n",
      "4     137\n",
      "5     137\n",
      "7     137\n",
      "6     137\n",
      "8     137\n",
      "1     126\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 9:\n",
      "dialect_sum\n",
      "11    137\n",
      "2     137\n",
      "6     137\n",
      "3     137\n",
      "4     137\n",
      "5     137\n",
      "8     137\n",
      "7     137\n",
      "9     137\n",
      "1     126\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 10:\n",
      "dialect_sum\n",
      "11    137\n",
      "2     137\n",
      "3     137\n",
      "7     137\n",
      "4     137\n",
      "5     137\n",
      "6     137\n",
      "9     137\n",
      "8     137\n",
      "10    137\n",
      "1     126\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 12: 334\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "12    334\n",
      "1     324\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "12    334\n",
      "2     334\n",
      "1     324\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "12    334\n",
      "2     334\n",
      "3     334\n",
      "1     324\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "12    334\n",
      "2     334\n",
      "3     334\n",
      "4     334\n",
      "1     324\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "12    334\n",
      "2     334\n",
      "4     334\n",
      "3     334\n",
      "5     334\n",
      "1     324\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 6:\n",
      "dialect_sum\n",
      "12    334\n",
      "2     334\n",
      "3     334\n",
      "5     334\n",
      "4     334\n",
      "6     334\n",
      "1     324\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 7:\n",
      "dialect_sum\n",
      "12    334\n",
      "2     334\n",
      "3     334\n",
      "4     334\n",
      "6     334\n",
      "5     334\n",
      "7     334\n",
      "1     324\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 8:\n",
      "dialect_sum\n",
      "12    334\n",
      "2     334\n",
      "3     334\n",
      "4     334\n",
      "5     334\n",
      "7     334\n",
      "6     334\n",
      "8     334\n",
      "1     324\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 9:\n",
      "dialect_sum\n",
      "12    334\n",
      "2     334\n",
      "6     334\n",
      "3     334\n",
      "4     334\n",
      "5     334\n",
      "8     334\n",
      "7     334\n",
      "9     334\n",
      "1     324\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 10:\n",
      "dialect_sum\n",
      "12    334\n",
      "2     334\n",
      "3     334\n",
      "7     334\n",
      "4     334\n",
      "5     334\n",
      "6     334\n",
      "9     334\n",
      "8     334\n",
      "1     324\n",
      "10    168\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 11:\n",
      "dialect_sum\n",
      "12    334\n",
      "2     334\n",
      "3     334\n",
      "4     334\n",
      "8     334\n",
      "5     334\n",
      "6     334\n",
      "7     334\n",
      "9     334\n",
      "1     324\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 13: 273\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "13    273\n",
      "1     270\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "1     270\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "3     273\n",
      "1     270\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "3     273\n",
      "4     273\n",
      "1     270\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "4     273\n",
      "3     273\n",
      "5     273\n",
      "1     270\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 6:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "3     273\n",
      "5     273\n",
      "4     273\n",
      "6     273\n",
      "1     270\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 7:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "3     273\n",
      "4     273\n",
      "6     273\n",
      "5     273\n",
      "7     273\n",
      "1     270\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 8:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "3     273\n",
      "4     273\n",
      "5     273\n",
      "7     273\n",
      "6     273\n",
      "8     273\n",
      "1     270\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 9:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "6     273\n",
      "3     273\n",
      "4     273\n",
      "5     273\n",
      "8     273\n",
      "7     273\n",
      "9     273\n",
      "1     270\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 10:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "3     273\n",
      "7     273\n",
      "4     273\n",
      "5     273\n",
      "6     273\n",
      "9     273\n",
      "8     273\n",
      "1     270\n",
      "10    168\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 11:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "3     273\n",
      "4     273\n",
      "8     273\n",
      "5     273\n",
      "6     273\n",
      "7     273\n",
      "9     273\n",
      "1     270\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 12:\n",
      "dialect_sum\n",
      "13    273\n",
      "2     273\n",
      "3     273\n",
      "4     273\n",
      "5     273\n",
      "9     273\n",
      "6     273\n",
      "7     273\n",
      "8     273\n",
      "12    273\n",
      "1     270\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 14: 256\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "14    256\n",
      "1     252\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "1     252\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "3     256\n",
      "1     252\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "3     256\n",
      "4     256\n",
      "1     252\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "4     256\n",
      "3     256\n",
      "5     256\n",
      "1     252\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 6:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "3     256\n",
      "5     256\n",
      "4     256\n",
      "6     256\n",
      "1     252\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 7:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "3     256\n",
      "4     256\n",
      "6     256\n",
      "5     256\n",
      "7     256\n",
      "1     252\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 8:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "3     256\n",
      "4     256\n",
      "5     256\n",
      "7     256\n",
      "6     256\n",
      "8     256\n",
      "1     252\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 9:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "6     256\n",
      "3     256\n",
      "4     256\n",
      "5     256\n",
      "8     256\n",
      "7     256\n",
      "9     256\n",
      "1     252\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 10:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "3     256\n",
      "7     256\n",
      "4     256\n",
      "5     256\n",
      "6     256\n",
      "9     256\n",
      "8     256\n",
      "1     252\n",
      "10    168\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 11:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "3     256\n",
      "4     256\n",
      "8     256\n",
      "5     256\n",
      "6     256\n",
      "7     256\n",
      "9     256\n",
      "1     252\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 12:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "3     256\n",
      "4     256\n",
      "5     256\n",
      "9     256\n",
      "6     256\n",
      "7     256\n",
      "8     256\n",
      "12    256\n",
      "1     252\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 13:\n",
      "dialect_sum\n",
      "14    256\n",
      "2     256\n",
      "4     256\n",
      "3     256\n",
      "5     256\n",
      "6     256\n",
      "13    256\n",
      "7     256\n",
      "8     256\n",
      "9     256\n",
      "12    256\n",
      "1     252\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 15: 532\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "15    532\n",
      "1     522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "1     522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "3     532\n",
      "1     522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "3     532\n",
      "4     532\n",
      "1     522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "4     532\n",
      "3     532\n",
      "5     532\n",
      "1     522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 6:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "3     532\n",
      "5     532\n",
      "4     532\n",
      "6     532\n",
      "1     522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 7:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "3     532\n",
      "4     532\n",
      "6     532\n",
      "5     532\n",
      "7     532\n",
      "1     522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 8:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "3     532\n",
      "4     532\n",
      "5     532\n",
      "7     532\n",
      "6     532\n",
      "8     532\n",
      "1     522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 9:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "6     532\n",
      "3     532\n",
      "4     532\n",
      "5     532\n",
      "8     532\n",
      "7     532\n",
      "9     532\n",
      "1     522\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 10:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "3     532\n",
      "7     532\n",
      "4     532\n",
      "5     532\n",
      "6     532\n",
      "9     532\n",
      "8     532\n",
      "1     522\n",
      "10    168\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 11:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "3     532\n",
      "4     532\n",
      "8     532\n",
      "5     532\n",
      "6     532\n",
      "7     532\n",
      "9     532\n",
      "1     522\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 12:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "3     532\n",
      "4     532\n",
      "5     532\n",
      "9     532\n",
      "6     532\n",
      "7     532\n",
      "8     532\n",
      "1     522\n",
      "12    334\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 13:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "4     532\n",
      "3     532\n",
      "5     532\n",
      "6     532\n",
      "8     532\n",
      "7     532\n",
      "9     532\n",
      "1     522\n",
      "12    334\n",
      "13    273\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 14:\n",
      "dialect_sum\n",
      "15    532\n",
      "2     532\n",
      "3     532\n",
      "5     532\n",
      "4     532\n",
      "6     532\n",
      "7     532\n",
      "9     532\n",
      "8     532\n",
      "1     522\n",
      "12    334\n",
      "13    273\n",
      "14    256\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Initial rows for dialect_sum = 18: 502\n",
      "Dialect_sum counts after combining with sum 1:\n",
      "dialect_sum\n",
      "18    502\n",
      "1     486\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 2:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "1     486\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 3:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "1     486\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 4:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "4     502\n",
      "1     486\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 5:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "4     502\n",
      "3     502\n",
      "5     502\n",
      "1     486\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 6:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "5     502\n",
      "4     502\n",
      "6     502\n",
      "1     486\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 7:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "4     502\n",
      "6     502\n",
      "5     502\n",
      "7     502\n",
      "1     486\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 8:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "4     502\n",
      "5     502\n",
      "7     502\n",
      "6     502\n",
      "8     502\n",
      "1     486\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 9:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "6     502\n",
      "3     502\n",
      "4     502\n",
      "5     502\n",
      "8     502\n",
      "7     502\n",
      "9     502\n",
      "1     486\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 10:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "7     502\n",
      "4     502\n",
      "5     502\n",
      "6     502\n",
      "9     502\n",
      "8     502\n",
      "1     486\n",
      "10    168\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 11:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "4     502\n",
      "8     502\n",
      "5     502\n",
      "6     502\n",
      "7     502\n",
      "9     502\n",
      "1     486\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 12:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "4     502\n",
      "5     502\n",
      "9     502\n",
      "6     502\n",
      "7     502\n",
      "8     502\n",
      "1     486\n",
      "12    334\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 13:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "4     502\n",
      "3     502\n",
      "5     502\n",
      "6     502\n",
      "8     502\n",
      "7     502\n",
      "9     502\n",
      "1     486\n",
      "12    334\n",
      "13    273\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 14:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "5     502\n",
      "4     502\n",
      "6     502\n",
      "7     502\n",
      "9     502\n",
      "8     502\n",
      "1     486\n",
      "12    334\n",
      "13    273\n",
      "14    256\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 15:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "4     502\n",
      "6     502\n",
      "5     502\n",
      "7     502\n",
      "8     502\n",
      "15    502\n",
      "9     502\n",
      "1     486\n",
      "12    334\n",
      "13    273\n",
      "14    256\n",
      "10    168\n",
      "11    137\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 16:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "3     502\n",
      "4     502\n",
      "5     502\n",
      "7     502\n",
      "6     502\n",
      "8     502\n",
      "9     502\n",
      "15    502\n",
      "1     486\n",
      "12    334\n",
      "13    273\n",
      "14    256\n",
      "10    168\n",
      "11    137\n",
      "16     11\n",
      "Name: count, dtype: int64\n",
      "Dialect_sum counts after combining with sum 17:\n",
      "dialect_sum\n",
      "18    502\n",
      "2     502\n",
      "15    502\n",
      "3     502\n",
      "4     502\n",
      "5     502\n",
      "6     502\n",
      "7     502\n",
      "8     502\n",
      "9     502\n",
      "1     486\n",
      "12    334\n",
      "13    273\n",
      "14    256\n",
      "10    168\n",
      "11    137\n",
      "17     26\n",
      "16     11\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Bahrain</th>\n",
       "      <th>Egypt</th>\n",
       "      <th>Iraq</th>\n",
       "      <th>Jordan</th>\n",
       "      <th>Kuwait</th>\n",
       "      <th>Lebanon</th>\n",
       "      <th>Libya</th>\n",
       "      <th>...</th>\n",
       "      <th>Palestine</th>\n",
       "      <th>Qatar</th>\n",
       "      <th>Saudi_Arabia</th>\n",
       "      <th>Sudan</th>\n",
       "      <th>Syria</th>\n",
       "      <th>Tunisia</th>\n",
       "      <th>UAE</th>\n",
       "      <th>Yemen</th>\n",
       "      <th>Computed</th>\n",
       "      <th>dialect_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>انا لله انا اليه راجعون الله يرحمه يصبر ذويه</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>التقيكم قليل مباراه برشلونهريالمدريد كونوا علي...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>ولن يخلف الله وعده</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>الله يرحمه ويرحم شهداءنا فخرنا وعزنا</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83</td>\n",
       "      <td>اللهم امين جميعا يارب</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6706</th>\n",
       "      <td>8331</td>\n",
       "      <td>تجعلوا الله الها اخر اني نذير مبين الذاريات51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6707</th>\n",
       "      <td>21928</td>\n",
       "      <td>يوم مليء بالمتخلفين كرويا يلا وقت نوم</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6708</th>\n",
       "      <td>23605</td>\n",
       "      <td>ÙØ§ÙÙ Ø¯Ø®Ù Ø¨ÙÙÙ ÙÙÙÙ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6709</th>\n",
       "      <td>51848</td>\n",
       "      <td>USER انا خلصت الزهايمر الاصلي وصلت لمرحله الزه...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6710</th>\n",
       "      <td>57499</td>\n",
       "      <td>USER الله يحفظه ويطول بعمره</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6711 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              tweet  Algeria  \\\n",
       "0        34       انا لله انا اليه راجعون الله يرحمه يصبر ذويه        1   \n",
       "1        53  التقيكم قليل مباراه برشلونهريالمدريد كونوا علي...        1   \n",
       "2        64                                 ولن يخلف الله وعده        1   \n",
       "3        70               الله يرحمه ويرحم شهداءنا فخرنا وعزنا        1   \n",
       "4        83                              اللهم امين جميعا يارب        1   \n",
       "...     ...                                                ...      ...   \n",
       "6706   8331      تجعلوا الله الها اخر اني نذير مبين الذاريات51        1   \n",
       "6707  21928              يوم مليء بالمتخلفين كرويا يلا وقت نوم        1   \n",
       "6708  23605                  Ù\n",
       "Ø§ÙÙ Ø¯Ø®Ù Ø¨ÙÙÙ\n",
       " ÙÙÙÙ\n",
       "        1   \n",
       "6709  51848  USER انا خلصت الزهايمر الاصلي وصلت لمرحله الزه...        1   \n",
       "6710  57499                        USER الله يحفظه ويطول بعمره        1   \n",
       "\n",
       "      Bahrain  Egypt  Iraq  Jordan  Kuwait  Lebanon  Libya  ...  Palestine  \\\n",
       "0           1      1     1       1       1        1      1  ...          1   \n",
       "1           1      1     1       1       1        1      1  ...          1   \n",
       "2           1      1     1       1       1        1      1  ...          1   \n",
       "3           1      1     1       1       1        1      1  ...          1   \n",
       "4           1      1     1       1       1        1      1  ...          1   \n",
       "...       ...    ...   ...     ...     ...      ...    ...  ...        ...   \n",
       "6706        1      1     1       1       1        1      0  ...          1   \n",
       "6707        1      1     1       1       1        1      1  ...          1   \n",
       "6708        1      1     1       1       1        1      1  ...          1   \n",
       "6709        1      1     1       1       1        1      0  ...          1   \n",
       "6710        1      1     1       1       1        1      1  ...          1   \n",
       "\n",
       "      Qatar  Saudi_Arabia  Sudan  Syria  Tunisia  UAE  Yemen  Computed  \\\n",
       "0         1             1      1      1        1    1      1       yes   \n",
       "1         1             1      1      1        1    1      1       yes   \n",
       "2         1             1      1      1        1    1      1       yes   \n",
       "3         1             1      1      1        1    1      1       yes   \n",
       "4         1             1      1      1        1    1      1       yes   \n",
       "...     ...           ...    ...    ...      ...  ...    ...       ...   \n",
       "6706      1             1      1      1        1    1      1       yes   \n",
       "6707      1             1      1      1        1    1      1       yes   \n",
       "6708      1             1      1      1        1    1      1       yes   \n",
       "6709      1             1      1      1        1    1      1       yes   \n",
       "6710      1             1      1      1        0    1      1       yes   \n",
       "\n",
       "      dialect_sum  \n",
       "0              18  \n",
       "1              18  \n",
       "2              18  \n",
       "3              18  \n",
       "4              18  \n",
       "...           ...  \n",
       "6706           17  \n",
       "6707           17  \n",
       "6708           17  \n",
       "6709           17  \n",
       "6710           17  \n",
       "\n",
       "[6711 rows x 22 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "def balance_countries(n_samples, dataset):\n",
    "    dialect_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "                       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "                       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "\n",
    "    rows_with_sum_1 = dataset[dataset['dialect_sum'] == 1]\n",
    "\n",
    " \n",
    "    balanced_rows = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for dialect in dialect_columns:\n",
    "    \n",
    "        dialect_rows = rows_with_sum_1[rows_with_sum_1[dialect] == 1]\n",
    "        \n",
    "        if len(dialect_rows) >= n_samples:\n",
    "            resampled_rows = resample(dialect_rows, replace=False, \n",
    "                                      n_samples=n_samples, \n",
    "                                      random_state=42)\n",
    "        else:\n",
    "           \n",
    "            resampled_rows = resample(dialect_rows, replace=True, \n",
    "                                      n_samples=n_samples, \n",
    "                                      random_state=42)\n",
    "        \n",
    "        balanced_rows = pd.concat([balanced_rows, resampled_rows], ignore_index=True)\n",
    "\n",
    "    return balanced_rows\n",
    "\n",
    "\n",
    "\n",
    "for i in list(range(2, 16)) + [18]:  \n",
    "    \n",
    "    current_rows = dataset[dataset['dialect_sum'] == i]\n",
    "    print(f\"Initial rows for dialect_sum = {i}: {len(current_rows)}\")\n",
    "    n = len(current_rows)\n",
    "    for j in range(1, i):  \n",
    "        if j == 1:\n",
    "            \n",
    "            resampled_rows = balance_countries(n// 18, dataset)\n",
    "        else:\n",
    "            \n",
    "            rows_with_sum_j = dataset[dataset[\"dialect_sum\"] == j]\n",
    "            \n",
    "            if len(rows_with_sum_j) > n:\n",
    "                resampled_rows = resample(rows_with_sum_j, replace=True, \n",
    "                                          n_samples=n, \n",
    "                                          random_state=42)\n",
    "            else:\n",
    "                resampled_rows = rows_with_sum_j\n",
    "        \n",
    "\n",
    "        current_rows = pd.concat([current_rows, resampled_rows], ignore_index=True)\n",
    "        print(f\"Dialect_sum counts after combining with sum {j}:\")\n",
    "        print(current_rows['dialect_sum'].value_counts())\n",
    "    current_rows.to_csv(f\"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_{i}.csv\")\n",
    "        \n",
    "current_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 35%|███▍      | 194/557 [00:29<00:20, 17.54it/s]\n",
      " 35%|███▍      | 194/557 [00:53<00:20, 17.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2457, 'grad_norm': 0.4768322706222534, 'learning_rate': 5e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                  \n",
      "\n",
      " 35%|███▍      | 194/557 [00:58<00:20, 17.54it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11737997084856033, 'eval_f1': 0.6376920194829524, 'eval_roc_auc': 0.7802744833604379, 'eval_accuracy': 0.5677680377612947, 'eval_runtime': 1.3101, 'eval_samples_per_second': 1131.972, 'eval_steps_per_second': 47.324, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███▍      | 194/557 [01:28<00:20, 17.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1094, 'grad_norm': 0.5849602222442627, 'learning_rate': 9.283387622149838e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                   \n",
      "\n",
      " 35%|███▍      | 194/557 [01:40<00:20, 17.54it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10440827161073685, 'eval_f1': 0.6676166726042038, 'eval_roc_auc': 0.8082384673356867, 'eval_accuracy': 0.6210384356035064, 'eval_runtime': 1.9164, 'eval_samples_per_second': 773.838, 'eval_steps_per_second': 32.352, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1114/1114 [01:17<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 77.725, 'train_samples_per_second': 343.416, 'train_steps_per_second': 14.333, 'train_loss': 0.16986481462817646, 'epoch': 2.0}\n",
      "Subset Accuracy: 0.1750\n",
      "Hamming Loss: 0.3458\n",
      "Micro Precision: 0.7222\n",
      "Micro Recall: 0.1096\n",
      "Micro F1-Score: 0.1902\n",
      "Precision per label: [1.         0.66666667 0.         1.         1.         1.\n",
      " 1.         0.5       ]\n",
      "Recall per label: [0.05714286 0.71794872 0.         0.046875   0.04761905 0.02173913\n",
      " 0.0952381  0.01694915]\n",
      "F1-Score per label: [0.10810811 0.69135802 0.         0.08955224 0.09090909 0.04255319\n",
      " 0.17391304 0.03278689]\n",
      "{0, 1, 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/3611314930.py:76: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "paths = [\"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_1.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_2.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_3.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_4.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_5.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_6.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_7.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_8.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_9.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_10.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_11.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_12.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_13.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_14.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_15.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_18.csv\"]\n",
    "\n",
    "stage = 0\n",
    "dataset_path = paths[stage]\n",
    "dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=f\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=28\n",
    ")\n",
    "\n",
    "trainer.save_dir = f'./exp_{trainer.exp_num}'\n",
    "\n",
    "trainer.train(\n",
    "    num_train_epochs=2,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 69.66 %\n",
      "MACRO AVERAGE RECALL SCORE: 65.52 %\n",
      "MACRO AVERAGE F1-SCORE: 65.49 %\n",
      "MACRO AVERAGE ACCURACY: 76.04 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"Project/Cross-Country-Dialectal-Arabic-Identification/exp_27/stage_16/-home-ali.mekky-Documents-NLP-Project-Cross-Country-Dialectal-Arabic-Identification-exp_27-stage_15-experiment-27_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 35%|███▍      | 194/557 [06:46<12:39,  2.09s/it]\n",
      " 50%|████▉     | 240/482 [00:13<00:13, 17.63it/s]\n",
      " 50%|█████     | 241/482 [00:14<00:13, 17.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18958240747451782, 'eval_f1': 0.5371062466631074, 'eval_roc_auc': 0.7369372576095561, 'eval_accuracy': 0.26127527216174184, 'eval_runtime': 0.516, 'eval_samples_per_second': 1246.076, 'eval_steps_per_second': 52.324, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482/482 [00:29<00:00, 17.90it/s]\n",
      "100%|██████████| 482/482 [00:31<00:00, 17.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18514694273471832, 'eval_f1': 0.5550935550935551, 'eval_roc_auc': 0.7517128733128935, 'eval_accuracy': 0.2581648522550544, 'eval_runtime': 0.4763, 'eval_samples_per_second': 1350.01, 'eval_steps_per_second': 56.688, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482/482 [00:33<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 33.4902, 'train_samples_per_second': 345.236, 'train_steps_per_second': 14.392, 'train_loss': 0.2068141129996272, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1833\n",
      "Hamming Loss: 0.3417\n",
      "Micro Precision: 0.6667\n",
      "Micro Recall: 0.1573\n",
      "Micro F1-Score: 0.2545\n",
      "Precision per label: [1.         0.67567568 0.375      0.85714286 1.         0.66666667\n",
      " 0.5        1.        ]\n",
      "Recall per label: [0.11428571 0.64102564 0.06       0.09375    0.07142857 0.17391304\n",
      " 0.28571429 0.01694915]\n",
      "F1-Score per label: [0.20512821 0.65789474 0.10344828 0.16901408 0.13333333 0.27586207\n",
      " 0.36363636 0.03333333]\n",
      "{0, 1, 2, 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|█████     | 262/524 [00:15<00:14, 18.00it/s]\n",
      " 50%|█████     | 262/524 [00:15<00:14, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21228648722171783, 'eval_f1': 0.6, 'eval_roc_auc': 0.7794143982808023, 'eval_accuracy': 0.22636103151862463, 'eval_runtime': 0.5242, 'eval_samples_per_second': 1331.557, 'eval_steps_per_second': 57.23, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 502/524 [00:30<00:01, 17.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2271, 'grad_norm': 1.8141015768051147, 'learning_rate': 5e-05, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [00:32<00:00, 17.77it/s]\n",
      "100%|██████████| 524/524 [00:34<00:00, 17.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21048694849014282, 'eval_f1': 0.6055575604474919, 'eval_roc_auc': 0.7765042979942693, 'eval_accuracy': 0.24785100286532952, 'eval_runtime': 0.5232, 'eval_samples_per_second': 1333.979, 'eval_steps_per_second': 57.334, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [00:36<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 36.511, 'train_samples_per_second': 344.116, 'train_steps_per_second': 14.352, 'train_loss': 0.22644754766507913, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.2083\n",
      "Hamming Loss: 0.3417\n",
      "Micro Precision: 0.6296\n",
      "Micro Recall: 0.1910\n",
      "Micro F1-Score: 0.2931\n",
      "Precision per label: [0.69230769 0.69444444 0.54545455 0.58333333 1.         0.76923077\n",
      " 0.42105263 0.5       ]\n",
      "Recall per label: [0.25714286 0.64102564 0.12       0.109375   0.04761905 0.2173913\n",
      " 0.38095238 0.01694915]\n",
      "F1-Score per label: [0.375      0.66666667 0.19672131 0.18421053 0.09090909 0.33898305\n",
      " 0.4        0.03278689]\n",
      "{0, 1, 2, 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|█████     | 378/756 [00:21<00:21, 17.56it/s]\n",
      " 50%|█████     | 378/756 [00:22<00:21, 17.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2331126630306244, 'eval_f1': 0.6429502700689141, 'eval_roc_auc': 0.8062684622438664, 'eval_accuracy': 0.2542204568023833, 'eval_runtime': 0.856, 'eval_samples_per_second': 1176.456, 'eval_steps_per_second': 49.068, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 502/756 [00:31<00:14, 17.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2494, 'grad_norm': 1.6846346855163574, 'learning_rate': 5e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 754/756 [00:45<00:00, 17.27it/s]\n",
      "100%|██████████| 756/756 [00:48<00:00, 17.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22674810886383057, 'eval_f1': 0.6492005393951069, 'eval_roc_auc': 0.8025298928008251, 'eval_accuracy': 0.2740814299900695, 'eval_runtime': 0.8599, 'eval_samples_per_second': 1171.079, 'eval_steps_per_second': 48.843, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 756/756 [00:50<00:00, 15.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 50.1171, 'train_samples_per_second': 361.394, 'train_steps_per_second': 15.085, 'train_loss': 0.24537800985669334, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1833\n",
      "Hamming Loss: 0.3344\n",
      "Micro Precision: 0.6241\n",
      "Micro Recall: 0.2472\n",
      "Micro F1-Score: 0.3541\n",
      "Precision per label: [0.69230769 0.75       0.63333333 0.60869565 0.5        0.61904762\n",
      " 0.44444444 0.5       ]\n",
      "Recall per label: [0.25714286 0.53846154 0.38       0.21875    0.07142857 0.2826087\n",
      " 0.38095238 0.01694915]\n",
      "F1-Score per label: [0.375      0.62686567 0.475      0.32183908 0.125      0.3880597\n",
      " 0.41025641 0.03278689]\n",
      "{0, 1, 2, 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|█████     | 266/532 [00:15<00:21, 12.31it/s]\n",
      " 50%|█████     | 266/532 [00:16<00:21, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26594865322113037, 'eval_f1': 0.6412047140986469, 'eval_roc_auc': 0.7983312475958286, 'eval_accuracy': 0.2333804809052334, 'eval_runtime': 0.5104, 'eval_samples_per_second': 1385.218, 'eval_steps_per_second': 58.779, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 502/532 [00:31<00:01, 17.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2772, 'grad_norm': 1.606418490409851, 'learning_rate': 4.99e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [00:33<00:00, 17.61it/s]\n",
      "100%|██████████| 532/532 [00:35<00:00, 17.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25957974791526794, 'eval_f1': 0.6564818856394588, 'eval_roc_auc': 0.8081950770984693, 'eval_accuracy': 0.25035360678925034, 'eval_runtime': 0.5494, 'eval_samples_per_second': 1286.955, 'eval_steps_per_second': 54.609, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532/532 [00:37<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 37.5134, 'train_samples_per_second': 339.132, 'train_steps_per_second': 14.182, 'train_loss': 0.2769548731638973, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1667\n",
      "Hamming Loss: 0.3323\n",
      "Micro Precision: 0.6194\n",
      "Micro Recall: 0.2697\n",
      "Micro F1-Score: 0.3757\n",
      "Precision per label: [0.69230769 0.72413793 0.63333333 0.61290323 0.42857143 0.59259259\n",
      " 0.5        0.5       ]\n",
      "Recall per label: [0.25714286 0.53846154 0.38       0.296875   0.07142857 0.34782609\n",
      " 0.38095238 0.01694915]\n",
      "F1-Score per label: [0.375      0.61764706 0.475      0.4        0.12244898 0.43835616\n",
      " 0.43243243 0.03278689]\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|████▉     | 372/746 [00:21<00:21, 17.44it/s]\n",
      " 50%|█████     | 373/746 [00:22<00:21, 17.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2781010568141937, 'eval_f1': 0.6735605770458317, 'eval_roc_auc': 0.8216588748510345, 'eval_accuracy': 0.2321608040201005, 'eval_runtime': 0.7083, 'eval_samples_per_second': 1404.741, 'eval_steps_per_second': 59.296, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 502/746 [00:31<00:14, 16.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2983, 'grad_norm': 2.0789074897766113, 'learning_rate': 5e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 746/746 [00:45<00:00, 17.86it/s]\n",
      "100%|██████████| 746/746 [00:47<00:00, 17.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2733314335346222, 'eval_f1': 0.6906627706202683, 'eval_roc_auc': 0.8253583383348998, 'eval_accuracy': 0.2562814070351759, 'eval_runtime': 0.7378, 'eval_samples_per_second': 1348.609, 'eval_steps_per_second': 56.926, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 746/746 [00:49<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 49.6325, 'train_samples_per_second': 360.61, 'train_steps_per_second': 15.03, 'train_loss': 0.2961420463173383, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.2000\n",
      "Hamming Loss: 0.3229\n",
      "Micro Precision: 0.6575\n",
      "Micro Recall: 0.2697\n",
      "Micro F1-Score: 0.3825\n",
      "Precision per label: [0.75       0.76923077 0.625      0.64285714 1.         0.60714286\n",
      " 0.53333333 0.66666667]\n",
      "Recall per label: [0.25714286 0.51282051 0.4        0.28125    0.04761905 0.36956522\n",
      " 0.38095238 0.03389831]\n",
      "F1-Score per label: [0.38297872 0.61538462 0.48780488 0.39130435 0.09090909 0.45945946\n",
      " 0.44444444 0.06451613]\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|█████     | 454/908 [00:26<00:26, 17.38it/s]\n",
      " 50%|█████     | 454/908 [00:27<00:26, 17.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2925367057323456, 'eval_f1': 0.6988783433994823, 'eval_roc_auc': 0.8225911068495941, 'eval_accuracy': 0.23573200992555832, 'eval_runtime': 1.0704, 'eval_samples_per_second': 1129.498, 'eval_steps_per_second': 47.646, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 502/908 [00:31<00:23, 17.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3084, 'grad_norm': 2.286717653274536, 'learning_rate': 5e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 906/908 [00:54<00:00, 17.55it/s]\n",
      "100%|██████████| 908/908 [00:57<00:00, 17.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2935512363910675, 'eval_f1': 0.6929075380616413, 'eval_roc_auc': 0.8242294465021346, 'eval_accuracy': 0.23573200992555832, 'eval_runtime': 1.0379, 'eval_samples_per_second': 1164.796, 'eval_steps_per_second': 49.135, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 908/908 [00:59<00:00, 15.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 59.9167, 'train_samples_per_second': 362.971, 'train_steps_per_second': 15.154, 'train_loss': 0.30393127407796583, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.3208\n",
      "Micro Precision: 0.6412\n",
      "Micro Recall: 0.3062\n",
      "Micro F1-Score: 0.4144\n",
      "Precision per label: [0.75       0.76923077 0.59459459 0.61764706 1.         0.59259259\n",
      " 0.5        0.6875    ]\n",
      "Recall per label: [0.25714286 0.51282051 0.44       0.328125   0.04761905 0.34782609\n",
      " 0.38095238 0.18644068]\n",
      "F1-Score per label: [0.38297872 0.61538462 0.50574713 0.42857143 0.09090909 0.43835616\n",
      " 0.43243243 0.29333333]\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|█████     | 332/664 [00:19<00:18, 17.80it/s]\n",
      " 50%|█████     | 332/664 [00:19<00:18, 17.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2912144064903259, 'eval_f1': 0.7484756097560976, 'eval_roc_auc': 0.8564448826690121, 'eval_accuracy': 0.23589164785553046, 'eval_runtime': 0.6202, 'eval_samples_per_second': 1428.548, 'eval_steps_per_second': 59.657, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 502/664 [00:31<00:09, 17.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.318, 'grad_norm': 4.657841682434082, 'learning_rate': 5e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 664/664 [00:40<00:00, 17.71it/s]\n",
      "100%|██████████| 664/664 [00:42<00:00, 17.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2789565324783325, 'eval_f1': 0.7514425210830005, 'eval_roc_auc': 0.8551355088322588, 'eval_accuracy': 0.21218961625282168, 'eval_runtime': 0.6339, 'eval_samples_per_second': 1397.783, 'eval_steps_per_second': 58.372, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 664/664 [00:44<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 44.6135, 'train_samples_per_second': 357.201, 'train_steps_per_second': 14.883, 'train_loss': 0.3175594892846533, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1750\n",
      "Hamming Loss: 0.3094\n",
      "Micro Precision: 0.6612\n",
      "Micro Recall: 0.3399\n",
      "Micro F1-Score: 0.4490\n",
      "Precision per label: [0.72727273 0.6875     0.63636364 0.63333333 0.66666667 0.56\n",
      " 0.57142857 0.77142857]\n",
      "Recall per label: [0.22857143 0.56410256 0.42       0.296875   0.04761905 0.30434783\n",
      " 0.38095238 0.45762712]\n",
      "F1-Score per label: [0.34782609 0.61971831 0.5060241  0.40425532 0.08888889 0.3943662\n",
      " 0.45714286 0.57446809]\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|████▉     | 180/362 [00:10<00:09, 18.37it/s]\n",
      " 50%|█████     | 181/362 [00:10<00:09, 18.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.285860538482666, 'eval_f1': 0.7718652181918615, 'eval_roc_auc': 0.8563706550763251, 'eval_accuracy': 0.2203742203742204, 'eval_runtime': 0.4029, 'eval_samples_per_second': 1193.766, 'eval_steps_per_second': 52.119, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 360/362 [00:22<00:00, 18.08it/s]\n",
      "100%|██████████| 362/362 [00:24<00:00, 18.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2893000543117523, 'eval_f1': 0.7708874860750092, 'eval_roc_auc': 0.8543045430406977, 'eval_accuracy': 0.2203742203742204, 'eval_runtime': 0.4068, 'eval_samples_per_second': 1182.381, 'eval_steps_per_second': 51.622, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:26<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 26.6584, 'train_samples_per_second': 324.776, 'train_steps_per_second': 13.579, 'train_loss': 0.3181882194392589, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1667\n",
      "Hamming Loss: 0.2948\n",
      "Micro Precision: 0.6698\n",
      "Micro Recall: 0.4045\n",
      "Micro F1-Score: 0.5044\n",
      "Precision per label: [0.75       0.66666667 0.63414634 0.65789474 0.66666667 0.60606061\n",
      " 0.57142857 0.78947368]\n",
      "Recall per label: [0.25714286 0.56410256 0.52       0.390625   0.0952381  0.43478261\n",
      " 0.38095238 0.50847458]\n",
      "F1-Score per label: [0.38297872 0.61111111 0.57142857 0.49019608 0.16666667 0.50632911\n",
      " 0.45714286 0.6185567 ]\n",
      "{0, 1, 2, 3, 4, 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 49%|████▉     | 62/126 [00:03<00:03, 18.20it/s]\n",
      " 50%|█████     | 63/126 [00:03<00:03, 18.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33933815360069275, 'eval_f1': 0.7540029112081513, 'eval_roc_auc': 0.8293483325770633, 'eval_accuracy': 0.16071428571428573, 'eval_runtime': 0.122, 'eval_samples_per_second': 1376.506, 'eval_steps_per_second': 57.354, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 124/126 [00:08<00:00, 17.96it/s]\n",
      "100%|██████████| 126/126 [00:10<00:00, 17.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3299466073513031, 'eval_f1': 0.7504725897920604, 'eval_roc_auc': 0.8291901263004963, 'eval_accuracy': 0.16071428571428573, 'eval_runtime': 0.1271, 'eval_samples_per_second': 1321.834, 'eval_steps_per_second': 55.076, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:12<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 12.975, 'train_samples_per_second': 232.138, 'train_steps_per_second': 9.711, 'train_loss': 0.3426731957329644, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.2000\n",
      "Hamming Loss: 0.2865\n",
      "Micro Precision: 0.6784\n",
      "Micro Recall: 0.4326\n",
      "Micro F1-Score: 0.5283\n",
      "Precision per label: [0.75       0.66666667 0.64444444 0.65853659 0.6        0.63636364\n",
      " 0.54545455 0.78723404]\n",
      "Recall per label: [0.25714286 0.56410256 0.58       0.421875   0.07142857 0.45652174\n",
      " 0.28571429 0.62711864]\n",
      "F1-Score per label: [0.38297872 0.61111111 0.61052632 0.51428571 0.12765957 0.53164557\n",
      " 0.375      0.69811321]\n",
      "{0, 1, 2, 3, 4, 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 55/114 [00:02<00:02, 19.97it/s]\n",
      " 50%|█████     | 57/114 [00:03<00:02, 19.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3666934669017792, 'eval_f1': 0.7551741544674407, 'eval_roc_auc': 0.8257222609735904, 'eval_accuracy': 0.12666666666666668, 'eval_runtime': 0.112, 'eval_samples_per_second': 1338.75, 'eval_steps_per_second': 62.475, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 112/114 [00:07<00:00, 19.91it/s]\n",
      "100%|██████████| 114/114 [00:09<00:00, 19.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35248255729675293, 'eval_f1': 0.7636544190665343, 'eval_roc_auc': 0.8342201979032026, 'eval_accuracy': 0.12, 'eval_runtime': 0.1201, 'eval_samples_per_second': 1248.53, 'eval_steps_per_second': 58.265, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:11<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.4344, 'train_samples_per_second': 235.43, 'train_steps_per_second': 9.97, 'train_loss': 0.37474806266918514, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1583\n",
      "Hamming Loss: 0.2781\n",
      "Micro Precision: 0.6642\n",
      "Micro Recall: 0.5056\n",
      "Micro F1-Score: 0.5742\n",
      "Precision per label: [0.75       0.66666667 0.66666667 0.66       0.54545455 0.59574468\n",
      " 0.58333333 0.75      ]\n",
      "Recall per label: [0.25714286 0.56410256 0.72       0.515625   0.14285714 0.60869565\n",
      " 0.33333333 0.66101695]\n",
      "F1-Score per label: [0.38297872 0.61111111 0.69230769 0.57894737 0.22641509 0.60215054\n",
      " 0.42424242 0.7027027 ]\n",
      "{0, 1, 2, 3, 4, 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|█████     | 137/274 [00:06<00:06, 20.35it/s]\n",
      " 50%|█████     | 137/274 [00:07<00:06, 20.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3434987962245941, 'eval_f1': 0.7798492661642206, 'eval_roc_auc': 0.8400918248339599, 'eval_accuracy': 0.1813186813186813, 'eval_runtime': 0.3078, 'eval_samples_per_second': 1182.777, 'eval_steps_per_second': 51.99, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 273/274 [00:15<00:00, 20.18it/s]\n",
      "100%|██████████| 274/274 [00:17<00:00, 20.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3450153172016144, 'eval_f1': 0.7812069982307843, 'eval_roc_auc': 0.8419473605826491, 'eval_accuracy': 0.20604395604395603, 'eval_runtime': 0.3418, 'eval_samples_per_second': 1065.089, 'eval_steps_per_second': 46.817, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:19<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 19.9576, 'train_samples_per_second': 327.795, 'train_steps_per_second': 13.729, 'train_loss': 0.36212394881422505, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1750\n",
      "Hamming Loss: 0.2792\n",
      "Micro Precision: 0.6549\n",
      "Micro Recall: 0.5225\n",
      "Micro F1-Score: 0.5813\n",
      "Precision per label: [0.75       0.64705882 0.61016949 0.67307692 0.55555556 0.64583333\n",
      " 0.5        0.72413793]\n",
      "Recall per label: [0.25714286 0.56410256 0.72       0.546875   0.11904762 0.67391304\n",
      " 0.28571429 0.71186441]\n",
      "F1-Score per label: [0.38297872 0.60273973 0.66055046 0.60344828 0.19607843 0.65957447\n",
      " 0.36363636 0.71794872]\n",
      "{0, 1, 2, 3, 4, 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 49%|████▉     | 122/248 [00:06<00:06, 19.85it/s]\n",
      " 50%|█████     | 124/248 [00:06<00:06, 19.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3421633541584015, 'eval_f1': 0.7988980716253443, 'eval_roc_auc': 0.8406691869086655, 'eval_accuracy': 0.19637462235649547, 'eval_runtime': 0.2722, 'eval_samples_per_second': 1216.101, 'eval_steps_per_second': 51.436, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:14<00:00, 20.28it/s]\n",
      "100%|██████████| 248/248 [00:16<00:00, 20.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34273961186408997, 'eval_f1': 0.8054372083586935, 'eval_roc_auc': 0.8454410373769592, 'eval_accuracy': 0.2054380664652568, 'eval_runtime': 0.2708, 'eval_samples_per_second': 1222.484, 'eval_steps_per_second': 51.706, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:18<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 18.9458, 'train_samples_per_second': 313.948, 'train_steps_per_second': 13.09, 'train_loss': 0.36534558573076803, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.2677\n",
      "Micro Precision: 0.6667\n",
      "Micro Recall: 0.5562\n",
      "Micro F1-Score: 0.6064\n",
      "Precision per label: [0.75       0.67567568 0.62295082 0.68518519 0.83333333 0.61111111\n",
      " 0.5        0.73770492]\n",
      "Recall per label: [0.25714286 0.64102564 0.76       0.578125   0.11904762 0.7173913\n",
      " 0.28571429 0.76271186]\n",
      "F1-Score per label: [0.38297872 0.65789474 0.68468468 0.62711864 0.20833333 0.66\n",
      " 0.36363636 0.75      ]\n",
      "{0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|████▉     | 126/254 [00:07<00:07, 18.18it/s]\n",
      " 50%|█████     | 127/254 [00:07<00:06, 18.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33735010027885437, 'eval_f1': 0.8137535816618912, 'eval_roc_auc': 0.8408249593945777, 'eval_accuracy': 0.15680473372781065, 'eval_runtime': 0.2361, 'eval_samples_per_second': 1431.742, 'eval_steps_per_second': 63.539, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 252/254 [00:15<00:00, 18.34it/s]\n",
      "100%|██████████| 254/254 [00:17<00:00, 18.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3358014225959778, 'eval_f1': 0.8177009873060649, 'eval_roc_auc': 0.8444836864368964, 'eval_accuracy': 0.16272189349112426, 'eval_runtime': 0.2332, 'eval_samples_per_second': 1449.168, 'eval_steps_per_second': 64.312, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:19<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 19.7556, 'train_samples_per_second': 307.254, 'train_steps_per_second': 12.857, 'train_loss': 0.36480205265555793, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1750\n",
      "Hamming Loss: 0.2562\n",
      "Micro Precision: 0.6608\n",
      "Micro Recall: 0.6348\n",
      "Micro F1-Score: 0.6476\n",
      "Precision per label: [0.8        0.66666667 0.58108108 0.67241379 0.73333333 0.625\n",
      " 0.54545455 0.74285714]\n",
      "Recall per label: [0.22857143 0.82051282 0.86       0.609375   0.26190476 0.76086957\n",
      " 0.28571429 0.88135593]\n",
      "F1-Score per label: [0.35555556 0.73563218 0.69354839 0.63934426 0.38596491 0.68627451\n",
      " 0.375      0.80620155]\n",
      "{0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|████▉     | 242/486 [00:13<00:13, 18.04it/s]\n",
      " 50%|█████     | 243/486 [00:14<00:13, 18.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3355901837348938, 'eval_f1': 0.8131308182263596, 'eval_roc_auc': 0.8440612603038908, 'eval_accuracy': 0.19290123456790123, 'eval_runtime': 0.4592, 'eval_samples_per_second': 1411.05, 'eval_steps_per_second': 58.794, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 486/486 [00:29<00:00, 18.53it/s]\n",
      "100%|██████████| 486/486 [00:31<00:00, 18.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33759135007858276, 'eval_f1': 0.8091344798421201, 'eval_roc_auc': 0.8405600342331241, 'eval_accuracy': 0.20679012345679013, 'eval_runtime': 0.4846, 'eval_samples_per_second': 1337.221, 'eval_steps_per_second': 55.718, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 486/486 [00:34<00:00, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 34.2428, 'train_samples_per_second': 340.51, 'train_steps_per_second': 14.193, 'train_loss': 0.3661141729158629, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.2573\n",
      "Micro Precision: 0.6687\n",
      "Micro Recall: 0.6067\n",
      "Micro F1-Score: 0.6362\n",
      "Precision per label: [0.8        0.66666667 0.61904762 0.67272727 0.83333333 0.6\n",
      " 0.5        0.73846154]\n",
      "Recall per label: [0.22857143 0.76923077 0.78       0.578125   0.35714286 0.7173913\n",
      " 0.28571429 0.81355932]\n",
      "F1-Score per label: [0.35555556 0.71428571 0.69026549 0.62184874 0.5        0.65346535\n",
      " 0.36363636 0.77419355]\n",
      "{0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|████▉     | 250/504 [00:13<00:14, 18.02it/s]\n",
      " 50%|█████     | 252/504 [00:14<00:13, 18.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36818137764930725, 'eval_f1': 0.8212688421401358, 'eval_roc_auc': 0.8283280402276034, 'eval_accuracy': 0.20535714285714285, 'eval_runtime': 0.4834, 'eval_samples_per_second': 1390.192, 'eval_steps_per_second': 57.925, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 503/504 [00:30<00:00, 17.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3826, 'grad_norm': 3.6993253231048584, 'learning_rate': 4.99e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 504/504 [00:32<00:00, 17.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38453784584999084, 'eval_f1': 0.8079353651707863, 'eval_roc_auc': 0.8113189757840414, 'eval_accuracy': 0.19345238095238096, 'eval_runtime': 0.519, 'eval_samples_per_second': 1294.735, 'eval_steps_per_second': 53.947, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [00:34<00:00, 14.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 34.6518, 'train_samples_per_second': 348.553, 'train_steps_per_second': 14.545, 'train_loss': 0.38235192308350213, 'epoch': 2.0}\n",
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.2427\n",
      "Micro Precision: 0.6762\n",
      "Micro Recall: 0.6629\n",
      "Micro F1-Score: 0.6695\n",
      "Precision per label: [0.8        0.69565217 0.60869565 0.69642857 0.73076923 0.62962963\n",
      " 0.6        0.72058824]\n",
      "Recall per label: [0.34285714 0.82051282 0.84       0.609375   0.45238095 0.73913043\n",
      " 0.42857143 0.83050847]\n",
      "F1-Score per label: [0.48       0.75294118 0.70588235 0.65       0.55882353 0.68\n",
      " 0.5        0.77165354]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 16):\n",
    "    paths = [\"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_1.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_2.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_3.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_4.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_5.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_6.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_7.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_8.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_9.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_10.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_11.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_12.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_13.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_14.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_15.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_18.csv\"]\n",
    "\n",
    "    stage = i\n",
    "    dataset_path = paths[stage]\n",
    "    dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "    labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "        'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "        'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "    trainer = BertTrainer(\n",
    "        training_dataset_path=dataset_path,\n",
    "        # model_name=f\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "        model_name=f\"Project/Cross-Country-Dialectal-Arabic-Identification/exp_28/stage_{stage}\",\n",
    "        labels=labels,\n",
    "        threshold=0.3,\n",
    "        exp_num=28,\n",
    "        stage = i + 1\n",
    "    )\n",
    "\n",
    "    trainer.save_dir = f'./exp_{trainer.exp_num}'\n",
    "\n",
    "    trainer.train(\n",
    "        num_train_epochs=2,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        greater_is_better=True,\n",
    "        per_device_train_batch_size=24,\n",
    "        per_device_eval_batch_size=24,\n",
    "    )\n",
    "    trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 68.52 %\n",
      "MACRO AVERAGE RECALL SCORE: 63.29 %\n",
      "MACRO AVERAGE F1-SCORE: 63.74 %\n",
      "MACRO AVERAGE ACCURACY: 75.73 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"Project/Cross-Country-Dialectal-Arabic-Identification/exp_28/stage_16/-home-ali.mekky-Documents-NLP-Project-Cross-Country-Dialectal-Arabic-Identification-exp_28-stage_15-experiment-28_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"UBC-NLP/MARBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 502/557 [00:34<00:03, 14.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2545, 'grad_norm': 1.5156006813049316, 'learning_rate': 5e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 556/557 [00:38<00:00, 14.64it/s]\n",
      "100%|██████████| 557/557 [00:42<00:00, 14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1142585352063179, 'eval_f1': 0.6541635408852213, 'eval_roc_auc': 0.7878307088175797, 'eval_accuracy': 0.5812542144302091, 'eval_runtime': 1.2794, 'eval_samples_per_second': 1159.181, 'eval_steps_per_second': 48.462, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:46<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 46.0798, 'train_samples_per_second': 289.628, 'train_steps_per_second': 12.088, 'train_loss': 0.24093889205631278, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1833\n",
      "Hamming Loss: 0.3458\n",
      "Micro Precision: 0.8333\n",
      "Micro Recall: 0.0843\n",
      "Micro F1-Score: 0.1531\n",
      "Precision per label: [0.         0.80645161 0.         0.         1.         0.\n",
      " 1.         1.        ]\n",
      "Recall per label: [0.         0.64102564 0.         0.         0.04761905 0.\n",
      " 0.04761905 0.03389831]\n",
      "F1-Score per label: [0.         0.71428571 0.         0.         0.09090909 0.\n",
      " 0.09090909 0.06557377]\n",
      "{0, 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|█████████▉| 240/241 [00:16<00:00, 14.43it/s]\n",
      "100%|██████████| 241/241 [00:20<00:00, 14.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19477435946464539, 'eval_f1': 0.5333333333333333, 'eval_roc_auc': 0.7351798430769726, 'eval_accuracy': 0.2161741835147745, 'eval_runtime': 0.5175, 'eval_samples_per_second': 1242.576, 'eval_steps_per_second': 52.177, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [00:23<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 23.3735, 'train_samples_per_second': 247.332, 'train_steps_per_second': 10.311, 'train_loss': 0.22623505335131128, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.2250\n",
      "Hamming Loss: 0.3354\n",
      "Micro Precision: 0.7361\n",
      "Micro Recall: 0.1489\n",
      "Micro F1-Score: 0.2477\n",
      "Precision per label: [0.75       0.88461538 0.         1.         1.         0.76923077\n",
      " 0.41176471 1.        ]\n",
      "Recall per label: [0.17142857 0.58974359 0.         0.03125    0.07142857 0.2173913\n",
      " 0.33333333 0.03389831]\n",
      "F1-Score per label: [0.27906977 0.70769231 0.         0.06060606 0.13333333 0.33898305\n",
      " 0.36842105 0.06557377]\n",
      "{0, 1, 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 262/262 [00:18<00:00, 14.79it/s]\n",
      "100%|██████████| 262/262 [00:21<00:00, 14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.218875452876091, 'eval_f1': 0.6062767475035663, 'eval_roc_auc': 0.7794591690544411, 'eval_accuracy': 0.22349570200573066, 'eval_runtime': 0.5828, 'eval_samples_per_second': 1197.769, 'eval_steps_per_second': 51.48, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262/262 [00:25<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 25.5901, 'train_samples_per_second': 245.486, 'train_steps_per_second': 10.238, 'train_loss': 0.2426065634225161, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.2000\n",
      "Hamming Loss: 0.3344\n",
      "Micro Precision: 0.6606\n",
      "Micro Recall: 0.2022\n",
      "Micro F1-Score: 0.3097\n",
      "Precision per label: [0.66666667 0.8        0.64285714 0.58333333 1.         0.76923077\n",
      " 0.35       1.        ]\n",
      "Recall per label: [0.28571429 0.61538462 0.18       0.109375   0.07142857 0.2173913\n",
      " 0.33333333 0.03389831]\n",
      "F1-Score per label: [0.4        0.69565217 0.28125    0.18421053 0.13333333 0.33898305\n",
      " 0.34146341 0.06557377]\n",
      "{0, 1, 2, 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 378/378 [00:26<00:00, 14.68it/s]\n",
      "100%|██████████| 378/378 [00:30<00:00, 14.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2365991771221161, 'eval_f1': 0.6438407145515445, 'eval_roc_auc': 0.8070294405895639, 'eval_accuracy': 0.2711022840119166, 'eval_runtime': 0.8792, 'eval_samples_per_second': 1145.3, 'eval_steps_per_second': 47.768, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:34<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 34.2707, 'train_samples_per_second': 264.249, 'train_steps_per_second': 11.03, 'train_loss': 0.25694686647445436, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.2000\n",
      "Hamming Loss: 0.3198\n",
      "Micro Precision: 0.6667\n",
      "Micro Recall: 0.2753\n",
      "Micro F1-Score: 0.3897\n",
      "Precision per label: [0.66666667 0.73333333 0.65517241 0.69230769 1.         0.77272727\n",
      " 0.35       1.        ]\n",
      "Recall per label: [0.28571429 0.56410256 0.38       0.28125    0.07142857 0.36956522\n",
      " 0.33333333 0.03389831]\n",
      "F1-Score per label: [0.4        0.63768116 0.48101266 0.4        0.13333333 0.5\n",
      " 0.34146341 0.06557377]\n",
      "{0, 1, 2, 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 266/266 [00:18<00:00, 10.92it/s]\n",
      "100%|██████████| 266/266 [00:22<00:00, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2681634724140167, 'eval_f1': 0.6599956493365238, 'eval_roc_auc': 0.8111509041234709, 'eval_accuracy': 0.2362093352192362, 'eval_runtime': 0.5859, 'eval_samples_per_second': 1206.607, 'eval_steps_per_second': 51.2, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:26<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 26.3246, 'train_samples_per_second': 241.637, 'train_steps_per_second': 10.105, 'train_loss': 0.27886323283489484, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.3146\n",
      "Micro Precision: 0.6849\n",
      "Micro Recall: 0.2809\n",
      "Micro F1-Score: 0.3984\n",
      "Precision per label: [0.64285714 0.81481481 0.66666667 0.67857143 1.         0.7826087\n",
      " 0.36842105 1.        ]\n",
      "Recall per label: [0.25714286 0.56410256 0.4        0.296875   0.07142857 0.39130435\n",
      " 0.33333333 0.03389831]\n",
      "F1-Score per label: [0.36734694 0.66666667 0.5        0.41304348 0.13333333 0.52173913\n",
      " 0.35       0.06557377]\n",
      "{0, 1, 2, 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|█████████▉| 372/373 [00:25<00:00, 14.31it/s]\n",
      "100%|██████████| 373/373 [00:29<00:00, 14.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28213486075401306, 'eval_f1': 0.6762199845081333, 'eval_roc_auc': 0.8212604051541182, 'eval_accuracy': 0.24120603015075376, 'eval_runtime': 0.737, 'eval_samples_per_second': 1350.154, 'eval_steps_per_second': 56.991, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [00:33<00:00, 14.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 33.847, 'train_samples_per_second': 264.396, 'train_steps_per_second': 11.02, 'train_loss': 0.30239585896918986, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [00:36<00:00, 10.14it/s]\n",
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.2167\n",
      "Hamming Loss: 0.3167\n",
      "Micro Precision: 0.6857\n",
      "Micro Recall: 0.2697\n",
      "Micro F1-Score: 0.3871\n",
      "Precision per label: [0.72727273 0.77777778 0.66666667 0.65517241 1.         0.72\n",
      " 0.42857143 1.        ]\n",
      "Recall per label: [0.22857143 0.53846154 0.4        0.296875   0.04761905 0.39130435\n",
      " 0.28571429 0.03389831]\n",
      "F1-Score per label: [0.34782609 0.63636364 0.5        0.40860215 0.09090909 0.50704225\n",
      " 0.34285714 0.06557377]\n",
      "{0, 1, 2, 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 454/454 [00:31<00:00, 14.91it/s]\n",
      "100%|██████████| 454/454 [00:36<00:00, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2966492474079132, 'eval_f1': 0.7104731095835018, 'eval_roc_auc': 0.8208758512320188, 'eval_accuracy': 0.25144747725392885, 'eval_runtime': 1.1475, 'eval_samples_per_second': 1053.551, 'eval_steps_per_second': 44.443, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:40<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 40.8575, 'train_samples_per_second': 266.144, 'train_steps_per_second': 11.112, 'train_loss': 0.3073196747229488, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.3094\n",
      "Micro Precision: 0.6810\n",
      "Micro Recall: 0.3118\n",
      "Micro F1-Score: 0.4277\n",
      "Precision per label: [0.69230769 0.83333333 0.6875     0.65714286 0.5        0.75\n",
      " 0.38888889 0.76923077]\n",
      "Recall per label: [0.25714286 0.51282051 0.44       0.359375   0.04761905 0.39130435\n",
      " 0.33333333 0.16949153]\n",
      "F1-Score per label: [0.375      0.63492063 0.53658537 0.46464646 0.08695652 0.51428571\n",
      " 0.35897436 0.27777778]\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 332/332 [00:23<00:00, 14.62it/s]\n",
      "100%|██████████| 332/332 [00:27<00:00, 14.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2901941239833832, 'eval_f1': 0.7510058113544926, 'eval_roc_auc': 0.853476678541087, 'eval_accuracy': 0.23137697516930023, 'eval_runtime': 0.6468, 'eval_samples_per_second': 1369.798, 'eval_steps_per_second': 57.204, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:31<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 31.3376, 'train_samples_per_second': 254.264, 'train_steps_per_second': 10.594, 'train_loss': 0.3091603014842573, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.2938\n",
      "Micro Precision: 0.6968\n",
      "Micro Recall: 0.3680\n",
      "Micro F1-Score: 0.4816\n",
      "Precision per label: [0.72727273 0.78571429 0.73333333 0.68965517 0.66666667 0.74074074\n",
      " 0.375      0.70454545]\n",
      "Recall per label: [0.22857143 0.56410256 0.44       0.3125     0.04761905 0.43478261\n",
      " 0.28571429 0.52542373]\n",
      "F1-Score per label: [0.34782609 0.65671642 0.55       0.43010753 0.08888889 0.54794521\n",
      " 0.32432432 0.60194175]\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 99%|█████████▉| 180/181 [00:12<00:00, 14.76it/s]\n",
      "100%|██████████| 181/181 [00:16<00:00, 14.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3073582649230957, 'eval_f1': 0.7628097633687348, 'eval_roc_auc': 0.8475830482384656, 'eval_accuracy': 0.2203742203742204, 'eval_runtime': 0.4571, 'eval_samples_per_second': 1052.174, 'eval_steps_per_second': 45.937, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [00:20<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 20.1167, 'train_samples_per_second': 215.195, 'train_steps_per_second': 8.998, 'train_loss': 0.3182462829252633, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.2833\n",
      "Micro Precision: 0.7059\n",
      "Micro Recall: 0.4045\n",
      "Micro F1-Score: 0.5143\n",
      "Precision per label: [0.72727273 0.76666667 0.73684211 0.67567568 0.66666667 0.74193548\n",
      " 0.46153846 0.70731707]\n",
      "Recall per label: [0.22857143 0.58974359 0.56       0.390625   0.04761905 0.5\n",
      " 0.28571429 0.49152542]\n",
      "F1-Score per label: [0.34782609 0.66666667 0.63636364 0.4950495  0.08888889 0.5974026\n",
      " 0.35294118 0.58      ]\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 98%|█████████▊| 62/63 [00:04<00:00, 14.47it/s]\n",
      "100%|██████████| 63/63 [00:07<00:00, 14.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3392128348350525, 'eval_f1': 0.7563600782778865, 'eval_roc_auc': 0.8303621753719119, 'eval_accuracy': 0.16666666666666666, 'eval_runtime': 0.1591, 'eval_samples_per_second': 1055.718, 'eval_steps_per_second': 43.988, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:11<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.8468, 'train_samples_per_second': 127.123, 'train_steps_per_second': 5.318, 'train_loss': 0.3364620814247737, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1667\n",
      "Hamming Loss: 0.2833\n",
      "Micro Precision: 0.6892\n",
      "Micro Recall: 0.4298\n",
      "Micro F1-Score: 0.5294\n",
      "Precision per label: [0.72727273 0.77419355 0.6744186  0.66666667 0.5        0.71428571\n",
      " 0.46153846 0.7173913 ]\n",
      "Recall per label: [0.22857143 0.61538462 0.58       0.40625    0.04761905 0.54347826\n",
      " 0.28571429 0.55932203]\n",
      "F1-Score per label: [0.34782609 0.68571429 0.62365591 0.50485437 0.08695652 0.61728395\n",
      " 0.35294118 0.62857143]\n",
      "{0, 1, 2, 3, 4, 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 98%|█████████▊| 56/57 [00:03<00:00, 15.55it/s]\n",
      "100%|██████████| 57/57 [00:07<00:00, 15.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.367114394903183, 'eval_f1': 0.7595599790466213, 'eval_roc_auc': 0.8263148152864106, 'eval_accuracy': 0.16, 'eval_runtime': 0.1215, 'eval_samples_per_second': 1234.403, 'eval_steps_per_second': 57.605, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:11<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.3832, 'train_samples_per_second': 118.244, 'train_steps_per_second': 5.007, 'train_loss': 0.3724201269317092, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1667\n",
      "Hamming Loss: 0.2792\n",
      "Micro Precision: 0.6818\n",
      "Micro Recall: 0.4635\n",
      "Micro F1-Score: 0.5518\n",
      "Precision per label: [0.72727273 0.77419355 0.64583333 0.65909091 0.4        0.71794872\n",
      " 0.5        0.71153846]\n",
      "Recall per label: [0.22857143 0.61538462 0.62       0.453125   0.04761905 0.60869565\n",
      " 0.28571429 0.62711864]\n",
      "F1-Score per label: [0.34782609 0.68571429 0.63265306 0.53703704 0.08510638 0.65882353\n",
      " 0.36363636 0.66666667]\n",
      "{0, 1, 2, 3, 4, 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 99%|█████████▉| 136/137 [00:09<00:00, 15.34it/s]\n",
      "100%|██████████| 137/137 [00:12<00:00, 15.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36184239387512207, 'eval_f1': 0.7759452936444087, 'eval_roc_auc': 0.8357413046321251, 'eval_accuracy': 0.23076923076923078, 'eval_runtime': 0.3405, 'eval_samples_per_second': 1069.058, 'eval_steps_per_second': 46.992, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:16<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16.9182, 'train_samples_per_second': 193.342, 'train_steps_per_second': 8.098, 'train_loss': 0.3570338339701186, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1583\n",
      "Hamming Loss: 0.2708\n",
      "Micro Precision: 0.6846\n",
      "Micro Recall: 0.5000\n",
      "Micro F1-Score: 0.5779\n",
      "Precision per label: [0.8        0.77419355 0.65384615 0.68181818 0.42857143 0.68888889\n",
      " 0.42857143 0.73684211]\n",
      "Recall per label: [0.22857143 0.61538462 0.68       0.46875    0.07142857 0.67391304\n",
      " 0.28571429 0.71186441]\n",
      "F1-Score per label: [0.35555556 0.68571429 0.66666667 0.55555556 0.12244898 0.68131868\n",
      " 0.34285714 0.72413793]\n",
      "{0, 1, 2, 3, 4, 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 124/124 [00:08<00:00, 15.20it/s]\n",
      "100%|██████████| 124/124 [00:11<00:00, 15.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36444926261901855, 'eval_f1': 0.7930075486690504, 'eval_roc_auc': 0.8350972839270423, 'eval_accuracy': 0.1903323262839879, 'eval_runtime': 0.3141, 'eval_samples_per_second': 1053.954, 'eval_steps_per_second': 44.578, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [00:16<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16.2713, 'train_samples_per_second': 182.776, 'train_steps_per_second': 7.621, 'train_loss': 0.37223099124047065, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1167\n",
      "Hamming Loss: 0.2604\n",
      "Micro Precision: 0.6626\n",
      "Micro Recall: 0.6067\n",
      "Micro F1-Score: 0.6334\n",
      "Precision per label: [0.8        0.77419355 0.58333333 0.72580645 0.47368421 0.63157895\n",
      " 0.42857143 0.75409836]\n",
      "Recall per label: [0.22857143 0.61538462 0.84       0.703125   0.21428571 0.7826087\n",
      " 0.28571429 0.77966102]\n",
      "F1-Score per label: [0.35555556 0.68571429 0.68852459 0.71428571 0.29508197 0.69902913\n",
      " 0.34285714 0.76666667]\n",
      "{0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 99%|█████████▉| 126/127 [00:08<00:00, 14.47it/s]\n",
      "100%|██████████| 127/127 [00:12<00:00, 14.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36366593837738037, 'eval_f1': 0.8018816717930161, 'eval_roc_auc': 0.8297825155728488, 'eval_accuracy': 0.14792899408284024, 'eval_runtime': 0.2321, 'eval_samples_per_second': 1456.037, 'eval_steps_per_second': 64.617, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:16<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16.5496, 'train_samples_per_second': 183.388, 'train_steps_per_second': 7.674, 'train_loss': 0.3785802285502276, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1250\n",
      "Hamming Loss: 0.2667\n",
      "Micro Precision: 0.6445\n",
      "Micro Recall: 0.6264\n",
      "Micro F1-Score: 0.6353\n",
      "Precision per label: [0.8        0.73684211 0.57534247 0.68181818 0.5        0.61016949\n",
      " 0.5        0.71875   ]\n",
      "Recall per label: [0.22857143 0.71794872 0.84       0.703125   0.28571429 0.7826087\n",
      " 0.28571429 0.77966102]\n",
      "F1-Score per label: [0.35555556 0.72727273 0.68292683 0.69230769 0.36363636 0.68571429\n",
      " 0.36363636 0.74796748]\n",
      "{0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|█████████▉| 242/243 [00:16<00:00, 14.58it/s]\n",
      "100%|██████████| 243/243 [00:20<00:00, 14.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.369238942861557, 'eval_f1': 0.8050389922015597, 'eval_roc_auc': 0.8367163699811189, 'eval_accuracy': 0.20833333333333334, 'eval_runtime': 0.4948, 'eval_samples_per_second': 1309.674, 'eval_steps_per_second': 54.57, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243/243 [00:24<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.7499, 'train_samples_per_second': 235.557, 'train_steps_per_second': 9.818, 'train_loss': 0.37568702226803624, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1667\n",
      "Hamming Loss: 0.2479\n",
      "Micro Precision: 0.6879\n",
      "Micro Recall: 0.6067\n",
      "Micro F1-Score: 0.6448\n",
      "Precision per label: [0.72727273 0.61702128 0.65517241 0.75       0.82608696 0.7\n",
      " 0.4        0.72413793]\n",
      "Recall per label: [0.22857143 0.74358974 0.76       0.609375   0.45238095 0.76086957\n",
      " 0.28571429 0.71186441]\n",
      "F1-Score per label: [0.34782609 0.6744186  0.7037037  0.67241379 0.58461538 0.72916667\n",
      " 0.33333333 0.71794872]\n",
      "{0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 252/252 [00:17<00:00, 15.19it/s]\n",
      "100%|██████████| 252/252 [00:21<00:00, 15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3891410827636719, 'eval_f1': 0.8113332231951098, 'eval_roc_auc': 0.8180673547704116, 'eval_accuracy': 0.20089285714285715, 'eval_runtime': 0.5345, 'eval_samples_per_second': 1257.19, 'eval_steps_per_second': 52.383, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:24<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.722, 'train_samples_per_second': 244.276, 'train_steps_per_second': 10.193, 'train_loss': 0.3943261948842851, 'epoch': 1.0}\n",
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.2302\n",
      "Micro Precision: 0.6912\n",
      "Micro Recall: 0.6854\n",
      "Micro F1-Score: 0.6883\n",
      "Precision per label: [0.8        0.62       0.62686567 0.75925926 0.76666667 0.71698113\n",
      " 0.5        0.72727273]\n",
      "Recall per label: [0.34285714 0.79487179 0.84       0.640625   0.54761905 0.82608696\n",
      " 0.42857143 0.81355932]\n",
      "F1-Score per label: [0.48       0.69662921 0.71794872 0.69491525 0.63888889 0.76767677\n",
      " 0.46153846 0.768     ]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3325639/790495932.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "paths = [\"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_1.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_2.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_3.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_4.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_5.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_6.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_7.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_8.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_9.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_10.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_11.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_12.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_13.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_14.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_15.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_18.csv\"]\n",
    "\n",
    "stage = 0\n",
    "dataset_path = paths[stage]\n",
    "dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=f\"UBC-NLP/MARBERT\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=29,\n",
    "    stage = 1\n",
    ")\n",
    "\n",
    "trainer.save_dir = f'./exp_{trainer.exp_num}'\n",
    "\n",
    "trainer.train(\n",
    "    num_train_epochs=1,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)\n",
    "for i in range(1, 16):\n",
    "    paths = [\"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_1.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_2.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_3.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_4.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_5.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_6.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_7.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_8.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_9.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_10.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_11.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_12.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_13.csv\",\n",
    "         \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_14.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_15.csv\", \"Project/Cross-Country-Dialectal-Arabic-Identification/CL_stages/stage_18.csv\"]\n",
    "\n",
    "    stage = i\n",
    "    dataset_path = paths[stage]\n",
    "    dev_path = \"Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "    labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "        'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "        'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "    trainer = BertTrainer(\n",
    "        training_dataset_path=dataset_path,\n",
    "        model_name=f\"Project/Cross-Country-Dialectal-Arabic-Identification/exp_29/stage_{stage}\",\n",
    "        labels=labels,\n",
    "        threshold=0.3,\n",
    "        exp_num=29,\n",
    "        stage = i + 1\n",
    "    )\n",
    "\n",
    "    trainer.save_dir = f'./exp_{trainer.exp_num}'\n",
    "\n",
    "    trainer.train(\n",
    "        num_train_epochs=1,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        greater_is_better=True,\n",
    "        per_device_train_batch_size=24,\n",
    "        per_device_eval_batch_size=24,\n",
    "    )\n",
    "    trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 68.96 %\n",
      "MACRO AVERAGE RECALL SCORE: 65.43 %\n",
      "MACRO AVERAGE F1-SCORE: 65.32 %\n",
      "MACRO AVERAGE ACCURACY: 76.98 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"Project/Cross-Country-Dialectal-Arabic-Identification/exp_29/stage_16/-home-ali.mekky-Documents-NLP-Project-Cross-Country-Dialectal-Arabic-Identification-exp_29-stage_15-experiment-29_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = 16\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    # model_name=f\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    model_name=f\"Project/Cross-Country-Dialectal-Arabic-Identification/exp_28/stage_{stage}\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=29,\n",
    "    # stage = i + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 68.96 %\n",
      "MACRO AVERAGE RECALL SCORE: 65.43 %\n",
      "MACRO AVERAGE F1-SCORE: 65.32 %\n",
      "MACRO AVERAGE ACCURACY: 76.98 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"Project/Cross-Country-Dialectal-Arabic-Identification/exp_28/stage_16/-home-ali.mekky-Documents-NLP-Project-Cross-Country-Dialectal-Arabic-Identification-exp_29-stage_15-experiment-28_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
